<!DOCTYPE html>



  


<html class="theme-next gemini use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.2" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Hexo, NexT" />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.2" />






<meta name="description" content="写在前面的话1、本pdf是公众号小小挖掘机连载的推荐系统遇上深度学习系列的前五十篇文章的pdf合集2、关于文中涉及的代码，有些链接已失效了，原因是整合到一个github中了，代码请关注：https://github.com/princewen/tensorflow_practice 3、相关数据在github的readme部分，tf版本是1.44、为防止侵权，文中几个部分加了公众号二维码，请大家见">
<meta property="og:type" content="article">
<meta property="og:title" content="Re:0">
<meta property="og:url" content="http://yjjblog.site/2020/03/07/推荐系统系列合集/index.html">
<meta property="og:site_name" content="Re:0">
<meta property="og:description" content="写在前面的话1、本pdf是公众号小小挖掘机连载的推荐系统遇上深度学习系列的前五十篇文章的pdf合集2、关于文中涉及的代码，有些链接已失效了，原因是整合到一个github中了，代码请关注：https://github.com/princewen/tensorflow_practice 3、相关数据在github的readme部分，tf版本是1.44、为防止侵权，文中几个部分加了公众号二维码，请大家见">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-2a82a353433e8a07.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/4155986-0f2974d9d54ec683?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/4155986-5fb87c7555fed3ca?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/4155986-f305b07b44b19b9e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/4155986-990377c58bf6a215.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/4155986-1f638fe25a63244c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/4155986-a262e2244174e776.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/4155986-6a02a396266a34d7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/4155986-6d08a2cdcc6668fb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-a09652fbd5cb768d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/4155986-b79f3cdc1229ffbb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/4155986-3723a0992d59f0e9?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-d04fed8047209d53.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-659e8f0e43d6310d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-d0f6963eb0505c31.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-e3da4d35478d62b3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-c2df975e6e6a7847.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-142f546cdaef9e42.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-b599e465a372c134.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-f4363ca2be689dbb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-5f476d2c5b616232.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-12f3119df69b7b5b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-7e036f56982d323b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-2b8d2e22017ad339.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-cd51e0bd97ab285d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-1118724d47e2c65e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-21fa429e42108e99.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-7984bc2c7474d6ea.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-d144aba541c68a34.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-f9af97ad7e0f5b88.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-366d825a661466a3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-cc075cd266bf2d5f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-295f18796ee0030e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-b8943ac16560285b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-53a1ed7584a8bb71.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-55ff1b9e36979f20.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/4155986-a9ead5ad8ff9d2d3?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-908ee89d46240580.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-4fedbff73c8489ef.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-3bef2f5a5f87fa26.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-aa7d9d209c6cf3d5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-0ce1a9e0a5d32ca1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-dfc32419b13cac1a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-feacc6f25a1a985d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-8de32b6dab68c108.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-aa82753a2af4b2cf.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-3b2e83dee702d12d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-6a3cad235da5dd61.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-9867a7134749f48e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-c3dc7a8ade52b842.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-996cfd8061a5a2fb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-dea545f127da8818.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-79596b0e03993e0d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-cc22b83064d309cf.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-25ed83f0405ce1d5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-6a7520193a39dd2a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-79596b0e03993e0d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-6304e98edc03f155.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-e1691579affb9878.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-3da6a7784a8aa7ad.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-efc8f371d4e694a4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-2ac2cd7b351795d8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-69309c37e2b2ba70.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-3fce559f6e92c043.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-a4ab3900deca2373.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-4ddc93512149a560.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-5e75fafe9e0d9a14.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-d9924e3ef896dc31.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-badf1326578a3cae.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-a66fbf3c57b4d1ab.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-23e359033294a7aa.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/4155986-e7e81f16136ad08f?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-186da78f1c6e9564.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-85c1002f1da19bfe.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-7128a3adfeb2f70c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-4b93c5f2497ba0c4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-23e359033294a7aa.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/4155986-e7e81f16136ad08f?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-87fcfdc28ceafb38.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-ce7c934ad3c8a4d2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-5b337dd109d82093.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-f84e680b632b12cc.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-f84e680b632b12cc.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-ea8d47e89b49164d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-c7908e1e8cdaf5e2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-d4972faaee7094cf.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-11fd2aaba03b027a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-06ad603c5de44b60.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-31296ad3c9f891e6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-3e988dcab6079c76.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-e770d9c378f64925.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-7efec2f9e6c1d919.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-6a7aa2038eb2cbee.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-d4d8eda66f1169ba.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/4155986-0c24244b8e8c01b7.gif?imageMogr2/auto-orient/strip">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-84670734192f9b0d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-7b554bfc12d8d180.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-d396340c58fa005c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-8a4cb50aefba2877.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-9587957913f5ad4e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-387590265d8350a6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-9cab8ff893c16d2a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-4cf5e3db52a1c7d3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-875ca45170bf6cf9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-4603109b9e326aa7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-b381c43ffb45efad.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-c104fa23f3d69c0b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-7448a3708977e2ca.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-9c075241d934afaa.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-0cc3550bf20a608c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-f33f0e781fce1331.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-cd8744d945dab6c9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-4f43b020b149514f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-c5c67c9c159e7820.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-f625ccafecdd8070.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-619cfe110711228a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-fc499022ad1423cf.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-abf4284380169268.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/4155986-85269ad91ee8ce57?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-f2b84165d0c8cf63.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-e191ab28dde4b7d1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-af26e938fe5ef24a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-282d39b8f1196a80.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-7deb00fa956f9895.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-861e6f749bf24730.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-6efefe77e0374972.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-98ae4c1ac8c0b953.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-07f5daf6b8fd7cd7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-8dfc87ed3e41e2c4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-a6fb06b46c5fe613.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-8c2127f9ff3a7f65.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-bfa2eb6125928f38.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-2c7dafc34881e607.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-3e50c9d6b97cf246.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-dea44a2b28da4d40.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-977d7b5d7a2b4b88.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-6e185841e4c9209f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-f69372a3786abe8b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-d755ce22b36b0002.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-a7c554ed808879e6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-5f1bbdc0797fb6b0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-01d3ee4bbd46526c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-3655ed112493f3ea.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-3601af8c6eccdd0c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-4e50f0f50332e38d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-a117a20d3ea35d70.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-0e2cf2a24f5ba18e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-794df050c633aeb5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-acc7a474bc91e358.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-0cbcc9cd2e73ccef.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-912568b7cf7c7fb8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-370bdc8452a29aa2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-69042c94745f5ddb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-1a7e60057b18469c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-e10d69fa48205994.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-0566ccbab3963044.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-76d46fb6f51a2ae4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-a00e2537e2dcec5c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-33878efd40cff99d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-64bd6ed4a2d3bd2d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-24ea9104c2946150.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-d7acbc028fa123e8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-e7121be601cb3954.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-9de3b313d6f36bdb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-e04544fdaaedfdca.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-6fb75530a0291d47.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-ea0c8b302f0b0f61.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-40eb640113d5ecfd.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-a777c2c0dd3abea7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-052b331fc36e5cbf.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-df9dea8b1d13f7f0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-2a82a353433e8a07.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/4155986-5aba3ad4e656a9ca?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/4155986-671a7f6cdb03cd25?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/4155986-0e3eac49259844d8?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/4155986-3a296a23b09d71b3?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/4155986-49e446be096fe4a1?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/4155986-be34a2e974da46f0?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/4155986-f1cf1e33db359f61?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-0a2af3358b3fae95.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-b823ba5c48118494.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-4a79c572c058b978.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-8a2e764a98d17eba.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-e75f3bd1a822ea0f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-b631860a8264420a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-11a7a60a51d7c56e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-5710b4ca602b8519.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-e19e155504234f93.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-2c1cfa453ef67a4e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-4cf62bc4715da3a7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-3baae7459c56d7d8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-2a82a353433e8a07.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-d4572939999edaf3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-d9da3d968a2fdc1c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-ab8a627ded650751.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-c217d227121fa10c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-65a604bb693beebf.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-8ee65bfd7a611be0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-236632a5e412a2bb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-5fd20cafb4a3fef7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-376938a54c692cbb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-85567850816136ef.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-11706f64ea66cf2e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-f73cc229c99980ae.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-4250d6869c6481a3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-482bdce2c1bee47c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-358cc3c6656a5408.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-936e11da7a2ab0fd.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-111dfa0155c33845.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-2ff44bcf7d3b4054.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-448758600f0d336f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-69a8d1e143b1953a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-ad1698c345e7df33.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-aff9654a61c327ce.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-d4ed014c118ab98b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-97f7101f2de74bb5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-3748528c2a5fc919.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-31ccd3226cc1426d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-570b012bc8af5139.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-e583e6dbf39b38d5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-148dbce6adc8dd64.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-716f554b649e1c43.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-3543355f3bed04dc.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-b50a185fa9778b6a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-bb500b1a936c380b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-5cb991131b04fb8f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-d362e6c0d3411296.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-deab3d8e2c561d13.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-bf036c9c9f1298b7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-c3282d4fea4535ea.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-47c5254c8ee023b0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-c515e9855fa8e08e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-a03ff1a10d70f0dc.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-298d24f7d518a3fa.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-a5bc6ddc83bb3c46.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-a74d49dd7f79aefa.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-7f45ccc66c59c2e7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-e9010bc3e0e254a3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/4155986-ba9b891cbfbc639d?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/4155986-56ca5a89ff79d8c1?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/4155986-4b29b0298355ca4f?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/4155986-d0f6bd99198a0531?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/4155986-e15af599d004bd42?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/4155986-530acc1b9a22c86d?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/4155986-a280ce81f900d1a2?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/4155986-960efa15cfe279c6?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/4155986-9d64ec1c08aa889f?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/4155986-1a568e1198813514?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/4155986-039b14d4d3e620ea?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/4155986-21ffa3238643a88d?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/4155986-7b02b04a2d1d205a?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/4155986-4e51511c195adf3b?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/4155986-8a6b13fe3e47f6eb?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/4155986-ba4c70369248eed0?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/4155986-e302cdf57ffb0f70?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/4155986-27dad6f0bb52cb33?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-21e01cfc409cf299.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-26eb5603bd1f3234.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-252e185cf5abe1d1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/4155986-c3b894ee29565fe6?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/4155986-149fe07ecdbaf7d2?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/4155986-86038ca7349f6a8c?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/4155986-3e3163f1c4437ed7?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/4155986-4008a1822870d64f?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-5e300d9acebcbe67.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-520af2feeea172e0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-14c58d46b353670b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-c4c5e5f40e25e652.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/4155986-87d500bcd2bb54ee?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/4155986-a0500b565e02b00f?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/4155986-d39c0f2053f68099?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/4155986-18ca7f734ceea229?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/4155986-f51933a36def647b?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/4155986-0122fd3309b4959e?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/4155986-b1452b98b574f72f?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/4155986-63b4322397513921?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/4155986-6bf081cf072223ef?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/4155986-fa3574d61d7760d5?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/4155986-ff7359020d1c3a99?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/4155986-8927d17fcce09bbd?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/4155986-e565ae61e140a7e2?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/4155986-ad12260191f657fc?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/4155986-dcd4347c2d4435f5?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/4155986-ca2165d2efb641ff?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/4155986-f6c4e1a55b1acf46?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/4155986-19a4de0536baec52?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/4155986-a9c0a7f30199f396?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/4155986-544b94047fdd8c13?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/4155986-00b6ab3db4a55a8f?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/4155986-930910cd08f46c27?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/4155986-52b89501393c2b5e?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/4155986-e0162f0ab1c39cf3?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/4155986-efad68ef04a968ae?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/4155986-b190ee2503d04297?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/4155986-3b4a0e3268bb8edb?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/4155986-16a87a156e89e83b?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/4155986-b5d125fbefffc44b?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/4155986-cbc3fbdd1a969529?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/4155986-eeb3fea23988b651?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/4155986-83066f007db3fa1a?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/4155986-a5bee70e9a4c5727?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/4155986-5213b5683f51b7a2?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/4155986-f827ea7a64fca997?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-a09ff98ee9c5061b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-e06e90515ee229f2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-d40c19bf4ee5dae5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-1ce3fe2781dac963.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/4155986-46e92f37f013a32e?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/4155986-a84eda06ad7f9216?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/4155986-f462bfa9b62e8a91?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/4155986-b4ce399c599794e2?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/4155986-1080c174ebef8089?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/4155986-1a71a28c081dd25b?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/4155986-580d573dc4e1e338?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/4155986-5f3e41adecc774a3?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/4155986-77cb449f01b3b8ce?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/4155986-6c402afad4e5e5dd?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/4155986-b6e9b6255fabc2ac?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/4155986-0fd41dfca9690b36?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/4155986-39d1aea425974886?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/4155986-89dd4ba58d2205c4?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-cfd89ba8439f1e28.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/4155986-808d0609856fbc30?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/4155986-ecee2d8901361e99?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-f6fd001a226680f5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-1f1a75dc0c18ed69.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-89974f1fc48762af.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-576be6de0a1dbeb5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-52d9f278c189208a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-865e97a5b034c18a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-090532cfbf3593fc.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-f41cc8ab14a3d724.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-173a66f80a7e4f4c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-99a5bcc517d40903.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-89974f1fc48762af.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-573b9e5a2a17dc49.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-f4815190d2e14ffc.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-7294179d54cd654b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-4b557a605b0cc417.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-576be6de0a1dbeb5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-18bdea2ca62019ee.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-4abc289d2bddc4b4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-451cca0e243381c5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-5b76de4fa1a74cd5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-701056a21ce4ae32.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-a348ceb8cc4e48c5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-e85258529eeb80c7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-de30a9f502d39855.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-02427e0f63729eae.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/4155986-14586a6e6389134b?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/4155986-df004503462d5103?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-9027fa8dc972d164.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-bbb5a385306857b6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-d3dd959e6239e3fa.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-b2316abcc1ebc474.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-6f5c535a91e4261d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-a626f3e0480c45e9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-970cccbe7a019632.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-2c96d542ec3fba69.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-e7a48a07f72a4f99.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-fc6a2c7fe6282758.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-753463da29dba12f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-5f38e8cd9445f703.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-cacd44ad941670c8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-38101a1635f99175.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-e5bfa3817ddf8931.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-c97b3fdeac807399.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-19c261943d9cad98.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-652bd379897ca712.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-8a66bcd8ace84076.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-562609a9e3dac2d5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-651f4bcf339244cf.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-2b95f73d788a2357.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-8a66bcd8ace84076.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-562609a9e3dac2d5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-8e7c2554cecde4fa.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-d88a94cd73303a53.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-67a326541ea4d52d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-45df2710a390835f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-5e1fd7b6de9ea72e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-00b00111e6a03729.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-53a3694d426368d9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-7b4f38f238ddcf89.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-715c64b5a02dfec6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-6da64978cf210a4d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-370fd47239d10fd7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-fe7e7abdf2069d4c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-2b95f73d788a2357.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-9ce78dc2d25f2aaa.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-97a81a280c801e8d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-bff868eddc87d42a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-ec734bf5b72d6bb8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-09ae91d82c63a767.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-c788c44fe8913dea.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-f43b8ee3f402e19f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-0970cea94b3fd1b3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-59c95891e06ffd2a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-8c15e292133d1c1d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-59fa1515cbc2cbe3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-7f6fe098898f2d08.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-0c27d9f50a9d88d4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-7b568648318a2957.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-3b9723bca8df010f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-a8f7730e2f8b59c0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/4155986-583b62c99689bdde?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-1bd15d306b436245.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-e6d7708efb922818.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-3b9723bca8df010f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-a8f7730e2f8b59c0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-b777458a52c85f50.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-cb61c885483a0a1f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-3acecc2c536c9a31.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-c3d7071e85b98fc7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-c18d3771e13ce146.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-252a7a1ff40e55d4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-d5b37b7bd4974a8e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-56ada6caabfb4da4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-ee2d810640b21c66.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-186018ac997bda9c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-a8b805960f4ad836.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-7997eb0e11c4f847.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-e0caf8d85815c28b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-a63feefb22ef262b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-64d00f3dd1a96efa.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-e20d29f3b24661a4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-869814388cc1d3de.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-c845356f44ac81ff.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-16a803537394b95a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-e8b0b2ee1cc9ed82.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-5765082045798292.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-463bebec02124b10.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-20b1538463d23c9d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-99c9aef032b1a478.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-91a2d1691cb61f6e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-9172bcbf97a0afbe.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-519f2de7d6fb2306.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-9812d3757fc86399.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-58f38b24cd9de620.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-287849dd019eedd7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-76667dddc59dcc68.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-41f583980fdb3bc7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-91ceeabb5ea82986.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-dccd3f4c564bbfc8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-85c79c0aa6170a8c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-0dc44e0c9339c622.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-611ba7e51f45d9dd.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-07cc0a61db6b564c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1000">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-82e45877838853f5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1000">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-15ea4557b870b6d2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1000">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-05842d10c21e0bf5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1000">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-8a5a65ea7f51b1d5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1000">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-1e4af32d70add8a1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1000">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-1d1c145fa1a4d91d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1000">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-aecfa7422423ddbd.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-c424f27784e71b0e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-447ab0826310a5c1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-033b09deeaa95aa8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-75761ad19aa5275b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-09f70538d959fd6b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-53fa30b3e107732f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-bf56007acf08cdc2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-1e8ed7eda16500ba.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-8e936ec49457a933.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-7f7630edc0692294.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-4cf3f6c03626825c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-cb5032986cde24f7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-1104f822da782760.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-0d60b64d050ee97b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-2375782eb9cd88de.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-2fa775e4d7be048b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-fe59f87b5d7c4313.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-2e46cf2dccca4760.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-cea02da3ed7c6547.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-ea543a843dddad7c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-448067ca34dd5e05.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-713f7704b18d92d6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-a5879bdcd26fe39d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-798b142f714a912e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-95876c8600ca8f48.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-086d6041aa5826a3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-98bc8e688d0cc59b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-70fa355a4b782567.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-27c78634cc082a87.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-a339b2661aeb9eee.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-0d25ba23d89178b4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-529b519cff603908.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-be455b49f2f46876.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-5d961ad4b3ce40fb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-c1e02a22d8e6d0ae.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-a0b3ed5bf2b85d32.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-c941e0a507ed5bfb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-190969135dc088c0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-6c3b5b223ea151b5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-94f1d148fca59fa0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-dc6eb648d3bffb9a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-d46efb6d87fee3e9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-3f62740bf4cb11cc.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-bf7dc4d4324e7092.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-b8523048a9109a37.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-594ba5d2ae5e3e7b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-5cd8ce88f20d30ee.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-0817085a39559036.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-b9a88a521373f961.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-c0133765c4872ee1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-fe4cf998fe8c58b0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-6809862c7f3230da.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-f69d8ff3f8850289.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-ea2db36ddc8e1cbf.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-7e2ee6d48c57188e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-8507f16f23d02b88.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-d52774165fc5fdd4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-04dcf009d4fdca89.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-d4b33ee1e530b097.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-d0dce1de379501e0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-4d6ed4dedc6a320d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-9d6c5b6ed8377976.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-9c625e311d68b944.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-f422cff606207f5b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-1344aa9c177c8295.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-b2219078a6e4a357.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-5b64421bc32744f0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-06bd98a0d27d8550.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-a0f67232b47dfad5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-eaa305403cfca82e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-b4f1e85b7b09c1bb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-6836e07a9eaa612e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-8e0500b3883cc35c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-d9f3d317b347ff3b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-e81e27c2dc3965b7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-a9332aae47a6f6a7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-4f13de6d360f8fe3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-b68ab37a463eef79.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-fc3fb0db85dc2417.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-5716e83c95541b49.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-57a0b47a296ad5a9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-82d1962dfa35dd9b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-3ec51bbf2b9da6b7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-899331747f53e7c7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-133cc7a4375e9e5f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-5ff41198fcb1f88e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-d29a07d0f0f2c18f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-96f47cb94f59ffe8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/4155986-c07a19e650bbdd2f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/4155986-c94fc5b6694567f4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/4155986-6e0dd341b4fa3103.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/4155986-f396d4d0742cd12e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/4155986-93585daff46fd7a9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/4155986-5904c3f7334d8559.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/4155986-f86bd9060a1fa3c0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/4155986-e90a31e7bf7b44f0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/4155986-94420bdd6bb22f47.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-1104f822da782760.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-2fa775e4d7be048b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-cbaf6a8236cbba12.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-6bff4b1e8d3ec8ad.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-40f93e5bcec661f7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-e63997cc3ff9eda5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-73995f68b88ba2d6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-f3de0f31b9062f0a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-3a8e8cb26c15c6ee.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-fd681f76356228e3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-3e01769c6f823a7a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-4bc69a6f3214534e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-2e21a03dd78ee30b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-4e038d04152a2863.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-0752f41f36717a38.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-1142b2ab24694f99.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-83d42241b7e8e93e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-8be2f05b3cb91521.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-4d7d30d2d8f79ff0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-5a27b4d1c7fc9437.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-729d66039a4805b9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-eb1a7c6253b6df60.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-bacdfb6d19d221c9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-fbfd30c2108abb43.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-48c3057c6312326b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-9d29c955351f76a9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-95e1f621cd59198e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-68aa16a80efe4a6e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-1e5bdc665a576058.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-c0ecd9295724634e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-2b2eebcf4638e27a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-86d9ddd018b5a765.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-2aed912966c4e1c9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-98b9c1e6f38e99d4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-c354c303bf1eea90.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-25f586db8a646805.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-1d990cc7afecb6f4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-cdabc24297f030fc.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-00157d5a3311bad2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-d5bf69c99382e95d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-64177b617bbde8a2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-f079b310dbda4506.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-258c5e7407106905.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-cbe14870791b79f7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-9c1a9491c0562557.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-5d1a11d5af5d296c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-46fbf30ab74fec30.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-2e280917a471e177.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-8ba617a55240682e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-35fd0641b1ec0e30.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-b5a2f1dbf6baf651.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-a1facb273d9b07d0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-808b3a393b77042e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-4d9f7be975212078.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-9fb99f234039131c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-640e77ef6bba55ea.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-274747a78535f71f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-df5b87bf3d864181.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-40bc49e52f9964d4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-cf0786f9622a7ad8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-543f71e75a347194.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-598d06bb47bbb81a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-6ff18221657d6323.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-7d89dbb5c7b02058.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-bfe8f016d5e720c3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-f3c0a64197a11613.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-ba96009fed7c6627.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-a4424f7279c08b40.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-7d8f6a4f3af747a7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-df0c2a6cc030d2ca.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-d50228540af11d69.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-9d172d789e417247.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-edab767295d40152.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-803da73a7d52d9d0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-eae047f2f9e30f42.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-287551c0c8c096d1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-d6a96b2997f0e937.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-5dcf5d8138a16d64.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-d4d9493a610caa4a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-6e0be6d89e78d0f9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-9f92d33224df1cac.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-365f9142c596e56c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-41b5fa1f466ca744.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-301b7c144b74414a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-7d393efb565658a4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-dd6ed1dedf886b90.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-3e386c68800cbf70.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-cc659c43b418fea8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-ca2f00e8ff352528.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-1464c70544e5228a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-22677981881da65e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-65bdeb51d5384cd6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-d8c61ccb285a16f5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-983a1cdc5101d62e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-3ec9550d64d059be.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-9902b0619fc4f52c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-3e06015d7ccbc784.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-4a225b5f18b69ea8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-9189773bf99b3679.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-25b45e98b517b1d7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-f5b9b52dde560500.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-abd743969e78dce8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-58e335ca5cfc1399.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-aa1743f85586964a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-6d46fb80cdf99200.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-b4d28dca8bb4ded6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-c89bb3d7fa3bf226.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-ed411c49aac4935e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-18f41be00404b127.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-b9d865ff498a5bd2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-67594b9dcbe23c72.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-c6b90a7091d0a556.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-b991fea11318d3ab.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-742daff90058f54b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-d6d74cc71b4c2ec9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-939cb93ba125a403.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-3066e1c6c2089c33.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-e6f7d2eb5005850f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-07a55b603af4509b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-0c7733c2429b0a07.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-bae2a47de25babf3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-c793bfe88d300871.jpeg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-c793bfe88d300871.jpeg?imageMogr2/auto-orient/strip%7CimageView2/2/w/430">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-1418d5b13a902e80.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-0583579fd0b29b1c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-4968ee5841ad858d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-852fe1e81e01bdb5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-896cf02b41fb81d7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-b274bb24444688ce.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-84a7a47b97edd862.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-fb9c26b4c0317f1b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-c793bfe88d300871.jpeg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-c793bfe88d300871.jpeg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-22677981881da65e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1000">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-6bde568142884aaa.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-c95d69d362880ff2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-4a225b5f18b69ea8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/802">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-6c3b5b223ea151b5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1000">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-94f1d148fca59fa0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/412">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-dc6eb648d3bffb9a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/406">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-d46efb6d87fee3e9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/476">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-3f62740bf4cb11cc.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/629">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-eb723ec62d6e4c06.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-cbaf6a8236cbba12.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/414">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-6bff4b1e8d3ec8ad.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1000">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-40f93e5bcec661f7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1000">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-e63997cc3ff9eda5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/622">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-73995f68b88ba2d6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1000">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-f3de0f31b9062f0a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1000">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-3a8e8cb26c15c6ee.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1000">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-d4572939999edaf3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-d9da3d968a2fdc1c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-ab8a627ded650751.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-65a604bb693beebf.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-4250d6869c6481a3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/4155986-df004503462d5103?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-bbb5a385306857b6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-d6a96b2997f0e937.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1000">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-1418d5b13a902e80.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-31ccd3226cc1426d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-e583e6dbf39b38d5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-148dbce6adc8dd64.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-716f554b649e1c43.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-c793bfe88d300871.jpeg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-306b286f881c22f7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-2df384d344e0d226.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-1b0b1129cb1ee0b7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-1d96d031c66bd767.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-3908706420c82e69.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-23ced314d7105fcb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-a92f10754346cdbc.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-1dd305aae670aeab.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-25d258b4efb6dd80.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-82ddb32266b1b04e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-7acf226020877794.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-741f57daf80ec332.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-d4d33881bd5c1ef2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-8c48a1f708512dcb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-77b3d22689e4a58f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-1496b661476dcf88.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-c0a9e2104d7b2bc9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4155986-c793bfe88d300871.jpeg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:updated_time" content="2019-12-29T13:53:02.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Re:0">
<meta name="twitter:description" content="写在前面的话1、本pdf是公众号小小挖掘机连载的推荐系统遇上深度学习系列的前五十篇文章的pdf合集2、关于文中涉及的代码，有些链接已失效了，原因是整合到一个github中了，代码请关注：https://github.com/princewen/tensorflow_practice 3、相关数据在github的readme部分，tf版本是1.44、为防止侵权，文中几个部分加了公众号二维码，请大家见">
<meta name="twitter:image" content="https://upload-images.jianshu.io/upload_images/4155986-2a82a353433e8a07.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    sidebar: {"position":"left","display":"post","offset":12,"offset_float":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yjjblog.site/2020/03/07/推荐系统系列合集/"/>





  <title> | Re:0</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Re:0</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">从零开始的代码之路</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yjjblog.site/2020/03/07/推荐系统系列合集/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="InvictusY">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar1.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Re:0">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline"></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-03-07T22:14:40+08:00">
                2020-03-07
              </time>
            

            

            
          </span>

          

          
            
          

          
          
             <span id="/2020/03/07/推荐系统系列合集/" class="leancloud_visitors" data-flag-title="">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">阅读次数 </span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>写在前面的话<br>1、本pdf是公众号<strong>小小挖掘机</strong>连载的<strong>推荐系统遇上深度学习</strong>系列的前五十篇文章的pdf合集<br>2、关于文中涉及的代码，有些链接已失效了，原因是整合到一个github中了，代码请关注：<a href="https://github.com/princewen/tensorflow_practice" target="_blank" rel="external">https://github.com/princewen/tensorflow_practice</a></p>
<p>3、相关数据在github的readme部分，tf版本是1.4<br>4、为防止侵权，文中几个部分加了公众号二维码，请大家见谅<br>5、本系列还在连载中，如果对本系列感兴趣，欢迎关注下方公众号：<br><img src="https://upload-images.jianshu.io/upload_images/4155986-2a82a353433e8a07.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>[TOC]</p>
<h1 id="推荐系统遇上深度学习-一-–FM模型理论和实践"><a href="#推荐系统遇上深度学习-一-–FM模型理论和实践" class="headerlink" title="推荐系统遇上深度学习(一)–FM模型理论和实践"></a>推荐系统遇上深度学习(一)–FM模型理论和实践</h1><h2 id="1、FM背景"><a href="#1、FM背景" class="headerlink" title="1、FM背景"></a>1、FM背景</h2><p>在计算广告和推荐系统中，CTR预估(click-through rate)是非常重要的一个环节，判断一个商品的是否进行推荐需要根据CTR预估的点击率来进行。在进行CTR预估时，除了单特征外，往往要对特征进行组合。对于特征组合来说，业界现在通用的做法主要有两大类：FM系列与Tree系列。今天，我们就来讲讲FM算法。</p>
<h2 id="2、one-hot编码带来的问题"><a href="#2、one-hot编码带来的问题" class="headerlink" title="2、one-hot编码带来的问题"></a>2、one-hot编码带来的问题</h2><p>FM(Factorization Machine)主要是为了解决数据稀疏的情况下，特征怎样组合的问题。已一个广告分类的问题为例，根据用户与广告位的一些特征，来预测用户是否会点击广告。数据如下：(本例来自美团技术团队分享的paper) </p>
<p><img src="http://upload-images.jianshu.io/upload_images/4155986-0f2974d9d54ec683?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>clicked是分类值，表明用户有没有点击该广告。1表示点击，0表示未点击。而country,day,ad_type则是对应的特征。对于这种categorical特征，一般都是进行one-hot编码处理。</p>
<p>将上面的数据进行one-hot编码以后，就变成了下面这样 ：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/4155986-5fb87c7555fed3ca?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>因为是categorical特征，所以经过one-hot编码以后，不可避免的样本的数据就变得很稀疏。举个非常简单的例子，假设淘宝或者京东上的item为100万，如果对item这个维度进行one-hot编码，光这一个维度数据的稀疏度就是百万分之一。由此可见，数据的稀疏性，是我们在实际应用场景中面临的一个非常常见的挑战与问题。</p>
<p>one-hot编码带来的另一个问题是特征空间变大。同样以上面淘宝上的item为例，将item进行one-hot编码以后，样本空间有一个categorical变为了百万维的数值特征，特征空间一下子暴增一百万。所以大厂动不动上亿维度，就是这么来的。</p>
<h2 id="3、对特征进行组合"><a href="#3、对特征进行组合" class="headerlink" title="3、对特征进行组合"></a>3、对特征进行组合</h2><p>普通的线性模型，我们都是将各个特征独立考虑的，并没有考虑到特征与特征之间的相互关系。但实际上，大量的特征之间是有关联的。最简单的以电商为例，一般女性用户看化妆品服装之类的广告比较多，而男性更青睐各种球类装备。那很明显，女性这个特征与化妆品类服装类商品有很大的关联性，男性这个特征与球类装备的关联性更为密切。如果我们能将这些有关联的特征找出来，显然是很有意义的。</p>
<p>一般的线性模型为： </p>
<p><img src="http://upload-images.jianshu.io/upload_images/4155986-f305b07b44b19b9e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>从上面的式子很容易看出，一般的线性模型压根没有考虑特征间的关联。为了表述特征间的相关性，我们采用多项式模型。在多项式模型中，特征xi与xj的组合用xixj表示。为了简单起见，我们讨论二阶多项式模型。具体的模型表达式如下：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/4155986-990377c58bf6a215.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>上式中，n表示样本的特征数量,xi表示第i个特征。<br>与线性模型相比，FM的模型就多了后面特征组合的部分。</p>
<h2 id="4、FM求解"><a href="#4、FM求解" class="headerlink" title="4、FM求解"></a>4、FM求解</h2><p>从上面的式子可以很容易看出，组合部分的特征相关参数共有n(n−1)/2个。但是如第二部分所分析，在数据很稀疏的情况下，满足xi,xj都不为0的情况非常少，这样将导致ωij无法通过训练得出。</p>
<p>为了求出ωij，我们对每一个特征分量xi引入辅助向量Vi=(vi1,vi2,⋯,vik)。然后，利用vivj^T对ωij进行求解。</p>
<p><img src="http://upload-images.jianshu.io/upload_images/4155986-1f638fe25a63244c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>那么ωij组成的矩阵可以表示为: </p>
<p><img src="http://upload-images.jianshu.io/upload_images/4155986-a262e2244174e776.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>那么，如何求解vi和vj呢？主要采用了公式：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/4155986-6a02a396266a34d7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>具体过程如下：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/4155986-6d08a2cdcc6668fb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>上面的式子中有同学曾经问我第一步是怎么推导的，其实也不难，看下面的手写过程(大伙可不要嫌弃字丑哟)</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-a09652fbd5cb768d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>经过这样的分解之后，我们就可以通过随机梯度下降SGD进行求解：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/4155986-b79f3cdc1229ffbb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<h2 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章"></a>参考文章</h2><p>1、<a href="http://blog.csdn.net/bitcarmanlee/article/details/52143909" target="_blank" rel="external">http://blog.csdn.net/bitcarmanlee/article/details/52143909</a><br>2、<a href="https://blog.csdn.net/u012871493/article/details/51593451" target="_blank" rel="external">https://blog.csdn.net/u012871493/article/details/51593451</a></p>
<h1 id="推荐系统遇上深度学习-二-–FFM模型理论和实践"><a href="#推荐系统遇上深度学习-二-–FFM模型理论和实践" class="headerlink" title="推荐系统遇上深度学习(二)–FFM模型理论和实践"></a>推荐系统遇上深度学习(二)–FFM模型理论和实践</h1><h2 id="1、FFM理论"><a href="#1、FFM理论" class="headerlink" title="1、FFM理论"></a>1、FFM理论</h2><p>在CTR预估中，经常会遇到one-hot类型的变量，one-hot类型变量会导致严重的数据特征稀疏的情况，为了解决这一问题，在上一讲中，我们介绍了FM算法。这一讲我们介绍一种在FM基础上发展出来的算法-FFM（Field-aware Factorization Machine）。</p>
<p>FFM模型中引入了类别的概念，即field。还是拿上一讲中的数据来讲，先看下图：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/4155986-3723a0992d59f0e9?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>在上面的广告点击案例中，“Day=26/11/15”、“Day=1/7/14”、“Day=19/2/15”这三个特征都是代表日期的，可以放到同一个field中。同理，Country也可以放到一个field中。简单来说，同一个categorical特征经过One-Hot编码生成的数值特征都可以放到同一个field，包括用户国籍，广告类型，日期等等。</p>
<p>在FFM中，每一维特征 xi，针对其它特征的每一种field fj，都会学习一个隐向量 v_i,fj。因此，隐向量不仅与特征相关，也与field相关。也就是说，“Day=26/11/15”这个特征与“Country”特征和“Ad_type”特征进行关联的时候使用不同的隐向量，这与“Country”和“Ad_type”的内在差异相符，也是FFM中“field-aware”的由来。</p>
<p>假设样本的 n个特征属于 f个field，那么FFM的二次项有 nf个隐向量。而在FM模型中，每一维特征的隐向量只有一个。FM可以看作FFM的特例，是把所有特征都归属到一个field时的FFM模型。根据FFM的field敏感特性，可以导出其模型方程。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-d04fed8047209d53.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>可以看到，如果隐向量的长度为 k，那么FFM的二次参数有 nfk 个，远多于FM模型的 nk个。此外，由于隐向量与field相关，FFM二次项并不能够化简，其预测复杂度是 O(kn^2)。</p>
<p>下面以一个例子简单说明FFM的特征组合方式。输入记录如下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-659e8f0e43d6310d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>这条记录可以编码成5个特征，其中“Genre=Comedy”和“Genre=Drama”属于同一个field，“Price”是数值型，不用One-Hot编码转换。为了方便说明FFM的样本格式，我们将所有的特征和对应的field映射成整数编号。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-d0f6963eb0505c31.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>那么，FFM的组合特征有10项，如下图所示。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-e3da4d35478d62b3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>其中，红色是field编号，蓝色是特征编号。</p>
<h2 id="2、FFM实现细节"><a href="#2、FFM实现细节" class="headerlink" title="2、FFM实现细节"></a>2、FFM实现细节</h2><p>这里讲得只是一种FFM的实现方式，并不是唯一的。</p>
<p><strong>损失函数</strong><br>FFM将问题定义为分类问题，使用的是logistic loss，同时加入了正则项</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-c2df975e6e6a7847.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>什么，这是logisitc loss？第一眼看到我是懵逼的，逻辑回归的损失函数我很熟悉啊，不是长这样的啊？其实是我目光太短浅了。逻辑回归其实是有两种表述方式的损失函数的，取决于你将类别定义为0和1还是1和-1。大家可以参考下下面的文章：<a href="https://www.cnblogs.com/ljygoodgoodstudydaydayup/p/6340129.html。当我们将类别设定为1和-1的时候，逻辑回归的损失函数就是上面的样子。" target="_blank" rel="external">https://www.cnblogs.com/ljygoodgoodstudydaydayup/p/6340129.html。当我们将类别设定为1和-1的时候，逻辑回归的损失函数就是上面的样子。</a></p>
<p><strong>随机梯度下降</strong></p>
<p>训练FFM使用的是随机梯度下降方法，即每次只选一条数据进行训练，这里还有必要补一补梯度下降的知识，梯度下降是有三种方式的，截图取自参考文献3：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-142f546cdaef9e42.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>总给人一种怪怪的感觉。batch为什么是全量的数据呢，哈哈。</p>
<p>#3、tensorflow实现代码</p>
<p>本文代码的github地址：<br><a href="https://github.com/princewen/tensorflow_practice/tree/master/recommendation-FFM-Demo" target="_blank" rel="external">https://github.com/princewen/tensorflow_practice/tree/master/recommendation-FFM-Demo</a></p>
<p>这里我们只讲解一些细节，具体的代码大家可以去github上看：</p>
<p><strong>生成数据</strong><br>这里我没有找到合适的数据，就自己产生了一点数据，数据涉及20维特征，前十维特征是一个field，后十维是一个field:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">def gen_data():</div><div class="line">    labels = [-1,1]</div><div class="line">    y = [np.random.choice(labels,1)[0] for _ in range(all_data_size)]</div><div class="line">    x_field = [i // 10 for i in range(input_x_size)]</div><div class="line">    x = np.random.randint(0,2,size=(all_data_size,input_x_size))</div><div class="line">    return x,y,x_field</div></pre></td></tr></table></figure>
<p><strong>定义权重项</strong><br>在ffm中，有三个权重项，首先是bias，然后是一维特征的权重，最后是交叉特征的权重：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">def createTwoDimensionWeight(input_x_size,field_size,vector_dimension):</div><div class="line">    weights = tf.truncated_normal([input_x_size,field_size,vector_dimension])</div><div class="line"></div><div class="line">    tf_weights = tf.Variable(weights)</div><div class="line"></div><div class="line">    return tf_weights</div><div class="line"></div><div class="line">def createOneDimensionWeight(input_x_size):</div><div class="line">    weights = tf.truncated_normal([input_x_size])</div><div class="line">    tf_weights = tf.Variable(weights)</div><div class="line">    return tf_weights</div><div class="line"></div><div class="line">def createZeroDimensionWeight():</div><div class="line">    weights = tf.truncated_normal([1])</div><div class="line">    tf_weights = tf.Variable(weights)</div><div class="line">    return tf_weights</div></pre></td></tr></table></figure>
<p><strong>计算估计值</strong><br>估计值的计算这里不能项FM一样先将公式化简再来做，对于交叉特征，只能写两重循环，所以对于特别多的特征的情况下，真的计算要爆炸呀！</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div></pre></td><td class="code"><pre><div class="line">def inference(input_x,input_x_field,zeroWeights,oneDimWeights,thirdWeight):</div><div class="line">    &quot;&quot;&quot;计算回归模型输出的值&quot;&quot;&quot;</div><div class="line"></div><div class="line">    secondValue = tf.reduce_sum(tf.multiply(oneDimWeights,input_x,name=&apos;secondValue&apos;))</div><div class="line"></div><div class="line">    firstTwoValue = tf.add(zeroWeights, secondValue, name=&quot;firstTwoValue&quot;)</div><div class="line"></div><div class="line">    thirdValue = tf.Variable(0.0,dtype=tf.float32)</div><div class="line">    input_shape = input_x_size</div><div class="line"></div><div class="line">    for i in range(input_shape):</div><div class="line">        featureIndex1 = I</div><div class="line">        fieldIndex1 = int(input_x_field[I])</div><div class="line">        for j in range(i+1,input_shape):</div><div class="line">            featureIndex2 = j</div><div class="line">            fieldIndex2 = int(input_x_field[j])</div><div class="line">            vectorLeft = tf.convert_to_tensor([[featureIndex1,fieldIndex2,i] for i in range(vector_dimension)])</div><div class="line">            weightLeft = tf.gather_nd(thirdWeight,vectorLeft)</div><div class="line">            weightLeftAfterCut = tf.squeeze(weightLeft)</div><div class="line"></div><div class="line">            vectorRight = tf.convert_to_tensor([[featureIndex2,fieldIndex1,i] for i in range(vector_dimension)])</div><div class="line">            weightRight = tf.gather_nd(thirdWeight,vectorRight)</div><div class="line">            weightRightAfterCut = tf.squeeze(weightRight)</div><div class="line"></div><div class="line">            tempValue = tf.reduce_sum(tf.multiply(weightLeftAfterCut,weightRightAfterCut))</div><div class="line"></div><div class="line">            indices2 = [I]</div><div class="line">            indices3 = [j]</div><div class="line"></div><div class="line">            xi = tf.squeeze(tf.gather_nd(input_x, indices2))</div><div class="line">            xj = tf.squeeze(tf.gather_nd(input_x, indices3))</div><div class="line"></div><div class="line">            product = tf.reduce_sum(tf.multiply(xi, xj))</div><div class="line"></div><div class="line">            secondItemVal = tf.multiply(tempValue, product)</div><div class="line"></div><div class="line">            tf.assign(thirdValue, tf.add(thirdValue, secondItemVal))</div><div class="line"></div><div class="line">    return tf.add(firstTwoValue,thirdValue)</div></pre></td></tr></table></figure>
<p><strong>定义损失函数</strong><br>损失函数我们就用逻辑回归损失函数来算，同时加入正则项：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line">lambda_w = tf.constant(0.001, name=&apos;lambda_w&apos;)</div><div class="line">lambda_v = tf.constant(0.001, name=&apos;lambda_v&apos;)</div><div class="line"></div><div class="line">zeroWeights = createZeroDimensionWeight()</div><div class="line"></div><div class="line">oneDimWeights = createOneDimensionWeight(input_x_size)</div><div class="line"></div><div class="line">thirdWeight = createTwoDimensionWeight(input_x_size,  # 创建二次项的权重变量</div><div class="line">                                       field_size,</div><div class="line">                                       vector_dimension)  # n * f * k</div><div class="line"></div><div class="line">y_ = inference(input_x, trainx_field,zeroWeights,oneDimWeights,thirdWeight)</div><div class="line"></div><div class="line">l2_norm = tf.reduce_sum(</div><div class="line">    tf.add(</div><div class="line">        tf.multiply(lambda_w, tf.pow(oneDimWeights, 2)),</div><div class="line">        tf.reduce_sum(tf.multiply(lambda_v, tf.pow(thirdWeight, 2)),axis=[1,2])</div><div class="line">    )</div><div class="line">)</div><div class="line"></div><div class="line">loss = tf.log(1 + tf.exp(-input_y * y_)) + l2_norm</div><div class="line"></div><div class="line">train_step = tf.train.GradientDescentOptimizer(learning_rate=lr).minimize(loss)</div></pre></td></tr></table></figure>
<p><strong>训练</strong><br>接下来就是训练了，每次只用喂一个数据就好：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">input_x_batch = trainx[t]</div><div class="line">input_y_batch = trainy[t]</div><div class="line">predict_loss,_, steps = sess.run([loss,train_step, global_step],</div><div class="line">                         feed_dict=&#123;input_x: input_x_batch, input_y: input_y_batch&#125;)</div></pre></td></tr></table></figure>
<p>跑的是相当的慢，我们来看看效果吧：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-b599e465a372c134.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<h2 id="参考文章-1"><a href="#参考文章-1" class="headerlink" title="参考文章"></a>参考文章</h2><p>1、<a href="https://tech.meituan.com/deep-understanding-of-ffm-principles-and-practices.html" target="_blank" rel="external">https://tech.meituan.com/deep-understanding-of-ffm-principles-and-practices.html</a><br>2、<a href="https://www.cnblogs.com/ljygoodgoodstudydaydayup/p/6340129.html" target="_blank" rel="external">https://www.cnblogs.com/ljygoodgoodstudydaydayup/p/6340129.html</a><br>3、<a href="https://www.cnblogs.com/pinard/p/5970503.html" target="_blank" rel="external">https://www.cnblogs.com/pinard/p/5970503.html</a></p>
<h1 id="推荐系统遇上深度学习-三-–DeepFM模型理论和实践"><a href="#推荐系统遇上深度学习-三-–DeepFM模型理论和实践" class="headerlink" title="推荐系统遇上深度学习(三)–DeepFM模型理论和实践"></a>推荐系统遇上深度学习(三)–DeepFM模型理论和实践</h1><h2 id="1、背景"><a href="#1、背景" class="headerlink" title="1、背景"></a>1、背景</h2><p><strong>特征组合的挑战</strong><br>对于一个基于CTR预估的推荐系统，最重要的是学习到用户点击行为背后隐含的特征组合。在不同的推荐场景中，低阶组合特征或者高阶组合特征可能都会对最终的CTR产生影响。</p>
<p>之前介绍的因子分解机(Factorization Machines, FM)通过对于每一维特征的隐变量内积来提取特征组合。最终的结果也非常好。但是，虽然理论上来讲FM可以对高阶特征组合进行建模，但实际上因为计算复杂度的原因一般都只用到了二阶特征组合。</p>
<p>那么对于高阶的特征组合来说，我们很自然的想法，通过多层的神经网络即DNN去解决。</p>
<p><strong>DNN的局限</strong><br>下面的图片来自于张俊林教授在AI大会上所使用的PPT。</p>
<p>我们之前也介绍过了，对于离散特征的处理，我们使用的是将特征转换成为one-hot的形式，但是将One-hot类型的特征输入到DNN中，会导致网络参数太多：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-f4363ca2be689dbb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>如何解决这个问题呢，类似于FFM中的思想，将特征分为不同的field：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-5f476d2c5b616232.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>再加两层的全链接层，让Dense Vector进行组合，那么高阶特征的组合就出来了</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-12f3119df69b7b5b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>但是低阶和高阶特征组合隐含地体现在隐藏层中，如果我们希望把低阶特征组合单独建模，然后融合高阶特征组合。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-7e036f56982d323b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>即将DNN与FM进行一个合理的融合：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-2b8d2e22017ad339.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>二者的融合总的来说有两种形式，一是串行结构，二是并行结构</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-cd51e0bd97ab285d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-1118724d47e2c65e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>而我们今天要讲到的DeepFM，就是并行结构中的一种典型代表。</p>
<h2 id="2、DeepFM模型"><a href="#2、DeepFM模型" class="headerlink" title="2、DeepFM模型"></a>2、DeepFM模型</h2><p>我们先来看一下DeepFM的模型结构：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-21fa429e42108e99.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>DeepFM包含两部分：神经网络部分与因子分解机部分，分别负责低阶特征的提取和高阶特征的提取。这两部分<strong>共享同样的输入</strong>。DeepFM的预测结果可以写为：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-7984bc2c7474d6ea.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p><strong>FM部分</strong></p>
<p>FM部分的详细结构如下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-d144aba541c68a34.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>FM部分是一个因子分解机。关于因子分解机可以参阅文章[Rendle, 2010] Steffen Rendle. Factorization machines. In ICDM, 2010.。因为引入了隐变量的原因，对于几乎不出现或者很少出现的隐变量，FM也可以很好的学习。</p>
<p>FM的输出公式为：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-f9af97ad7e0f5b88.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p><strong>深度部分</strong></p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-366d825a661466a3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>深度部分是一个前馈神经网络。与图像或者语音这类输入不同，图像语音的输入一般是连续而且密集的，然而用于CTR的输入一般是及其稀疏的。因此需要重新设计网络结构。具体实现中为，在第一层隐含层之前，引入一个嵌入层来完成将输入向量压缩到低维稠密向量。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-cc075cd266bf2d5f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>嵌入层(embedding layer)的结构如上图所示。当前网络结构有两个有趣的特性，1）尽管不同field的输入长度不同，但是embedding之后向量的长度均为K。2)在FM里得到的隐变量Vik现在作为了嵌入层网络的权重。</p>
<p>这里的第二点如何理解呢，假设我们的k=5，首先，对于输入的一条记录，同一个field 只有一个位置是1，那么在由输入得到dense vector的过程中，输入层只有一个神经元起作用，得到的dense vector其实就是输入层到embedding层该神经元相连的五条线的权重，即vi1，vi2，vi3，vi4，vi5。这五个值组合起来就是我们在FM中所提到的Vi。在FM部分和DNN部分，这一块是共享权重的，对同一个特征来说，得到的Vi是相同的。</p>
<p>有关模型具体如何操作，我们可以通过代码来进一步加深认识。</p>
<h2 id="3、相关知识"><a href="#3、相关知识" class="headerlink" title="3、相关知识"></a>3、相关知识</h2><p>我们先来讲两个代码中会用到的相关知识吧，代码是参考的github上星数最多的DeepFM实现代码。</p>
<p><strong>Gini Normalization</strong><br>代码中将CTR预估问题设定为一个二分类问题，绘制了Gini Normalization来评价不同模型的效果。这个是什么东西，不太懂，百度了很多，发现了一个比较通俗易懂的介绍。</p>
<p>假设我们有下面两组结果，分别表示预测值和实际值：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">predictions = [0.9, 0.3, 0.8, 0.75, 0.65, 0.6, 0.78, 0.7, 0.05, 0.4, 0.4, 0.05, 0.5, 0.1, 0.1]</div><div class="line">actual = [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]</div></pre></td></tr></table></figure>
<p>然后我们将预测值按照从小到大排列，并根据索引序对实际值进行排序：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">Sorted Actual Values [0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1]</div></pre></td></tr></table></figure>
<p>然后，我们可以画出如下的图片：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-295f18796ee0030e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>接下来我们将数据Normalization到0，1之间。并画出45度线。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-b8943ac16560285b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>橙色区域的面积，就是我们得到的Normalization的Gini系数。</p>
<p>这里，由于我们是将预测概率从小到大排的，所以我们希望实际值中的0尽可能出现在前面，因此Normalization的Gini系数越大，分类效果越好。</p>
<p><strong>embedding_lookup</strong><br>在tensorflow中有个embedding_lookup函数，我们可以直接根据一个序号来得到一个词或者一个特征的embedding值，那么他内部其实是包含一个网络结构的，如下图所示：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-53a1ed7584a8bb71.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>假设我们想要找到2的embedding值，这个值其实是输入层第二个神经元与embedding层连线的权重值。</p>
<p>之前有大佬跟我探讨word2vec输入的问题，现在也算是有个比较明确的答案，输入其实就是one-hot Embedding，而word2vec要学习的是new Embedding。</p>
<h2 id="4、代码解析"><a href="#4、代码解析" class="headerlink" title="4、代码解析"></a>4、代码解析</h2><p>好，一贯的风格，先来介绍几个地址：<br>原代码地址：<a href="https://github.com/ChenglongChen/tensorflow-DeepFM" target="_blank" rel="external">https://github.com/ChenglongChen/tensorflow-DeepFM</a><br>本文代码地址：<a href="https://github.com/princewen/tensorflow_practice/tree/master/Basic-DeepFM-model" target="_blank" rel="external">https://github.com/princewen/tensorflow_practice/tree/master/Basic-DeepFM-model</a><br>数据下载地址：<a href="https://www.kaggle.com/c/porto-seguro-safe-driver-prediction" target="_blank" rel="external">https://www.kaggle.com/c/porto-seguro-safe-driver-prediction</a></p>
<p>好了，话不多说，我们来看看代码目录吧，接下来，我们将主要对网络的构建进行介绍，而对数据的处理，流程的控制部分，相信大家根据代码就可以看懂。</p>
<p><strong>项目结构</strong><br>项目结构如下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-55ff1b9e36979f20.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>其实还应该有一个存放data的路径。config.py保存了我们模型的一些配置。DataReader对数据进行处理，得到模型可以使用的输入。DeepFM是我们构建的模型。main是项目的入口。metrics是计算normalized gini系数的代码。</p>
<p><strong>模型输入</strong></p>
<p>模型的输入主要有下面几个部分：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">self.feat_index = tf.placeholder(tf.int32,</div><div class="line">                                 shape=[None,None],</div><div class="line">                                 name=&apos;feat_index&apos;)</div><div class="line">self.feat_value = tf.placeholder(tf.float32,</div><div class="line">                               shape=[None,None],</div><div class="line">                               name=&apos;feat_value&apos;)</div><div class="line"></div><div class="line">self.label = tf.placeholder(tf.float32,shape=[None,1],name=&apos;label&apos;)</div><div class="line">self.dropout_keep_fm = tf.placeholder(tf.float32,shape=[None],name=&apos;dropout_keep_fm&apos;)</div><div class="line">self.dropout_keep_deep = tf.placeholder(tf.float32,shape=[None],name=&apos;dropout_deep_deep&apos;)</div></pre></td></tr></table></figure>
<p><strong>feat_index</strong>是特征的一个序号，主要用于通过embedding_lookup选择我们的embedding。<strong>feat_value</strong>是对应的特征值，如果是离散特征的话，就是1，如果不是离散特征的话，就保留原来的特征值。<strong>label</strong>是实际值。还定义了两个dropout来防止过拟合。</p>
<p><strong>权重构建</strong><br>权重的设定主要有两部分，第一部分是从输入到embedding中的权重，其实也就是我们的dense vector。另一部分就是深度神经网络每一层的权重。第二部分很好理解，我们主要来看看第一部分：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">#embeddings</div><div class="line">weights[&apos;feature_embeddings&apos;] = tf.Variable(</div><div class="line">    tf.random_normal([self.feature_size,self.embedding_size],0.0,0.01),</div><div class="line">    name=&apos;feature_embeddings&apos;)</div><div class="line">weights[&apos;feature_bias&apos;] = tf.Variable(tf.random_normal([self.feature_size,1],0.0,1.0),name=&apos;feature_bias&apos;)</div></pre></td></tr></table></figure>
<p>weights[‘feature_embeddings’] 存放的每一个值其实就是FM中的vik，所以它是F * K的。其中，F代表feture的大小(将离散特征转换成one-hot之后的特征总量),K代表dense vector的大小。</p>
<p>weights[‘feature_bias’]是FM中的一次项的权重。</p>
<p><strong>Embedding part</strong><br>这个部分很简单啦，是根据feat_index选择对应的weights[‘feature_embeddings’]中的embedding值，然后再与对应的feat_value相乘就可以了：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"># model</div><div class="line">self.embeddings = tf.nn.embedding_lookup(self.weights[&apos;feature_embeddings&apos;],self.feat_index) # N * F * K</div><div class="line">feat_value = tf.reshape(self.feat_value,shape=[-1,self.field_size,1])</div><div class="line">self.embeddings = tf.multiply(self.embeddings,feat_value)</div></pre></td></tr></table></figure>
<p><strong>FM part</strong><br>首先来回顾一下我们之前对FM的化简公式，之前去今日头条面试还问到过公式的推导。</p>
<p><img src="http://upload-images.jianshu.io/upload_images/4155986-a9ead5ad8ff9d2d3?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>所以我们的二次项可以根据化简公式轻松的得到，再加上我们的一次项，FM的part就算完了。同时更为方便的是，由于权重共享，我们这里可以直接用<strong>Embedding part</strong>计算出的embeddings来得到我们的二次项：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"># first order term</div><div class="line">self.y_first_order = tf.nn.embedding_lookup(self.weights[&apos;feature_bias&apos;],self.feat_index)</div><div class="line">self.y_first_order = tf.reduce_sum(tf.multiply(self.y_first_order,feat_value),2)</div><div class="line">self.y_first_order = tf.nn.dropout(self.y_first_order,self.dropout_keep_fm[0])</div><div class="line"></div><div class="line"># second order term</div><div class="line"># sum-square-part</div><div class="line">self.summed_features_emb = tf.reduce_sum(self.embeddings,1) # None * k</div><div class="line">self.summed_features_emb_square = tf.square(self.summed_features_emb) # None * K</div><div class="line"></div><div class="line"># squre-sum-part</div><div class="line">self.squared_features_emb = tf.square(self.embeddings)</div><div class="line">self.squared_sum_features_emb = tf.reduce_sum(self.squared_features_emb, 1)  # None * K</div><div class="line"></div><div class="line">#second order</div><div class="line">self.y_second_order = 0.5 * tf.subtract(self.summed_features_emb_square,self.squared_sum_features_emb)</div><div class="line">self.y_second_order = tf.nn.dropout(self.y_second_order,self.dropout_keep_fm[1])</div></pre></td></tr></table></figure>
<p><strong>DNN part</strong><br>DNNpart的话，就是将<strong>Embedding part</strong>的输出再经过几层全链接层：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"># Deep component</div><div class="line">self.y_deep = tf.reshape(self.embeddings,shape=[-1,self.field_size * self.embedding_size])</div><div class="line">self.y_deep = tf.nn.dropout(self.y_deep,self.dropout_keep_deep[0])</div><div class="line"></div><div class="line">for i in range(0,len(self.deep_layers)):</div><div class="line">    self.y_deep = tf.add(tf.matmul(self.y_deep,self.weights[&quot;layer_%d&quot; %i]), self.weights[&quot;bias_%d&quot;%I])</div><div class="line">    self.y_deep = self.deep_layers_activation(self.y_deep)</div><div class="line">    self.y_deep = tf.nn.dropout(self.y_deep,self.dropout_keep_deep[i+1])</div></pre></td></tr></table></figure>
<p>最后，我们要将DNN和FM两部分的输出进行结合：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">concat_input = tf.concat([self.y_first_order, self.y_second_order, self.y_deep], axis=1)</div></pre></td></tr></table></figure>
<p><strong>损失及优化器</strong><br>我们可以使用logloss(如果定义为分类问题)，或者mse(如果定义为预测问题)，以及多种的优化器去进行尝试，这些根据不同的参数设定得到：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line"># loss</div><div class="line">if self.loss_type == &quot;logloss&quot;:</div><div class="line">    self.out = tf.nn.sigmoid(self.out)</div><div class="line">    self.loss = tf.losses.log_loss(self.label, self.out)</div><div class="line">elif self.loss_type == &quot;mse&quot;:</div><div class="line">    self.loss = tf.nn.l2_loss(tf.subtract(self.label, self.out))</div><div class="line"># l2 regularization on weights</div><div class="line">if self.l2_reg &gt; 0:</div><div class="line">    self.loss += tf.contrib.layers.l2_regularizer(</div><div class="line">        self.l2_reg)(self.weights[&quot;concat_projection&quot;])</div><div class="line">    if self.use_deep:</div><div class="line">        for i in range(len(self.deep_layers)):</div><div class="line">            self.loss += tf.contrib.layers.l2_regularizer(</div><div class="line">                self.l2_reg)(self.weights[&quot;layer_%d&quot; % I])</div><div class="line"></div><div class="line"></div><div class="line">if self.optimizer_type == &quot;adam&quot;:</div><div class="line">    self.optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate, beta1=0.9, beta2=0.999,</div><div class="line">                                            epsilon=1e-8).minimize(self.loss)</div><div class="line">elif self.optimizer_type == &quot;adagrad&quot;:</div><div class="line">    self.optimizer = tf.train.AdagradOptimizer(learning_rate=self.learning_rate,</div><div class="line">                                               initial_accumulator_value=1e-8).minimize(self.loss)</div><div class="line">elif self.optimizer_type == &quot;gd&quot;:</div><div class="line">    self.optimizer = tf.train.GradientDescentOptimizer(learning_rate=self.learning_rate).minimize(self.loss)</div><div class="line">elif self.optimizer_type == &quot;momentum&quot;:</div><div class="line">    self.optimizer = tf.train.MomentumOptimizer(learning_rate=self.learning_rate, momentum=0.95).minimize(</div><div class="line">        self.loss)</div></pre></td></tr></table></figure>
<p><strong>模型效果</strong><br>前面提到了，我们用logloss作为损失函数去进行模型的参数更新，但是代码中输出了模型的 Normalization 的 Gini值来进行模型评价，我们可以对比一下(记住，Gini值越大越好呦)：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-908ee89d46240580.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>好啦，本文只是提供一个引子，有关DeepFM更多的知识大家可以更多的进行学习呦。</p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p>1、<a href="http://www.360doc.com/content/17/0315/10/10408243_637001469.shtml" target="_blank" rel="external">http://www.360doc.com/content/17/0315/10/10408243_637001469.shtml</a><br>2、<a href="https://blog.csdn.net/u010665216/article/details/78528261" target="_blank" rel="external">https://blog.csdn.net/u010665216/article/details/78528261</a></p>
<h1 id="推荐系统遇上深度学习-四-–多值离散特征的embedding解决方案"><a href="#推荐系统遇上深度学习-四-–多值离散特征的embedding解决方案" class="headerlink" title="推荐系统遇上深度学习(四)–多值离散特征的embedding解决方案"></a>推荐系统遇上深度学习(四)–多值离散特征的embedding解决方案</h1><h2 id="1、背景-1"><a href="#1、背景-1" class="headerlink" title="1、背景"></a>1、背景</h2><p>在本系列第三篇文章中，在处理DeepFM数据时，由于每一个离散特征只有一个取值，因此我们在处理的过程中，将原始数据处理成了两个文件，一个记录特征的索引，一个记录了特征的值，而每一列，则代表一个离散特征。</p>
<p>但假如，我们某一个离散特征有多个取值呢？举个例子来说，每个人喜欢的NBA球队，有的人可能喜欢火箭和湖人，有的人可能只喜欢勇士，也有的人喜欢骑士、绿军、猛龙等一大堆。对于这种特征，我们本文将其称为多值离散特征。</p>
<p>根据DeepFM的思想，我们需要将每一个field的特征转换为定长的embedding，即使有多个取值，也是要变换成定长的embedding。</p>
<p>那么，一种思路来了，比如一个用户喜欢两个球队，这个field的特征可能是[1,1,0,0,0,0,0…..0]，那么我们使用两次embedding lookup，再取个平均不就好了嘛。</p>
<p>嗯，这的确也许可能是一种思路吧，在tensorflow中，其实有一个函数能够实现我们上述的思路，那就是tf.nn.embedding_lookup_sparse。别着急，我们一步一步来实现多值离散特征的embedding处理过程。</p>
<h2 id="2、解决方案"><a href="#2、解决方案" class="headerlink" title="2、解决方案"></a>2、解决方案</h2><p><strong>输入数据</strong></p>
<p>假设我们有三条数据，每条数据代表一个user所喜欢的nba球员，比如有登哥，炮哥，杜老四，慕斯等等：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">csv = [</div><div class="line">  &quot;1,harden|james|curry&quot;,</div><div class="line">  &quot;2,wrestbrook|harden|durant&quot;,</div><div class="line">  &quot;3,|paul|towns&quot;,</div><div class="line">]</div></pre></td></tr></table></figure>
<p>我们建立一个所有球员的集合：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">TAG_SET = [&quot;harden&quot;, &quot;james&quot;, &quot;curry&quot;, &quot;durant&quot;, &quot;paul&quot;,&quot;towns&quot;,&quot;wrestbrook&quot;]</div></pre></td></tr></table></figure>
<p><strong>数据处理</strong><br>这里我们需要一个得到一个SparseTensor，即多为稀疏矩阵的一种表示方式，我们只记录非0值所在的位置和值。</p>
<p>比如说，下面就是我们对上面数据处理过后的一个SparseTensor，indices是数组中非0元素的下标，values跟indices一一对应，表示该下标位置的值，最后一个表示的是数组的大小。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-4fedbff73c8489ef.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>处理得到SparseTensor的完整代码如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">def sparse_from_csv(csv):</div><div class="line">  ids, post_tags_str = tf.decode_csv(csv, [[-1], [&quot;&quot;]])</div><div class="line">  table = tf.contrib.lookup.index_table_from_tensor(</div><div class="line">      mapping=TAG_SET, default_value=-1) ## 这里构造了个查找表 ##</div><div class="line">  split_tags = tf.string_split(post_tags_str, &quot;|&quot;)</div><div class="line">  return tf.SparseTensor(</div><div class="line">      indices=split_tags.indices,</div><div class="line">      values=table.lookup(split_tags.values), ## 这里给出了不同值通过表查到的index ##</div><div class="line">      dense_shape=split_tags.dense_shape)</div></pre></td></tr></table></figure>
<p><strong>定义embedding变量</strong></p>
<p>定义我们的embedding的大小为3:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">TAG_EMBEDDING_DIM = 3</div><div class="line">embedding_params = tf.Variable(tf.truncated_normal([len(TAG_SET), TAG_EMBEDDING_DIM]))</div></pre></td></tr></table></figure>
<p><strong>得到embedding值</strong></p>
<p>将我们刚才得到的SparseTensor，传入到tf.nn.embedding_lookup_sparse中，我们就可以得到多值离散特征的embedding值。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">tags = sparse_from_csv(csv)</div><div class="line">embedded_tags = tf.nn.embedding_lookup_sparse(embedding_params, sp_ids=tags, sp_weights=None)</div></pre></td></tr></table></figure>
<p>sp_ids就是我们刚刚得到的SparseTensor，而sp_weights=None代表的每一个取值的权重，如果是None的话，所有权重都是1，也就是相当于取了平均。如果不是None的话，我们需要同样传入一个SparseTensor，代表不同球员的喜欢权重。大家感兴趣可以自己去尝试。</p>
<p><strong>测试输出</strong></p>
<p>最后我们来看看得到的效果：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">with tf.Session() as s:</div><div class="line">  s.run([tf.global_variables_initializer(), tf.tables_initializer()])</div><div class="line">  print(s.run([embedded_tags]))</div></pre></td></tr></table></figure>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-3bef2f5a5f87fa26.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>这只是一种解决方案，大家可以去探索更多的方法哟。</p>
<h1 id="推荐系统遇上深度学习-五-–Deep-amp-Cross-Network模型理论和实践"><a href="#推荐系统遇上深度学习-五-–Deep-amp-Cross-Network模型理论和实践" class="headerlink" title="推荐系统遇上深度学习(五)–Deep&amp;Cross-Network模型理论和实践"></a>推荐系统遇上深度学习(五)–Deep&amp;Cross-Network模型理论和实践</h1><h2 id="1、原理"><a href="#1、原理" class="headerlink" title="1、原理"></a>1、原理</h2><p>Deep&amp;Cross Network模型我们下面将简称DCN模型：</p>
<p>一个DCN模型从嵌入和堆积层开始，接着是一个交叉网络和一个与之平行的深度网络，之后是最后的组合层，它结合了两个网络的输出。完整的网络模型如图：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-aa7d9d209c6cf3d5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p><strong>嵌入和堆叠层</strong><br>我们考虑具有离散和连续特征的输入数据。在网络规模推荐系统中，如CTR预测，输入主要是分类特征，如“country=usa”。这些特征通常是编码为独热向量如“[ 0,1,0 ]”；然而，这往往导致过度的高维特征空间大的词汇。 </p>
<p>为了减少维数，我们采用嵌入过程将这些离散特征转换成实数值的稠密向量（通常称为嵌入向量）：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-0ce1a9e0a5d32ca1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>然后，我们将嵌入向量与连续特征向量叠加起来形成一个向量：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-dfc32419b13cac1a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>拼接起来的向量X0将作为我们Cross Network和Deep Network的输入</p>
<p><strong>Cross Network</strong><br>交叉网络的核心思想是以有效的方式应用显式特征交叉。交叉网络由交叉层组成，每个层具有以下公式：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-feacc6f25a1a985d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>一个交叉层的可视化如图所示:</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-8de32b6dab68c108.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>可以看到，交叉网络的特殊结构使交叉特征的程度随着层深度的增加而增大。多项式的最高程度（就输入X0而言）为L层交叉网络L + 1。如果用Lc表示交叉层数，d表示输入维度。然后，参数的数量参与跨网络参数为：d <em> Lc </em> 2 (w和b)</p>
<p>交叉网络的少数参数限制了模型容量。为了捕捉高度非线性的相互作用，模型并行地引入了一个深度网络。</p>
<p><strong>Deep Network</strong></p>
<p>深度网络就是一个全连接的前馈神经网络，每个深度层具有如下公式：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-aa82753a2af4b2cf.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p><strong>Combination Layer</strong></p>
<p>链接层将两个并行网络的输出连接起来，经过一层全链接层得到输出：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-3b2e83dee702d12d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>如果采用的是对数损失函数，那么损失函数形式如下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-6a3cad235da5dd61.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p><strong>总结</strong><br>DCN能够有效地捕获有限度的有效特征的相互作用，学会高度非线性的相互作用，不需要人工特征工程或遍历搜索，并具有较低的计算成本。<br>论文的主要贡献包括：</p>
<p>1）提出了一种新的交叉网络，在每个层上明确地应用特征交叉，有效地学习有界度的预测交叉特征，并且不需要手工特征工程或穷举搜索。<br>2）跨网络简单而有效。通过设计，各层的多项式级数最高，并由层深度决定。网络由所有的交叉项组成，它们的系数各不相同。<br>3）跨网络内存高效，易于实现。<br>4）实验结果表明，交叉网络（DCN）在LogLoss上与DNN相比少了近一个量级的参数量。</p>
<p>这个是从论文中翻译过来的，哈哈。</p>
<h2 id="2、实现解析"><a href="#2、实现解析" class="headerlink" title="2、实现解析"></a>2、实现解析</h2><p>本文的代码根据之前DeepFM的代码进行改进，我们只介绍模型的实现部分，其他数据处理的细节大家可以参考我的github上的代码：<br><a href="https://github.com/princewen/tensorflow_practice/tree/master/Basic-DCN-Demo" target="_blank" rel="external">https://github.com/princewen/tensorflow_practice/tree/master/Basic-DCN-Demo</a></p>
<p>数据下载地址：<a href="https://link.jianshu.com/?t=https%3A%2F%2Fwww.kaggle.com%2Fc%2Fporto-seguro-safe-driver-prediction" target="_blank" rel="external">https://www.kaggle.com/c/porto-seguro-safe-driver-prediction</a></p>
<p>不去下载也没关系，我在github上保留了几千行的数据用作模型测试。</p>
<p><strong>模型输入</strong></p>
<p><strong>模型输入</strong></p>
<p>模型的输入主要有下面几个部分：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">self.feat_index = tf.placeholder(tf.int32,</div><div class="line">                                 shape=[None,None],</div><div class="line">                                 name=&apos;feat_index&apos;)</div><div class="line">self.feat_value = tf.placeholder(tf.float32,</div><div class="line">                               shape=[None,None],</div><div class="line">                               name=&apos;feat_value&apos;)</div><div class="line"></div><div class="line">self.numeric_value = tf.placeholder(tf.float32,[None,None],name=&apos;num_value&apos;)</div><div class="line"></div><div class="line">self.label = tf.placeholder(tf.float32,shape=[None,1],name=&apos;label&apos;)</div><div class="line">self.dropout_keep_deep = tf.placeholder(tf.float32,shape=[None],name=&apos;dropout_deep_deep&apos;)</div></pre></td></tr></table></figure>
<p>可以看到，这里与DeepFM相比，一个明显的变化是将离散特征和连续特征分开，连续特征不在转换成embedding进行输入，所以我们的输入共有五部分。<br><strong>feat_index</strong>是离散特征的一个序号，主要用于通过embedding_lookup选择我们的embedding。<strong>feat_value</strong>是对应离散特征的特征值。<strong>numeric_value</strong>是我们的连续特征值。<strong>label</strong>是实际值。还定义了两个dropout来防止过拟合。</p>
<p><strong>权重构建</strong></p>
<p>权重主要包含四部分，embedding层的权重，cross network中的权重，deep network中的权重以及最后链接层的权重，我们使用一个字典来表示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div></pre></td><td class="code"><pre><div class="line">def _initialize_weights(self):</div><div class="line">weights = dict()</div><div class="line"></div><div class="line">#embeddings</div><div class="line">weights[&apos;feature_embeddings&apos;] = tf.Variable(</div><div class="line">    tf.random_normal([self.cate_feature_size,self.embedding_size],0.0,0.01),</div><div class="line">    name=&apos;feature_embeddings&apos;)</div><div class="line">weights[&apos;feature_bias&apos;] = tf.Variable(tf.random_normal([self.cate_feature_size,1],0.0,1.0),name=&apos;feature_bias&apos;)</div><div class="line"></div><div class="line"></div><div class="line">#deep layers</div><div class="line">num_layer = len(self.deep_layers)</div><div class="line">glorot = np.sqrt(2.0/(self.total_size + self.deep_layers[0]))</div><div class="line"></div><div class="line">weights[&apos;deep_layer_0&apos;] = tf.Variable(</div><div class="line">    np.random.normal(loc=0,scale=glorot,size=(self.total_size,self.deep_layers[0])),dtype=np.float32</div><div class="line">)</div><div class="line">weights[&apos;deep_bias_0&apos;] = tf.Variable(</div><div class="line">    np.random.normal(loc=0,scale=glorot,size=(1,self.deep_layers[0])),dtype=np.float32</div><div class="line">)</div><div class="line"></div><div class="line"></div><div class="line">for i in range(1,num_layer):</div><div class="line">    glorot = np.sqrt(2.0 / (self.deep_layers[i - 1] + self.deep_layers[I]))</div><div class="line">    weights[&quot;deep_layer_%d&quot; % i] = tf.Variable(</div><div class="line">        np.random.normal(loc=0, scale=glorot, size=(self.deep_layers[i - 1], self.deep_layers[i])),</div><div class="line">        dtype=np.float32)  # layers[i-1] * layers[I]</div><div class="line">    weights[&quot;deep_bias_%d&quot; % i] = tf.Variable(</div><div class="line">        np.random.normal(loc=0, scale=glorot, size=(1, self.deep_layers[i])),</div><div class="line">        dtype=np.float32)  # 1 * layer[I]</div><div class="line"></div><div class="line">for i in range(self.cross_layer_num):</div><div class="line"></div><div class="line">    weights[&quot;cross_layer_%d&quot; % i] = tf.Variable(</div><div class="line">        np.random.normal(loc=0, scale=glorot, size=(self.total_size,1)),</div><div class="line">        dtype=np.float32)</div><div class="line">    weights[&quot;cross_bias_%d&quot; % i] = tf.Variable(</div><div class="line">        np.random.normal(loc=0, scale=glorot, size=(self.total_size,1)),</div><div class="line">        dtype=np.float32)  # 1 * layer[I]</div><div class="line"></div><div class="line"># final concat projection layer</div><div class="line"></div><div class="line">input_size = self.total_size + self.deep_layers[-1]</div><div class="line"></div><div class="line">glorot = np.sqrt(2.0/(input_size + 1))</div><div class="line">weights[&apos;concat_projection&apos;] = tf.Variable(np.random.normal(loc=0,scale=glorot,size=(input_size,1)),dtype=np.float32)</div><div class="line">weights[&apos;concat_bias&apos;] = tf.Variable(tf.constant(0.01),dtype=np.float32)</div><div class="line"></div><div class="line">return weights</div></pre></td></tr></table></figure>
<p><strong>计算网络输入</strong><br>这一块我们要计算两个并行网络的输入X0，我们需要将离散特征转换成embedding，同时拼接上连续特征：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"># model</div><div class="line">self.embeddings = tf.nn.embedding_lookup(self.weights[&apos;feature_embeddings&apos;],self.feat_index) # N * F * K</div><div class="line">feat_value = tf.reshape(self.feat_value,shape=[-1,self.field_size,1])</div><div class="line">self.embeddings = tf.multiply(self.embeddings,feat_value)</div><div class="line"></div><div class="line">self.x0 = tf.concat([self.numeric_value,</div><div class="line">                     tf.reshape(self.embeddings,shape=[-1,self.field_size * self.embedding_size])]</div><div class="line">                    ,axis=1)</div></pre></td></tr></table></figure>
<p><strong>Cross Network</strong><br>根据论文中的计算公式，一步步计算得到cross network的输出：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"># cross_part</div><div class="line">self._x0 = tf.reshape(self.x0, (-1, self.total_size, 1))</div><div class="line">x_l = self._x0</div><div class="line">for l in range(self.cross_layer_num):</div><div class="line">    x_l = tf.tensordot(tf.matmul(self._x0, x_l, transpose_b=True),</div><div class="line">                        self.weights[&quot;cross_layer_%d&quot; % l],1) + self.weights[&quot;cross_bias_%d&quot; % l] + x_l</div><div class="line"></div><div class="line">self.cross_network_out = tf.reshape(x_l, (-1, self.total_size))</div></pre></td></tr></table></figure>
<p><strong>Deep Network</strong><br>这一块就是一个多层全链接神经网络：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">self.y_deep = tf.nn.dropout(self.x0,self.dropout_keep_deep[0])</div><div class="line"></div><div class="line">for i in range(0,len(self.deep_layers)):</div><div class="line">    self.y_deep = tf.add(tf.matmul(self.y_deep,self.weights[&quot;deep_layer_%d&quot; %i]), self.weights[&quot;deep_bias_%d&quot;%I])</div><div class="line">    self.y_deep = self.deep_layers_activation(self.y_deep)</div><div class="line">    self.y_deep = tf.nn.dropout(self.y_deep,self.dropout_keep_deep[i+1])</div></pre></td></tr></table></figure>
<p><strong>Combination Layer</strong><br>最后将两个网络的输出拼接起来，经过一层全链接得到最终的输出：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"># concat_part</div><div class="line">concat_input = tf.concat([self.cross_network_out, self.y_deep], axis=1)</div><div class="line"></div><div class="line">self.out = tf.add(tf.matmul(concat_input,self.weights[&apos;concat_projection&apos;]),self.weights[&apos;concat_bias&apos;])</div></pre></td></tr></table></figure>
<p><strong>定义损失</strong><br>这里我们可以选择logloss或者mse，并加上L2正则项：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"># loss</div><div class="line">if self.loss_type == &quot;logloss&quot;:</div><div class="line">    self.out = tf.nn.sigmoid(self.out)</div><div class="line">    self.loss = tf.losses.log_loss(self.label, self.out)</div><div class="line">elif self.loss_type == &quot;mse&quot;:</div><div class="line">    self.loss = tf.nn.l2_loss(tf.subtract(self.label, self.out))</div><div class="line"># l2 regularization on weights</div><div class="line">if self.l2_reg &gt; 0:</div><div class="line">    self.loss += tf.contrib.layers.l2_regularizer(</div><div class="line">        self.l2_reg)(self.weights[&quot;concat_projection&quot;])</div><div class="line">    for i in range(len(self.deep_layers)):</div><div class="line">        self.loss += tf.contrib.layers.l2_regularizer(</div><div class="line">            self.l2_reg)(self.weights[&quot;deep_layer_%d&quot; % I])</div><div class="line">    for i in range(self.cross_layer_num):</div><div class="line">        self.loss += tf.contrib.layers.l2_regularizer(</div><div class="line">            self.l2_reg)(self.weights[&quot;cross_layer_%d&quot; % I])</div></pre></td></tr></table></figure>
<p>剩下的代码就不介绍啦！</p>
<p>好啦，本文只是提供一个引子，有关DCN的知识大家可以更多的进行学习呦。</p>
<h2 id="参考文章："><a href="#参考文章：" class="headerlink" title="参考文章："></a>参考文章：</h2><p>1、<a href="https://blog.csdn.net/roguesir/article/details/79763204" target="_blank" rel="external">https://blog.csdn.net/roguesir/article/details/79763204</a><br>2、论文：<a href="https://arxiv.org/abs/1708.05123" target="_blank" rel="external">https://arxiv.org/abs/1708.05123</a></p>
<h1 id="推荐系统遇上深度学习-六-–PNN模型理论和实践"><a href="#推荐系统遇上深度学习-六-–PNN模型理论和实践" class="headerlink" title="推荐系统遇上深度学习(六)–PNN模型理论和实践"></a>推荐系统遇上深度学习(六)–PNN模型理论和实践</h1><h2 id="1、原理-1"><a href="#1、原理-1" class="headerlink" title="1、原理"></a>1、原理</h2><p>PNN，全称为Product-based Neural Network，认为在embedding输入到MLP之后学习的交叉特征表达并不充分，提出了一种product layer的思想，既基于乘法的运算来体现体征交叉的DNN网络结构，如下图：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-9867a7134749f48e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>按照论文的思路，我们也从上往下来看这个网络结构：</p>
<p><strong>输出层</strong><br>输出层很简单，将上一层的网络输出通过一个全链接层，经过sigmoid函数转换后映射到(0,1)的区间中，得到我们的点击率的预测值：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-c3dc7a8ade52b842.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p><strong>l2层</strong><br>根据l1层的输出，经一个全链接层 ，并使用relu进行激活，得到我们l2的输出结果：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-996cfd8061a5a2fb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p><strong>l1层</strong><br>l1层的输出由如下的公式计算：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-dea545f127da8818.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>重点马上就要来了，我们可以看到在得到l1层输出时，我们输入了三部分，分别是lz，lp 和 b1，b1是我们的偏置项，这里可以先不管。lz和lp的计算就是PNN的精华所在了。我们慢慢道来</p>
<p><strong>Product Layer</strong></p>
<p>product思想来源于，在ctr预估中，认为特征之间的关系更多是一种and“且”的关系，而非add”加”的关系。例如，性别为男且喜欢游戏的人群，比起性别男和喜欢游戏的人群，前者的组合比后者更能体现特征交叉的意义。</p>
<p>product layer可以分成两个部分，一部分是线性部分lz，一部分是非线性部分lp。二者的形式如下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-79596b0e03993e0d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>在这里，我们要使用到论文中所定义的一种运算方式，其实就是矩阵的点乘啦：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-cc22b83064d309cf.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>我们先继续介绍网络结构，有关Product Layer的更详细的介绍，我们在下一章中介绍。</p>
<p><strong>Embedding Layer</strong></p>
<p>Embedding Layer跟DeepFM中相同，将每一个field的特征转换成同样长度的向量，这里用f来表示。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-25ed83f0405ce1d5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p><strong>损失函数</strong><br>使用和逻辑回归同样的损失函数，如下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-6a7520193a39dd2a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<h2 id="2、Product-Layer详细介绍"><a href="#2、Product-Layer详细介绍" class="headerlink" title="2、Product Layer详细介绍"></a>2、Product Layer详细介绍</h2><p>前面提到了，product layer可以分成两个部分，一部分是线性部分lz，一部分是非线性部分lp。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-79596b0e03993e0d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>看上面的公式，我们首先需要知道z和p，这都是由我们的embedding层得到的，其中z是线性信号向量，因此我们直接用embedding层得到：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-6304e98edc03f155.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>论文中使用的等号加一个三角形，其实就是相等的意思，你可以认为z就是embedding层的复制。</p>
<p>对于p来说，这里需要一个公式进行映射：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-e1691579affb9878.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-3da6a7784a8aa7ad.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>不同的g的选择使得我们有了两种PNN的计算方法，一种叫做Inner PNN，简称IPNN，一种叫做Outer PNN，简称OPNN。</p>
<p>接下来，我们分别来具体介绍这两种形式的PNN模型，由于涉及到复杂度的分析，所以我们这里先定义Embedding的大小为M，field的大小为N，而lz和lp的长度为D1。</p>
<h3 id="2-1-IPNN"><a href="#2-1-IPNN" class="headerlink" title="2.1 IPNN"></a>2.1 IPNN</h3><p>IPNN的示意图如下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-efc8f371d4e694a4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>IPNN中p的计算方式如下，即使用内积来代表pij：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-2ac2cd7b351795d8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>所以，pij其实是一个数，得到一个pij的时间复杂度为M，p的大小为N*N，因此计算得到p的时间复杂度为N*N*M。而再由p得到lp的时间复杂度是N*N*D1。因此 对于IPNN来说，总的时间复杂度为N*N(D1+M)。文章对这一结构进行了优化，可以看到，我们的p是一个对称矩阵，因此我们的权重也可以是一个对称矩阵，对称矩阵就可以进行如下的分解：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-69309c37e2b2ba70.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>因此：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-3fce559f6e92c043.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-a4ab3900deca2373.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>因此：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-4ddc93512149a560.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>从而得到：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-5e75fafe9e0d9a14.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>可以看到，我们的权重只需要D1 * N就可以了，时间复杂度也变为了D1*M*N。</p>
<h3 id="2-2-OPNN"><a href="#2-2-OPNN" class="headerlink" title="2.2 OPNN"></a>2.2 OPNN</h3><p>OPNN的示意图如下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-d9924e3ef896dc31.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>OPNN中p的计算方式如下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-badf1326578a3cae.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>此时pij为M*M的矩阵，计算一个pij的时间复杂度为M*M，而p是N*N*M*M的矩阵，因此计算p的事件复杂度为N*N*M*M。从而计算lp的时间复杂度变为D1 * N*N*M*M。这个显然代价很高的。为了减少负责度，论文使用了叠加的思想，它重新定义了p矩阵：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-a66fbf3c57b4d1ab.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>这里计算p的时间复杂度变为了D1*M*(M+N)</p>
<h2 id="3、代码实战"><a href="#3、代码实战" class="headerlink" title="3、代码实战"></a>3、代码实战</h2><p>终于到了激动人心的代码实战环节了，一直想找一个实现比较好的代码，找来找去tensorflow没有什么合适的，倒是pytorch有一个不错的。没办法，只能自己来实现啦，因此本文的代码严格根据论文得到，有不对的的地方或者改进之处还望大家多多指正。</p>
<p>本文的github地址为：<br><a href="https://github.com/princewen/tensorflow_practice/tree/master/Basic-PNN-Demo" target="_blank" rel="external">https://github.com/princewen/tensorflow_practice/tree/master/Basic-PNN-Demo</a>.</p>
<p>本文的代码根据之前DeepFM的代码进行改进，我们只介绍模型的实现部分，其他数据处理的细节大家可以参考我的github上的代码.</p>
<p><strong>模型输入</strong></p>
<p>模型的输入主要有下面几个部分:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">self.feat_index = tf.placeholder(tf.int32,</div><div class="line">                                 shape=[None,None],</div><div class="line">                                 name=&apos;feat_index&apos;)</div><div class="line">self.feat_value = tf.placeholder(tf.float32,</div><div class="line">                               shape=[None,None],</div><div class="line">                               name=&apos;feat_value&apos;)</div><div class="line"></div><div class="line">self.label = tf.placeholder(tf.float32,shape=[None,1],name=&apos;label&apos;)</div><div class="line">self.dropout_keep_deep = tf.placeholder(tf.float32,shape=[None],name=&apos;dropout_deep_deep&apos;)</div></pre></td></tr></table></figure>
<p><strong>feat_index</strong>是特征的一个序号，主要用于通过embedding_lookup选择我们的embedding。<strong>feat_value</strong>是对应的特征值，如果是离散特征的话，就是1，如果不是离散特征的话，就保留原来的特征值。label是实际值。还定义了dropout来防止过拟合。</p>
<p><strong>权重构建</strong></p>
<p>权重由四部分构成，首先是embedding层的权重，然后是product层的权重，有线性信号权重，还有平方信号权重，根据IPNN和OPNN分别定义。最后是Deep Layer各层的权重以及输出层的权重。</p>
<p>对线性信号权重来说，大小为D1 <em> N </em> M<br>对平方信号权重来说，IPNN 的大小为D1 <em> N，OPNN为D1 </em> M * M。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div></pre></td><td class="code"><pre><div class="line">def _initialize_weights(self):</div><div class="line">    weights = dict()</div><div class="line"></div><div class="line">    #embeddings</div><div class="line">    weights[&apos;feature_embeddings&apos;] = tf.Variable(</div><div class="line">        tf.random_normal([self.feature_size,self.embedding_size],0.0,0.01),</div><div class="line">        name=&apos;feature_embeddings&apos;)</div><div class="line">    weights[&apos;feature_bias&apos;] = tf.Variable(tf.random_normal([self.feature_size,1],0.0,1.0),name=&apos;feature_bias&apos;)</div><div class="line"></div><div class="line"></div><div class="line">    #Product Layers</div><div class="line">    if self.use_inner:</div><div class="line">        weights[&apos;product-quadratic-inner&apos;] = tf.Variable(tf.random_normal([self.deep_init_size,self.field_size],0.0,0.01))</div><div class="line">    else:</div><div class="line">        weights[&apos;product-quadratic-outer&apos;] = tf.Variable(</div><div class="line">            tf.random_normal([self.deep_init_size, self.embedding_size,self.embedding_size], 0.0, 0.01))</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line">    weights[&apos;product-linear&apos;] = tf.Variable(tf.random_normal([self.deep_init_size,self.field_size,self.embedding_size],0.0,0.01))</div><div class="line">    weights[&apos;product-bias&apos;] = tf.Variable(tf.random_normal([self.deep_init_size,],0,0,1.0))</div><div class="line">    #deep layers</div><div class="line">    num_layer = len(self.deep_layers)</div><div class="line">    input_size = self.deep_init_size</div><div class="line">    glorot = np.sqrt(2.0/(input_size + self.deep_layers[0]))</div><div class="line"></div><div class="line">    weights[&apos;layer_0&apos;] = tf.Variable(</div><div class="line">        np.random.normal(loc=0,scale=glorot,size=(input_size,self.deep_layers[0])),dtype=np.float32</div><div class="line">    )</div><div class="line">    weights[&apos;bias_0&apos;] = tf.Variable(</div><div class="line">        np.random.normal(loc=0,scale=glorot,size=(1,self.deep_layers[0])),dtype=np.float32</div><div class="line">    )</div><div class="line"></div><div class="line"></div><div class="line">    for i in range(1,num_layer):</div><div class="line">        glorot = np.sqrt(2.0 / (self.deep_layers[i - 1] + self.deep_layers[i]))</div><div class="line">        weights[&quot;layer_%d&quot; % i] = tf.Variable(</div><div class="line">            np.random.normal(loc=0, scale=glorot, size=(self.deep_layers[i - 1], self.deep_layers[i])),</div><div class="line">            dtype=np.float32)  # layers[i-1] * layers[i]</div><div class="line">        weights[&quot;bias_%d&quot; % i] = tf.Variable(</div><div class="line">            np.random.normal(loc=0, scale=glorot, size=(1, self.deep_layers[i])),</div><div class="line">            dtype=np.float32)  # 1 * layer[i]</div><div class="line"></div><div class="line"></div><div class="line">    glorot = np.sqrt(2.0/(input_size + 1))</div><div class="line">    weights[&apos;output&apos;] = tf.Variable(np.random.normal(loc=0,scale=glorot,size=(self.deep_layers[-1],1)),dtype=np.float32)</div><div class="line">    weights[&apos;output_bias&apos;] = tf.Variable(tf.constant(0.01),dtype=np.float32)</div><div class="line"></div><div class="line"></div><div class="line">    return weights</div></pre></td></tr></table></figure>
<p><strong>Embedding Layer</strong><br>这个部分很简单啦，是根据feat_index选择对应的weights[‘feature_embeddings’]中的embedding值，然后再与对应的feat_value相乘就可以了：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"># Embeddings</div><div class="line">self.embeddings = tf.nn.embedding_lookup(self.weights[&apos;feature_embeddings&apos;],self.feat_index) # N * F * K</div><div class="line">feat_value = tf.reshape(self.feat_value,shape=[-1,self.field_size,1])</div><div class="line">self.embeddings = tf.multiply(self.embeddings,feat_value) # N * F * K</div></pre></td></tr></table></figure>
<p><strong>Product Layer</strong><br>根据之前的介绍，我们分别计算线性信号向量，二次信号向量，以及偏置项，三者相加同时经过relu激活得到深度网络部分的输入。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line"># Linear Singal</div><div class="line">linear_output = []</div><div class="line">for i in range(self.deep_init_size):</div><div class="line">    linear_output.append(tf.reshape(</div><div class="line">        tf.reduce_sum(tf.multiply(self.embeddings,self.weights[&apos;product-linear&apos;][i]),axis=[1,2]),shape=(-1,1)))# N * 1</div><div class="line"></div><div class="line">self.lz = tf.concat(linear_output,axis=1) # N * init_deep_size</div><div class="line"></div><div class="line"># Quardatic Singal</div><div class="line">quadratic_output = []</div><div class="line">if self.use_inner:</div><div class="line">    for i in range(self.deep_init_size):</div><div class="line">        theta = tf.multiply(self.embeddings,tf.reshape(self.weights[&apos;product-quadratic-inner&apos;][i],(1,-1,1))) # N * F * K</div><div class="line">        quadratic_output.append(tf.reshape(tf.norm(tf.reduce_sum(theta,axis=1),axis=1),shape=(-1,1))) # N * 1</div><div class="line"></div><div class="line">else:</div><div class="line">    embedding_sum = tf.reduce_sum(self.embeddings,axis=1)</div><div class="line">    p = tf.matmul(tf.expand_dims(embedding_sum,2),tf.expand_dims(embedding_sum,1)) # N * K * K</div><div class="line">    for i in range(self.deep_init_size):</div><div class="line">        theta = tf.multiply(p,tf.expand_dims(self.weights[&apos;product-quadratic-outer&apos;][i],0)) # N * K * K</div><div class="line">        quadratic_output.append(tf.reshape(tf.reduce_sum(theta,axis=[1,2]),shape=(-1,1))) # N * 1</div><div class="line"></div><div class="line">self.lp = tf.concat(quadratic_output,axis=1) # N * init_deep_size</div><div class="line"></div><div class="line">self.y_deep = tf.nn.relu(tf.add(tf.add(self.lz, self.lp), self.weights[&apos;product-bias&apos;]))</div><div class="line">self.y_deep = tf.nn.dropout(self.y_deep, self.dropout_keep_deep[0])</div></pre></td></tr></table></figure>
<p><strong>Deep Part</strong><br>论文中的Deep Part实际上只有一层，不过我们可以随意设置，最后得到输出：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"># Deep component</div><div class="line">for i in range(0,len(self.deep_layers)):</div><div class="line">    self.y_deep = tf.add(tf.matmul(self.y_deep,self.weights[&quot;layer_%d&quot; %i]), self.weights[&quot;bias_%d&quot;%i])</div><div class="line">    self.y_deep = self.deep_layers_activation(self.y_deep)</div><div class="line">    self.y_deep = tf.nn.dropout(self.y_deep,self.dropout_keep_deep[i+1])</div><div class="line">self.out = tf.add(tf.matmul(self.y_deep,self.weights[&apos;output&apos;]),self.weights[&apos;output_bias&apos;])</div></pre></td></tr></table></figure>
<p>剩下的代码就不介绍啦！<br>好啦，本文只是提供一个引子，有关PNN的知识大家可以更多的进行学习呦。</p>
<p><strong>参考文献</strong><br>1 、<a href="https://zhuanlan.zhihu.com/p/33177517" target="_blank" rel="external">https://zhuanlan.zhihu.com/p/33177517</a><br>2、<a href="https://cloud.tencent.com/developer/article/1104673?fromSource=waitui" target="_blank" rel="external">https://cloud.tencent.com/developer/article/1104673?fromSource=waitui</a><br>3、<a href="https://arxiv.org/abs/1611.00144" target="_blank" rel="external">https://arxiv.org/abs/1611.00144</a></p>
<h1 id="推荐系统遇上深度学习-七-–NFM模型理论和实践"><a href="#推荐系统遇上深度学习-七-–NFM模型理论和实践" class="headerlink" title="推荐系统遇上深度学习(七)–NFM模型理论和实践"></a>推荐系统遇上深度学习(七)–NFM模型理论和实践</h1><h2 id="1、引言"><a href="#1、引言" class="headerlink" title="1、引言"></a>1、引言</h2><p>在CTR预估中，为了解决稀疏特征的问题，学者们提出了FM模型来建模特征之间的交互关系。但是FM模型只能表达特征之间两两组合之间的关系，无法建模两个特征之间深层次的关系或者说多个特征之间的交互关系，因此学者们通过Deep Network来建模更高阶的特征之间的关系。</p>
<p>因此 FM和深度网络DNN的结合也就成为了CTR预估问题中主流的方法。有关FM和DNN的结合有两种主流的方法，并行结构和串行结构。两种结构的理解以及实现如下表所示：</p>
<table>
<thead>
<tr>
<th>结构</th>
<th style="text-align:center">描述</th>
<th style="text-align:center">常见模型</th>
</tr>
</thead>
<tbody>
<tr>
<td>并行结构</td>
<td style="text-align:center">FM部分和DNN部分分开计算，只在输出层进行一次融合得到结果</td>
<td style="text-align:center">DeepFM，DCN，Wide&amp;Deep</td>
</tr>
<tr>
<td>串行结构</td>
<td style="text-align:center">将FM的一次项和二次项结果(或其中之一)作为DNN部分的输入，经DNN得到最终结果</td>
<td style="text-align:center">PNN,NFM,AFM</td>
</tr>
</tbody>
</table>
<p>今天介绍的NFM模型(Neural Factorization Machine)，便是串行结构中一种较为简单的网络模型。</p>
<h2 id="2、NFM模型介绍"><a href="#2、NFM模型介绍" class="headerlink" title="2、NFM模型介绍"></a>2、NFM模型介绍</h2><p>我们首先来回顾一下FM模型，FM模型用n个隐变量来刻画特征之间的交互关系。这里要强调的一点是，n是特征的总数，是one-hot展开之后的，比如有三组特征，两个连续特征，一个离散特征有5个取值，那么n=7而不是n=3.</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-23e359033294a7aa.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>顺便回顾一下化简过程：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/4155986-e7e81f16136ad08f?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>可以看到，不考虑最外层的求和，我们可以得到一个K维的向量。</p>
<p>对于NFM模型，目标值的预测公式变为：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-186da78f1c6e9564.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>其中，f(x)是用来建模特征之间交互关系的多层前馈神经网络模块，架构图如下所示：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-85c1002f1da19bfe.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p><strong>Embedding Layer</strong>和我们之间几个网络是一样的，embedding 得到的vector其实就是我们在FM中要学习的隐变量v。</p>
<p><strong>Bi-Interaction Layer</strong>名字挺高大上的，其实它就是计算FM中的二次项的过程，因此得到的向量维度就是我们的Embedding的维度。最终的结果是：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-7128a3adfeb2f70c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>Hidden Layers就是我们的DNN部分，将Bi-Interaction Layer得到的结果接入多层的神经网络进行训练，从而捕捉到特征之间复杂的非线性关系。</p>
<p>在进行多层训练之后，将最后一层的输出求和同时加上一次项和偏置项，就得到了我们的预测输出：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-4b93c5f2497ba0c4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>是不是很简单呢，哈哈。</p>
<h2 id="3、代码实战-1"><a href="#3、代码实战-1" class="headerlink" title="3、代码实战"></a>3、代码实战</h2><p>终于到了激动人心的代码实战环节了，本文的代码有不对的的地方或者改进之处还望大家多多指正。</p>
<p>本文的github地址为：<br><a href="https://github.com/princewen/tensorflow_practice/tree/master/recommendation/Basic-NFM-Demo" target="_blank" rel="external">https://github.com/princewen/tensorflow_practice/tree/master/recommendation/Basic-NFM-Demo</a></p>
<p>本文的代码根据之前DeepFM的代码进行改进，我们只介绍模型的实现部分，其他数据处理的细节大家可以参考我的github上的代码.</p>
<p><strong>模型输入</strong></p>
<p>模型的输入主要有下面几个部分:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">self.feat_index = tf.placeholder(tf.int32,</div><div class="line">                                 shape=[None,None],</div><div class="line">                                 name=&apos;feat_index&apos;)</div><div class="line">self.feat_value = tf.placeholder(tf.float32,</div><div class="line">                               shape=[None,None],</div><div class="line">                               name=&apos;feat_value&apos;)</div><div class="line"></div><div class="line">self.label = tf.placeholder(tf.float32,shape=[None,1],name=&apos;label&apos;)</div><div class="line">self.dropout_keep_deep = tf.placeholder(tf.float32,shape=[None],name=&apos;dropout_deep_deep&apos;)</div></pre></td></tr></table></figure>
<p><strong>feat_index</strong>是特征的一个序号，主要用于通过embedding_lookup选择我们的embedding。<strong>feat_value</strong>是对应的特征值，如果是离散特征的话，就是1，如果不是离散特征的话，就保留原来的特征值。label是实际值。还定义了dropout来防止过拟合。</p>
<p><strong>权重构建</strong></p>
<p>权重主要分以下几部分，偏置项，一次项权重，embeddings，以及DNN的权重</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div></pre></td><td class="code"><pre><div class="line">def _initialize_weights(self):</div><div class="line">    weights = dict()</div><div class="line"></div><div class="line">    #embeddings</div><div class="line">    weights[&apos;feature_embeddings&apos;] = tf.Variable(</div><div class="line">        tf.random_normal([self.feature_size,self.embedding_size],0.0,0.01),</div><div class="line">        name=&apos;feature_embeddings&apos;)</div><div class="line">    weights[&apos;feature_bias&apos;] = tf.Variable(tf.random_normal([self.feature_size,1],0.0,1.0),name=&apos;feature_bias&apos;)</div><div class="line">    weights[&apos;bias&apos;] = tf.Variable(tf.constant(0.1),name=&apos;bias&apos;)</div><div class="line"></div><div class="line">    #deep layers</div><div class="line">    num_layer = len(self.deep_layers)</div><div class="line">    input_size = self.embedding_size</div><div class="line">    glorot = np.sqrt(2.0/(input_size + self.deep_layers[0]))</div><div class="line"></div><div class="line">    weights[&apos;layer_0&apos;] = tf.Variable(</div><div class="line">        np.random.normal(loc=0,scale=glorot,size=(input_size,self.deep_layers[0])),dtype=np.float32</div><div class="line">    )</div><div class="line">    weights[&apos;bias_0&apos;] = tf.Variable(</div><div class="line">        np.random.normal(loc=0,scale=glorot,size=(1,self.deep_layers[0])),dtype=np.float32</div><div class="line">    )</div><div class="line"></div><div class="line"></div><div class="line">    for i in range(1,num_layer):</div><div class="line">        glorot = np.sqrt(2.0 / (self.deep_layers[i - 1] + self.deep_layers[i]))</div><div class="line">        weights[&quot;layer_%d&quot; % i] = tf.Variable(</div><div class="line">            np.random.normal(loc=0, scale=glorot, size=(self.deep_layers[i - 1], self.deep_layers[i])),</div><div class="line">            dtype=np.float32)  # layers[i-1] * layers[i]</div><div class="line">        weights[&quot;bias_%d&quot; % i] = tf.Variable(</div><div class="line">            np.random.normal(loc=0, scale=glorot, size=(1, self.deep_layers[i])),</div><div class="line">            dtype=np.float32)  # 1 * layer[i]</div><div class="line"></div><div class="line">    return weights</div></pre></td></tr></table></figure>
<p><strong>Embedding Layer</strong><br>这个部分很简单啦，是根据feat_index选择对应的weights[‘feature_embeddings’]中的embedding值，然后再与对应的feat_value相乘就可以了：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"># Embeddings</div><div class="line">self.embeddings = tf.nn.embedding_lookup(self.weights[&apos;feature_embeddings&apos;],self.feat_index) # N * F * K</div><div class="line">feat_value = tf.reshape(self.feat_value,shape=[-1,self.field_size,1])</div><div class="line">self.embeddings = tf.multiply(self.embeddings,feat_value) # N * F * K</div></pre></td></tr></table></figure>
<p><strong>Bi-Interaction Layer</strong><br>我们直接根据化简后的结果进行计算，得到一个K维的向量：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"># sum-square-part</div><div class="line">self.summed_features_emb = tf.reduce_sum(self.embeddings, 1)  # None * k</div><div class="line">self.summed_features_emb_square = tf.square(self.summed_features_emb)  # None * K</div><div class="line"></div><div class="line"># squre-sum-part</div><div class="line">self.squared_features_emb = tf.square(self.embeddings)</div><div class="line">self.squared_sum_features_emb = tf.reduce_sum(self.squared_features_emb, 1)  # None * K</div><div class="line"></div><div class="line"># second order</div><div class="line">self.y_second_order = 0.5 * tf.subtract(self.summed_features_emb_square, self.squared_sum_features_emb)</div></pre></td></tr></table></figure>
<p><strong>Deep Part</strong><br>将Bi-Interaction Layer层得到的结果经过一个多层的神经网络，得到交互项的输出：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">self.y_deep = self.y_second_order</div><div class="line">for i in range(0, len(self.deep_layers)):</div><div class="line">    self.y_deep = tf.add(tf.matmul(self.y_deep, self.weights[&quot;layer_%d&quot; % i]), self.weights[&quot;bias_%d&quot; % i])</div><div class="line">    self.y_deep = self.deep_layers_activation(self.y_deep)</div><div class="line">    self.y_deep = tf.nn.dropout(self.y_deep, self.dropout_keep_deep[i + 1])</div></pre></td></tr></table></figure>
<p><strong>得到预测输出</strong><br>为了得到预测输出，我们还需要两部分，分别是偏置项和一次项：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"># first order term</div><div class="line">self.y_first_order = tf.nn.embedding_lookup(self.weights[&apos;feature_bias&apos;], self.feat_index)</div><div class="line">self.y_first_order = tf.reduce_sum(tf.multiply(self.y_first_order, feat_value), 2)</div><div class="line"></div><div class="line"># bias</div><div class="line">self.y_bias = self.weights[&apos;bias&apos;] * tf.ones_like(self.label)</div></pre></td></tr></table></figure>
<p>而我们的最终输出如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"># out</div><div class="line">self.out = tf.add_n([tf.reduce_sum(self.y_first_order,axis=1,keep_dims=True),</div><div class="line">                     tf.reduce_sum(self.y_deep,axis=1,keep_dims=True),</div><div class="line">                     self.y_bias])</div></pre></td></tr></table></figure>
<p>剩下的代码就不介绍啦！<br>好啦，本文只是提供一个引子，有关NFM的知识大家可以更多的进行学习呦。</p>
<h2 id="4、小结"><a href="#4、小结" class="headerlink" title="4、小结"></a>4、小结</h2><p>NFM模型将FM与神经网络结合以提升FM捕捉特征间多阶交互信息的能力。根据论文中实验结果，NFM的预测准确度相较FM有明显提升，并且与现有的并行神经网络模型相比，复杂度更低。</p>
<p>NFM本质上还是基于FM，FM会让一个特征固定一个特定的向量，当这个特征与其他特征做交叉时，都是用同样的向量去做计算。这个是很不合理的，因为不同的特征之间的交叉，重要程度是不一样的。因此，学者们提出了AFM模型（Attentional factorization machines），将attention机制加入到我们的模型中，关于AFM的知识，我们下一篇来一探究竟。</p>
<h1 id="推荐系统遇上深度学习-八-–AFM模型理论和实践"><a href="#推荐系统遇上深度学习-八-–AFM模型理论和实践" class="headerlink" title="推荐系统遇上深度学习(八)–AFM模型理论和实践"></a>推荐系统遇上深度学习(八)–AFM模型理论和实践</h1><h2 id="1、引言-1"><a href="#1、引言-1" class="headerlink" title="1、引言"></a>1、引言</h2><p>在CTR预估中，为了解决稀疏特征的问题，学者们提出了FM模型来建模特征之间的交互关系。但是FM模型只能表达特征之间两两组合之间的关系，无法建模两个特征之间深层次的关系或者说多个特征之间的交互关系，因此学者们通过Deep Network来建模更高阶的特征之间的关系。</p>
<p>因此 FM和深度网络DNN的结合也就成为了CTR预估问题中主流的方法。有关FM和DNN的结合有两种主流的方法，并行结构和串行结构。两种结构的理解以及实现如下表所示：</p>
<table>
<thead>
<tr>
<th>结构</th>
<th style="text-align:center">描述</th>
<th style="text-align:center">常见模型</th>
</tr>
</thead>
<tbody>
<tr>
<td>并行结构</td>
<td style="text-align:center">FM部分和DNN部分分开计算，只在输出层进行一次融合得到结果</td>
<td style="text-align:center">DeepFM，DCN，Wide&amp;Deep</td>
</tr>
<tr>
<td>串行结构</td>
<td style="text-align:center">将FM的一次项和二次项结果(或其中之一)作为DNN部分的输入，经DNN得到最终结果</td>
<td style="text-align:center">PNN,NFM,AFM</td>
</tr>
</tbody>
</table>
<p>今天介绍的AFM模型(Attentional Factorization Machine)，便是串行结构中一种网络模型。</p>
<h2 id="2、AFM模型介绍"><a href="#2、AFM模型介绍" class="headerlink" title="2、AFM模型介绍"></a>2、AFM模型介绍</h2><p>我们首先来回顾一下FM模型，FM模型用n个隐变量来刻画特征之间的交互关系。这里要强调的一点是，n是特征的总数，是one-hot展开之后的，比如有三组特征，两个连续特征，一个离散特征有5个取值，那么n=7而不是n=3.</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-23e359033294a7aa.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>顺便回顾一下化简过程：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/4155986-e7e81f16136ad08f?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>可以看到，不考虑最外层的求和，我们可以得到一个K维的向量。</p>
<p>不难发现，在进行预测时，FM会让一个特征固定一个特定的向量，当这个特征与其他特征做交叉时，都是用同样的向量去做计算。这个是很不合理的，因为不同的特征之间的交叉，重要程度是不一样的。如何体现这种重要程度，之前介绍的FFM模型是一个方案。另外，结合了attention机制的AFM模型，也是一种解决方案。</p>
<p>关于什么是attention model？本文不打算详细赘述，我们这里只需要知道的是，attention机制相当于一个加权平均，attention的值就是其中权重，判断不同特征之间交互的重要性。</p>
<p>刚才提到了，attention相等于加权的过程，因此我们的预测公式变为：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-87fcfdc28ceafb38.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>圆圈中有个点的符号代表的含义是element-wise product，即：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-ce7c934ad3c8a4d2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>因此，我们在求和之后得到的是一个K维的向量，还需要跟一个向量p相乘，得到一个具体的数值。</p>
<p>可以看到，AFM的前两部分和FM相同，后面的一项经由如下的网络得到：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-5b337dd109d82093.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>图中的前三部分：sparse iput，embedding layer，pair-wise interaction layer，都和FM是一样的。而后面的两部分，则是AFM的创新所在，也就是我们的Attention net。Attention背后的数学公式如下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-f84e680b632b12cc.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>总结一下，不难看出AFM只是在FM的基础上添加了attention的机制，但是实际上，由于最后的加权累加，二次项并没有进行更深的网络去学习非线性交叉特征，所以AFM并没有发挥出DNN的优势，也许结合DNN可以达到更好的结果。</p>
<h2 id="3、代码实现"><a href="#3、代码实现" class="headerlink" title="3、代码实现"></a>3、代码实现</h2><p>终于到了激动人心的代码实战环节了，本文的代码有不对的的地方或者改进之处还望大家多多指正。</p>
<p>本文的github地址为：<br><a href="https://github.com/princewen/tensorflow_practice/tree/master/recommendation/Basic-AFM-Demo" target="_blank" rel="external">https://github.com/princewen/tensorflow_practice/tree/master/recommendation/Basic-AFM-Demo</a></p>
<p>本文的代码根据之前DeepFM的代码进行改进，我们只介绍模型的实现部分，其他数据处理的细节大家可以参考我的github上的代码.</p>
<p>在介绍之前，我们先定义几个维度，方便下面的介绍：<br><strong>Embedding Size：K</strong><br><strong>Batch Size：N</strong><br><strong>Attention Size ：A</strong><br><strong>Field Size (这里是field size 不是feature size！！！！）： F</strong></p>
<p><strong>模型输入</strong></p>
<p>模型的输入主要有下面几个部分:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">self.feat_index = tf.placeholder(tf.int32,</div><div class="line">                                 shape=[None,None],</div><div class="line">                                 name=&apos;feat_index&apos;)</div><div class="line">self.feat_value = tf.placeholder(tf.float32,</div><div class="line">                               shape=[None,None],</div><div class="line">                               name=&apos;feat_value&apos;)</div><div class="line"></div><div class="line">self.label = tf.placeholder(tf.float32,shape=[None,1],name=&apos;label&apos;)</div><div class="line">self.dropout_keep_deep = tf.placeholder(tf.float32,shape=[None],name=&apos;dropout_deep_deep&apos;)</div></pre></td></tr></table></figure>
<p><strong>feat_index</strong>是特征的一个序号，主要用于通过embedding_lookup选择我们的embedding。<strong>feat_value</strong>是对应的特征值，如果是离散特征的话，就是1，如果不是离散特征的话，就保留原来的特征值。label是实际值。还定义了dropout来防止过拟合。</p>
<p><strong>权重构建</strong></p>
<p>权重主要分以下几部分，偏置项，一次项权重，embeddings，以及Attention部分的权重。除Attention部分的权重如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">def _initialize_weights(self):</div><div class="line">    weights = dict()</div><div class="line"></div><div class="line">    #embeddings</div><div class="line">    weights[&apos;feature_embeddings&apos;] = tf.Variable(</div><div class="line">        tf.random_normal([self.feature_size,self.embedding_size],0.0,0.01),</div><div class="line">        name=&apos;feature_embeddings&apos;)</div><div class="line">    weights[&apos;feature_bias&apos;] = tf.Variable(tf.random_normal([self.feature_size,1],0.0,1.0),name=&apos;feature_bias&apos;)</div><div class="line">    weights[&apos;bias&apos;] = tf.Variable(tf.constant(0.1),name=&apos;bias&apos;)</div></pre></td></tr></table></figure>
<p>Attention部分的权重我们详细介绍一下，这里共有四个部分，分别对应公式中的w，b，h和p。</p>
<p>weights[‘attention_w’] 的维度为 K <em> A，<br>weights[‘attention_b’] 的维度为 A，<br>weights[‘attention_h’] 的维度为 A，<br>weights[‘attention_p’] 的维度为 K </em> 1</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"># attention part</div><div class="line">glorot = np.sqrt(2.0 / (self.attention_size + self.embedding_size))</div><div class="line"></div><div class="line">weights[&apos;attention_w&apos;] = tf.Variable(np.random.normal(loc=0,scale=glorot,size=(self.embedding_size,self.attention_size)),</div><div class="line">                                     dtype=tf.float32,name=&apos;attention_w&apos;)</div><div class="line"></div><div class="line">weights[&apos;attention_b&apos;] = tf.Variable(np.random.normal(loc=0,scale=glorot,size=(self.attention_size,)),</div><div class="line">                                     dtype=tf.float32,name=&apos;attention_b&apos;)</div><div class="line"></div><div class="line">weights[&apos;attention_h&apos;] = tf.Variable(np.random.normal(loc=0,scale=1,size=(self.attention_size,)),</div><div class="line">                                     dtype=tf.float32,name=&apos;attention_h&apos;)</div><div class="line"></div><div class="line"></div><div class="line">weights[&apos;attention_p&apos;] = tf.Variable(np.ones((self.embedding_size,1)),dtype=np.float32)</div></pre></td></tr></table></figure>
<p><strong>Embedding Layer</strong><br>这个部分很简单啦，是根据feat_index选择对应的weights[‘feature_embeddings’]中的embedding值，然后再与对应的feat_value相乘就可以了：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"># Embeddings</div><div class="line">self.embeddings = tf.nn.embedding_lookup(self.weights[&apos;feature_embeddings&apos;],self.feat_index) # N * F * K</div><div class="line">feat_value = tf.reshape(self.feat_value,shape=[-1,self.field_size,1])</div><div class="line">self.embeddings = tf.multiply(self.embeddings,feat_value) # N * F * K</div></pre></td></tr></table></figure>
<p><strong>Attention Net</strong><br>Attention部分的实现严格按照上面给出的数学公式：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-f84e680b632b12cc.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>这里我们一步步来实现。</p>
<p>对于得到的embedding向量，我们首先需要两两计算其element-wise-product。即：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-ea8d47e89b49164d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>通过嵌套循环的方式得到的结果需要通过stack将其变为一个tenser，此时的维度为(F <em> F - 1 / 2) </em> N<em> K，因此我们需要一个转置操作，来得到维度为 N </em> (F <em> F - 1 / 2) </em> K的element-wize-product结果。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">element_wise_product_list = []</div><div class="line">for i in range(self.field_size):</div><div class="line">    for j in range(i+1,self.field_size):</div><div class="line">        element_wise_product_list.append(tf.multiply(self.embeddings[:,i,:],self.embeddings[:,j,:])) # None * K</div><div class="line"></div><div class="line">self.element_wise_product = tf.stack(element_wise_product_list) # (F * F - 1 / 2) * None * K</div><div class="line">self.element_wise_product = tf.transpose(self.element_wise_product,perm=[1,0,2],name=&apos;element_wise_product&apos;) # None * (F * F - 1 / 2) *  K</div></pre></td></tr></table></figure>
<p>得到了element-wise-product之后，我们接下来计算：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-c7908e1e8cdaf5e2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>计算之前，我们需要先对element-wise-product进行reshape，将其变为二维的tensor，在计算完之后再变换回三维tensor，此时的维度为 N <em> (F </em> F - 1 / 2) * A：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">self.attention_wx_plus_b = tf.reshape(tf.add(tf.matmul(tf.reshape(self.element_wise_product,shape=(-1,self.embedding_size)),</div><div class="line">                                                       self.weights[&apos;attention_w&apos;]),</div><div class="line">                                             self.weights[&apos;attention_b&apos;]),</div><div class="line">                                      shape=[-1,num_interactions,self.attention_size]) # N * ( F * F - 1 / 2) * A</div></pre></td></tr></table></figure>
<p>然后我们计算：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-d4972faaee7094cf.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>此时的维度为 N <em>  ( F </em> F - 1 / 2)  * 1</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">self.attention_exp = tf.exp(tf.reduce_sum(tf.multiply(tf.nn.relu(self.attention_wx_plus_b),</div><div class="line">                                               self.weights[&apos;attention_h&apos;]),</div><div class="line">                                   axis=2,keep_dims=True)) # N * ( F * F - 1 / 2) * 1</div></pre></td></tr></table></figure>
<p>然后计算：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-11fd2aaba03b027a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>这一层相当于softmax了，不过我们还是用基本的方式写出来：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">self.attention_exp_sum = tf.reduce_sum(self.attention_exp,axis=1,keep_dims=True) # N * 1 * 1</div><div class="line"></div><div class="line">self.attention_out = tf.div(self.attention_exp,self.attention_exp_sum,name=&apos;attention_out&apos;)  # N * ( F * F - 1 / 2) * 1</div></pre></td></tr></table></figure>
<p>最后，我们计算得到经attention net加权后的二次项结果：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-06ad603c5de44b60.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">self.attention_x_product = tf.reduce_sum(tf.multiply(self.attention_out,self.element_wise_product),axis=1,name=&apos;afm&apos;) # N * K</div><div class="line"></div><div class="line">self.attention_part_sum = tf.matmul(self.attention_x_product,self.weights[&apos;attention_p&apos;]) # N * 1</div></pre></td></tr></table></figure>
<p><strong>得到预测输出</strong><br>为了得到预测输出，除Attention part的输出外，我们还需要两部分，分别是偏置项和一次项：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"># first order term</div><div class="line">self.y_first_order = tf.nn.embedding_lookup(self.weights[&apos;feature_bias&apos;], self.feat_index)</div><div class="line">self.y_first_order = tf.reduce_sum(tf.multiply(self.y_first_order, feat_value), 2)</div><div class="line"></div><div class="line"># bias</div><div class="line">self.y_bias = self.weights[&apos;bias&apos;] * tf.ones_like(self.label)</div></pre></td></tr></table></figure>
<p>而我们的最终输出如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"># out</div><div class="line">self.out = tf.add_n([tf.reduce_sum(self.y_first_order,axis=1,keep_dims=True),</div><div class="line">                     self.attention_part_sum,</div><div class="line">                     self.y_bias],name=&apos;out_afm&apos;)</div></pre></td></tr></table></figure>
<p>剩下的代码就不介绍啦！<br>好啦，本文只是提供一个引子，有关AFM的知识大家可以更多的进行学习呦。</p>
<p>#参考文献：<br><a href="https://zhuanlan.zhihu.com/p/33540686" target="_blank" rel="external">https://zhuanlan.zhihu.com/p/33540686</a></p>
<h1 id="推荐系统遇上深度学习-九-–评价指标AUC原理及实践"><a href="#推荐系统遇上深度学习-九-–评价指标AUC原理及实践" class="headerlink" title="推荐系统遇上深度学习(九)–评价指标AUC原理及实践"></a>推荐系统遇上深度学习(九)–评价指标AUC原理及实践</h1><h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>CTR问题我们有两种角度去理解，一种是分类的角度，即将点击和未点击作为两种类别。另一种是回归的角度，将点击和未点击作为回归的值。不管是分类问题还是回归问题，一般在预估的时候都是得到一个[0,1]之间的概率值，代表点击的可能性的大小。</p>
<p>如果将CTR预估问题当作回归问题，我们经常使用的损失函数是MSE；如果当作二分类问题，我们经常使用的损失函数是LogLoss。而对于一个训练好的模型，我们往往需要评估一下模型的效果，或者说泛化能力，MSE和LogLoss当然也可以作为我们的评价指标，但除此之外，我们最常用的还是AUC。</p>
<p>想到这里，我想到一个问题，AUC是否可以直接用作损失函数去优化呢？可以参考知乎的文章，还没太搞懂：<a href="https://www.zhihu.com/question/39840928" target="_blank" rel="external">https://www.zhihu.com/question/39840928</a></p>
<p>说了这么多，我们还不知道AUC是什么呢？不着急，我们从二分类的评估指标慢慢说起，提醒一下，本文二分类的类别均为0和1，1代表正例，0代表负例。</p>
<h2 id="1、从二分类评估指标说起"><a href="#1、从二分类评估指标说起" class="headerlink" title="1、从二分类评估指标说起"></a>1、从二分类评估指标说起</h2><h4 id="1-1-混淆矩阵"><a href="#1-1-混淆矩阵" class="headerlink" title="1.1 混淆矩阵"></a>1.1 混淆矩阵</h4><p>我们首先来看一下混淆矩阵，对于二分类问题，真实的样本标签有两类，我们学习器预测的类别有两类，那么根据二者的类别组合可以划分为四组，如下表所示：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-31296ad3c9f891e6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>上表即为<strong>混淆矩阵</strong>，其中，行表示预测的label值，列表示真实label值。TP，FP，FN，TN分别表示如下意思：</p>
<p><strong>TP（true positive）</strong>：表示样本的真实类别为正，最后预测得到的结果也为正；<br><strong>FP（false positive）</strong>：表示样本的真实类别为负，最后预测得到的结果却为正；<br><strong>FN（false negative）</strong>：表示样本的真实类别为正，最后预测得到的结果却为负；<br><strong>TN（true negative）</strong>：表示样本的真实类别为负，最后预测得到的结果也为负.</p>
<p>可以看到，TP和TN是我们预测准确的样本，而FP和FN为我们预测错误的样本。</p>
<h4 id="1-2-准确率Accruacy"><a href="#1-2-准确率Accruacy" class="headerlink" title="1.2 准确率Accruacy"></a>1.2 准确率Accruacy</h4><p>准确率表示的是分类正确的样本数占样本总数的比例，假设我们预测了10条样本，有8条的预测正确，那么准确率即为80%。</p>
<p>用混淆矩阵计算的话，准确率可以表示为：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-3e988dcab6079c76.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>虽然准确率可以在一定程度上评价我们的分类器的性能，不过对于二分类问题或者说CTR预估问题，样本是极其不平衡的。对于大数据集来说，标签为1的正样本数据往往不足10%，那么如果分类器将所有样本判别为负样本，那么仍然可以达到90%以上的分类准确率，但这个分类器的性能显然是非常差的。</p>
<h4 id="1-3-精确率Precision和召回率Recall"><a href="#1-3-精确率Precision和召回率Recall" class="headerlink" title="1.3 精确率Precision和召回率Recall"></a>1.3 精确率Precision和召回率Recall</h4><p>为了衡量分类器对正样本的预测能力，我们引入了<strong>精确率Precision和召回率Recall</strong>。</p>
<p><strong>精确率</strong>表示预测结果中，预测为正样本的样本中，正确预测为正样本的概率；<br><strong>召回率</strong>表示在原始样本的正样本中，最后被正确预测为正样本的概率；</p>
<p>二者用混淆矩阵计算如下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-e770d9c378f64925.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>精确率和召回率往往是一对矛盾的指标。在CTR预估问题中，预测结果往往表示会被点击的概率。如果我们对所有的预测结果进行降序排序，排在前面的是学习器认为<strong>最可能</strong>被点击的样本，排在后面的是学习期认为<strong>最不可能</strong>被点击的样本。</p>
<p>如果我们设定一个阈值，在这个阈值之上的学习器认为是正样本，阈值之下的学习器认为是负样本。可以想象到的是，当阈值很高时，预测为正样本的是分类器最有把握的一批样本，此时精确率往往很高，但是召回率一般较低。相反，当阈值很低时，分类器把很多拿不准的样本都预测为了正样本，此时召回率很高，但是精确率却往往偏低。</p>
<h4 id="1-4-F-1-Score"><a href="#1-4-F-1-Score" class="headerlink" title="1.4 F-1 Score"></a>1.4 F-1 Score</h4><p>为了折中精确率和召回率的结果，我们又引入了F-1 Score，计算公式如下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-7efec2f9e6c1d919.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>对于F1 Score有很多的变化形式，感兴趣的话大家可以参考一下周志华老师的西瓜书，我们这里就不再介绍了。</p>
<h4 id="1-5-ROC与AUC"><a href="#1-5-ROC与AUC" class="headerlink" title="1.5 ROC与AUC"></a>1.5 ROC与AUC</h4><p>在许多分类学习器中，产生的是一个概率预测值，然后将这个概率预测值与一个提前设定好的分类阈值进行比较，大于该阈值则认为是正例，小于该阈值则认为是负例。如果对所有的排序结果按照概率值进行降序排序，那么阈值可以将结果截断为两部分，前面的认为是正例，后面的认为是负例。</p>
<p>我们可以根据实际任务的需要选取不同的阈值。如果重视精确率，我们可以设定一个很高的阈值，如果更重视召回率，可以设定一个很低的阈值。</p>
<p>到这里，我们会抛出两个问题：<br>1)设定阈值然后再来计算精确率，召回率和F1-Score太麻烦了，这个阈值到底该设定为多少呢？有没有可以不设定阈值来直接评价我们的模型性能的方法呢？</p>
<p>2)排序结果很重要呀，不管预测值是多少，只要正例的预测概率都大于负例的就好了呀。</p>
<p>没错，ROC和AUC便可以解决我们上面抛出的两个问题。</p>
<p>ROC全称是“受试者工作特征”，（receiver operating characteristic）。我们根据学习器的预测结果进行排序，然后按此顺序逐个把样本作为正例进行预测，每次计算出两个重要的值，分别以这两个值作为横纵坐标作图，就得到了ROC曲线。</p>
<p>这两个指标是什么呢？是精确率和召回率么？并不是的，哈哈。</p>
<p>ROC曲线的横轴为“假正例率”（False Positive Rate,FPR)，又称为“假阳率”；纵轴为“真正例率”(True Positive Rate,TPR)，又称为“真阳率”，</p>
<p>假阳率，简单通俗来理解就是预测为正样本但是预测错了的可能性，显然，我们不希望该指标太高。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-6a7aa2038eb2cbee.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>真阳率，则是代表预测为正样本但是预测对了的可能性，当然，我们希望真阳率越高越好。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-d4d8eda66f1169ba.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>ROC计算过程如下：<br>1)首先每个样本都需要有一个label值，并且还需要一个预测的score值（取值0到1）;<br>2)然后按这个score对样本由大到小进行排序，假设这些数据位于表格中的一列，从上到下依次降序;<br>3)现在从上到下按照样本点的取值进行划分，位于分界点上面的我们把它归为预测为正样本，位于分界点下面的归为负样本;<br>4)分别计算出此时的TPR和FPR，然后在图中绘制（FPR, TPR）点。</p>
<p>说这么多，不如直接看图来的简单：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/4155986-0c24244b8e8c01b7.gif?imageMogr2/auto-orient/strip" alt=""></p>
<p>AUC（area under the curve）就是ROC曲线下方的面积，如下图所示，阴影部分面积即为AUC的值：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-84670734192f9b0d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>AUC量化了ROC曲线表达的分类能力。这种分类能力是与概率、阈值紧密相关的，分类能力越好（AUC越大），那么输出概率越合理，排序的结果越合理。</p>
<p>在CTR预估中，我们不仅希望分类器给出是否点击的分类信息，更需要分类器给出准确的概率值，作为排序的依据。所以，这里的AUC就直观地反映了CTR的准确性（也就是CTR的排序能力）。</p>
<p>终于介绍完了，那么这个值该怎么计算呢？</p>
<h2 id="2、AUC的计算"><a href="#2、AUC的计算" class="headerlink" title="2、AUC的计算"></a>2、AUC的计算</h2><p>关于AUC的计算方法，如果仅仅根据上面的描述，我们可能只能想到一种方法，那就是积分法，我们先来介绍这种方法，然后再来介绍其他的方法。</p>
<h4 id="2-1-积分思维"><a href="#2-1-积分思维" class="headerlink" title="2.1 积分思维"></a>2.1 积分思维</h4><p>这里的积分法其实就是我们之前介绍的绘制ROC曲线的过程，用代码简单描述下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">auc = 0.0</div><div class="line">height = 0.0</div><div class="line"></div><div class="line">for each training example x_i, y_i：</div><div class="line">  if y_i = 1.0:</div><div class="line">    height = height + 1/(tp+fn)</div><div class="line">  else </div><div class="line">    auc +=  height * 1/(tn+fp)</div><div class="line"></div><div class="line">return auc</div></pre></td></tr></table></figure>
<p>在上面的计算过程中，我们计算面积过程中隐含着一个假定，即所有样本的预测概率值不想等，因此我们的面积可以由一个个小小的矩形拼起来。但如果有两个或多个的预测值相同，我们调整一下阈值，得到的不是往上或者往右的延展，而是斜着向上形成一个梯形，此时计算梯形的面积就比较麻烦，因此这种方法其实并不是很常用。</p>
<h4 id="2-2-Wilcoxon-Mann-Witney-Test"><a href="#2-2-Wilcoxon-Mann-Witney-Test" class="headerlink" title="2.2 Wilcoxon-Mann-Witney Test"></a>2.2 Wilcoxon-Mann-Witney Test</h4><p>关于AUC还有一个很有趣的性质，它和Wilcoxon-Mann-Witney是等价的，而Wilcoxon-Mann-Witney Test就是<strong>测试任意给一个正类样本和一个负类样本，正类样本的score有多大的概率大于负类样本的score</strong>。</p>
<p>根据这个定义我们可以来探讨一下二者为什么是等价的？首先我们偷换一下概念，其实意思还是一样的，<strong>任意给定一个负样本，所有正样本的score中有多大比例是大于该负类样本的score？</strong> 由于每个负类样本的选中概率相同，那么Wilcoxon-Mann-Witney Test其实就是上面n2（负样本的个数）个比例的平均值。</p>
<p>那么对每个负样本来说，有多少的正样本的score比它的score大呢？是不是就是<strong>当结果按照score排序，阈值恰好为该负样本score时的真正例率TPR</strong>？没错，相信你的眼睛，是这样的！理解到这一层，二者等价的关系也就豁然开朗了。<strong>ROC曲线下的面积或者说AUC的值 与 测试任意给一个正类样本和一个负类样本，正类样本的score有多大的概率大于负类样本的score</strong></p>
<p>哈哈，那么我们只要计算出这个概率值就好了呀。我们知道，在有限样本中我们常用的得到概率的办法就是通过频率来估计之。这种估计随着样本规模的扩大而逐渐逼近真实值。样本数越多，计算的AUC越准确类似，也和计算积分的时候，小区间划分的越细，计算的越准确是同样的道理。具体来说就是： <strong>统计一下所有的 M×N(M为正类样本的数目，N为负类样本的数目)个正负样本对中，有多少个组中的正样本的score大于负样本的score。当二元组中正负样本的 score相等的时候，按照0.5计算。然后除以MN。</strong>公式表示如下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-7b554bfc12d8d180.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>实现这个方法的复杂度为O(n^2 )。n为样本数(即n=M+N)</p>
<h4 id="2-3-Wilcoxon-Mann-Witney-Test的化简"><a href="#2-3-Wilcoxon-Mann-Witney-Test的化简" class="headerlink" title="2.3 Wilcoxon-Mann-Witney Test的化简"></a>2.3 Wilcoxon-Mann-Witney Test的化简</h4><p>该方法和上述第二种方法原理一样，但复杂度降低了。首先对score从大到小排序，然后令最大score对应的sample的rank值为n，第二大score对应sample的rank值为n-1，以此类推从n到1。然后把所有的正类样本的rank相加，再减去正类样本的score为最小的那M个值的情况。得到的结果就是有多少对正类样本的score值大于负类样本的score值，最后再除以M×N即可。值得注意的是，当存在score相等的时候，对于score相等的样本，需要赋予相同的rank值(无论这个相等的score是出现在同类样本还是不同类的样本之间，都需要这样处理)。具体操作就是再把所有这些score相等的样本 的rank取平均。然后再使用上述公式。此公式描述如下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-d396340c58fa005c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>有了这个公式，我们计算AUC就非常简单了，下一节我们会给出一个简单的Demo</p>
<h2 id="3、AUC计算代码示例"><a href="#3、AUC计算代码示例" class="headerlink" title="3、AUC计算代码示例"></a>3、AUC计算代码示例</h2><p>这一节，我们给出一个AUC计算的小Demo，供大家参考：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line">import numpy as np</div><div class="line"></div><div class="line"></div><div class="line">label_all = np.random.randint(0,2,[10,1]).tolist()</div><div class="line">pred_all = np.random.random((10,1)).tolist()</div><div class="line"></div><div class="line">print(label_all)</div><div class="line">print(pred_all)</div><div class="line"></div><div class="line">posNum = len(list(filter(lambda s: s[0] == 1, label_all)))</div><div class="line"></div><div class="line">if (posNum &gt; 0):</div><div class="line">    negNum = len(label_all) - posNum</div><div class="line">    sortedq = sorted(enumerate(pred_all), key=lambda x: x[1])</div><div class="line"></div><div class="line">    posRankSum = 0</div><div class="line">    for j in range(len(pred_all)):</div><div class="line">        if (label_all[j][0] == 1):</div><div class="line">            posRankSum += list(map(lambda x: x[0], sortedq)).index(j) + 1</div><div class="line">    auc = (posRankSum - posNum * (posNum + 1) / 2) / (posNum * negNum)</div><div class="line">    print(&quot;auc:&quot;, auc)</div></pre></td></tr></table></figure>
<p>输出为：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">[[1], [1], [1], [1], [0], [0], [1], [0], [1], [0]]</div><div class="line">[[0.3338126725065774], [0.916003907444231], [0.21214487870979226], [0.7598235037160891], [0.07060830328081447], [0.7650759555141832], [0.16157972737309945], [0.6526480840746645], [0.9327233203035652], [0.6581121768195201]]</div><div class="line"></div><div class="line">auc: 0.5833333333333334</div></pre></td></tr></table></figure>
<p>#参考文献<br><a href="https://www.jianshu.com/p/848838ecbc2d" target="_blank" rel="external">https://www.jianshu.com/p/848838ecbc2d</a><br><a href="https://blog.csdn.net/dream_angel_z/article/details/50867951" target="_blank" rel="external">https://blog.csdn.net/dream_angel_z/article/details/50867951</a><br><a href="https://www.zhihu.com/question/39840928" target="_blank" rel="external">https://www.zhihu.com/question/39840928</a><br><a href="https://stats.stackexchange.com/questions/105501/understanding-roc-curve/105577" target="_blank" rel="external">https://stats.stackexchange.com/questions/105501/understanding-roc-curve/105577</a><br><a href="http://www.cnblogs.com/peizhe123/p/5081559.html" target="_blank" rel="external">http://www.cnblogs.com/peizhe123/p/5081559.html</a><br><a href="http://blog.revolutionanalytics.com/2017/03/auc-meets-u-stat.html" target="_blank" rel="external">http://blog.revolutionanalytics.com/2017/03/auc-meets-u-stat.html</a></p>
<h1 id="推荐系统遇上深度学习-十-–GBDT-LR融合方案实战"><a href="#推荐系统遇上深度学习-十-–GBDT-LR融合方案实战" class="headerlink" title="推荐系统遇上深度学习(十)–GBDT+LR融合方案实战"></a>推荐系统遇上深度学习(十)–GBDT+LR融合方案实战</h1><h2 id="写在前面的话"><a href="#写在前面的话" class="headerlink" title="写在前面的话"></a>写在前面的话</h2><p>GBDT和LR的融合在广告点击率预估中算是发展比较早的算法，为什么会在这里写这么一篇呢？本来想尝试写一下阿里的深度兴趣网络(Deep Interest Network)，发现阿里之前还有一个算法MLR，然后去查找相关的资料，里面提及了树模型也就是GBDT+LR方案的缺点，恰好之前也不太清楚GBDT+LR到底是怎么做的，所以今天我们先来了解一下GBDT和LR的融合方案。</p>
<h2 id="1、背景-2"><a href="#1、背景-2" class="headerlink" title="1、背景"></a>1、背景</h2><p>在CTR预估问题的发展初期，使用最多的方法就是逻辑回归(LR)，LR使用了Sigmoid变换将函数值映射到0~1区间，映射后的函数值就是CTR的预估值。<br>LR属于线性模型，容易并行化，可以轻松处理上亿条数据，但是学习能力十分有限，需要大量的特征工程来增加模型的学习能力。但大量的特征工程耗时耗力同时并不一定会带来效果提升。因此，如何自动发现有效的特征、特征组合，弥补人工经验不足，缩短LR特征实验周期，是亟需解决的问题。</p>
<p>FM模型通过隐变量的方式，发现两两特征之间的组合关系，但这种特征组合仅限于两两特征之间，后来发展出来了使用深度神经网络去挖掘更高层次的特征组合关系。但其实在使用神经网络之前，GBDT也是一种经常用来发现特征组合的有效思路。</p>
<p>Facebook 2014年的文章介绍了通过GBDT解决LR的特征组合问题，随后Kaggle竞赛也有实践此思路，GBDT与LR融合开始引起了业界关注。</p>
<p>在介绍这个模型之前，我们先来介绍两个问题：<br>1）为什么要使用集成的决策树模型，而不是单棵的决策树模型：一棵树的表达能力很弱，不足以表达多个有区分性的特征组合，多棵树的表达能力更强一些。可以更好的发现有效的特征和特征组合<br>2）为什么建树采用GBDT而非RF：RF也是多棵树，但从效果上有实践证明不如GBDT。且GBDT前面的树，特征分裂主要体现对多数样本有区分度的特征；后面的树，主要体现的是经过前N颗树，残差仍然较大的少数样本。优先选用在整体上有区分度的特征，再选用针对少数样本有区分度的特征，思路更加合理，这应该也是用GBDT的原因。</p>
<p>了解了为什么要用GBDT，我们就来看看到底二者是怎么融合的吧！</p>
<h2 id="2、GBDT和LR的融合方案"><a href="#2、GBDT和LR的融合方案" class="headerlink" title="2、GBDT和LR的融合方案"></a>2、GBDT和LR的融合方案</h2><p>GBDT和LR的融合方案，FaceBook的paper中有个例子：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-8a4cb50aefba2877.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>图中共有两棵树，x为一条输入样本，遍历两棵树后，x样本分别落到两颗树的叶子节点上，每个叶子节点对应LR一维特征，那么通过遍历树，就得到了该样本对应的所有LR特征。构造的新特征向量是取值0/1的。举例来说：上图有两棵树，左树有三个叶子节点，右树有两个叶子节点，最终的特征即为五维的向量。对于输入x，假设他落在左树第一个节点，编码[1,0,0]，落在右树第二个节点则编码[0,1]，所以整体的编码为[1,0,0,0,1]，这类编码作为特征，输入到LR中进行分类。</p>
<p>这个方案还是很简单的吧，在继续介绍下去之前，我们先介绍一下代码实践部分。</p>
<h2 id="3、GBDT-LR代码实践"><a href="#3、GBDT-LR代码实践" class="headerlink" title="3、GBDT+LR代码实践"></a>3、GBDT+LR代码实践</h2><p>本文介绍的代码只是一个简单的Demo，实际中大家需要根据自己的需要进行参照或者修改。</p>
<p>github地址：<a href="https://github.com/princewen/tensorflow_practice/tree/master/recommendation/GBDT%2BLR-Demo" target="_blank" rel="external">https://github.com/princewen/tensorflow_practice/tree/master/recommendation/GBDT%2BLR-Demo</a></p>
<p><strong>训练GBDT模型</strong><br>本文使用lightgbm包来训练我们的GBDT模型，训练共100棵树，每棵树有64个叶子结点。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div></pre></td><td class="code"><pre><div class="line">df_train = pd.read_csv(&apos;data/train.csv&apos;)</div><div class="line">df_test = pd.read_csv(&apos;data/test.csv&apos;)</div><div class="line"></div><div class="line">NUMERIC_COLS = [</div><div class="line">    &quot;ps_reg_01&quot;, &quot;ps_reg_02&quot;, &quot;ps_reg_03&quot;,</div><div class="line">    &quot;ps_car_12&quot;, &quot;ps_car_13&quot;, &quot;ps_car_14&quot;, &quot;ps_car_15&quot;,</div><div class="line">]</div><div class="line"></div><div class="line">print(df_test.head(10))</div><div class="line"></div><div class="line">y_train = df_train[&apos;target&apos;]  # training label</div><div class="line">y_test = df_test[&apos;target&apos;]  # testing label</div><div class="line">X_train = df_train[NUMERIC_COLS]  # training dataset</div><div class="line">X_test = df_test[NUMERIC_COLS]  # testing dataset</div><div class="line"></div><div class="line"># create dataset for lightgbm</div><div class="line">lgb_train = lgb.Dataset(X_train, y_train)</div><div class="line">lgb_eval = lgb.Dataset(X_test, y_test, reference=lgb_train)</div><div class="line"></div><div class="line">params = &#123;</div><div class="line">    &apos;task&apos;: &apos;train&apos;,</div><div class="line">    &apos;boosting_type&apos;: &apos;gbdt&apos;,</div><div class="line">    &apos;objective&apos;: &apos;binary&apos;,</div><div class="line">    &apos;metric&apos;: &#123;&apos;binary_logloss&apos;&#125;,</div><div class="line">    &apos;num_leaves&apos;: 64,</div><div class="line">    &apos;num_trees&apos;: 100,</div><div class="line">    &apos;learning_rate&apos;: 0.01,</div><div class="line">    &apos;feature_fraction&apos;: 0.9,</div><div class="line">    &apos;bagging_fraction&apos;: 0.8,</div><div class="line">    &apos;bagging_freq&apos;: 5,</div><div class="line">    &apos;verbose&apos;: 0</div><div class="line">&#125;</div><div class="line"></div><div class="line"># number of leaves,will be used in feature transformation</div><div class="line">num_leaf = 64</div><div class="line"></div><div class="line">print(&apos;Start training...&apos;)</div><div class="line"># train</div><div class="line">gbm = lgb.train(params,</div><div class="line">                lgb_train,</div><div class="line">                num_boost_round=100,</div><div class="line">                valid_sets=lgb_train)</div><div class="line"></div><div class="line">print(&apos;Save model...&apos;)</div><div class="line"># save model to file</div><div class="line">gbm.save_model(&apos;model.txt&apos;)</div><div class="line"></div><div class="line">print(&apos;Start predicting...&apos;)</div><div class="line"># predict and get data on leaves, training data</div></pre></td></tr></table></figure>
<p><strong>特征转换</strong></p>
<p>在训练得到100棵树之后，我们需要得到的不是GBDT的预测结果，而是每一条训练数据落在了每棵树的哪个叶子结点上，因此需要使用下面的语句：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">y_pred = gbm.predict(X_train, pred_leaf=True)</div></pre></td></tr></table></figure>
<p>打印上面结果的输出，可以看到shape是(8001,100)，即训练数据量*树的棵树</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">print(np.array(y_pred).shape)</div><div class="line">print(y_pred[0])</div></pre></td></tr></table></figure>
<p>结果为：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">(8001, 100)</div><div class="line">[[43 26 47 47 47 19 36 19 50 52 29  0  0  0 46 23 13 27 27 13 10 22  0 10</div><div class="line">   4 57 17 55 54 57 59 42 22 22 22 13  8  5 27  5 58 23 58 14 16 16 10 32</div><div class="line">  60 32  4  4  4  4  4 46 57 48 57 34 54  6 35  6  4 55 13 23 15 51 40  0</div><div class="line">  47 40 10 29 24 24 31 24 55  3 41  3 22 57  6  0  6  6 57 55 57 16 12 18</div><div class="line">  30 15 17 30]]</div></pre></td></tr></table></figure>
<p>然后我们需要将每棵树的特征进行one-hot处理，如前面所说，假设第一棵树落在43号叶子结点上，那我们需要建立一个64维的向量，除43维之外全部都是0。因此用于LR训练的特征维数共num_trees * num_leaves。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">print(&apos;Writing transformed training data&apos;)</div><div class="line">transformed_training_matrix = np.zeros([len(y_pred), len(y_pred[0]) * num_leaf],</div><div class="line">                                       dtype=np.int64)  # N * num_tress * num_leafs</div><div class="line">for i in range(0, len(y_pred)):</div><div class="line">    temp = np.arange(len(y_pred[0])) * num_leaf + np.array(y_pred[I])</div><div class="line">    transformed_training_matrix[i][temp] += 1</div></pre></td></tr></table></figure>
<p>当然，对于测试集也要进行同样的处理:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">y_pred = gbm.predict(X_test, pred_leaf=True)</div><div class="line">print(&apos;Writing transformed testing data&apos;)</div><div class="line">transformed_testing_matrix = np.zeros([len(y_pred), len(y_pred[0]) * num_leaf], dtype=np.int64)</div><div class="line">for i in range(0, len(y_pred)):</div><div class="line">    temp = np.arange(len(y_pred[0])) * num_leaf + np.array(y_pred[I])</div><div class="line">    transformed_testing_matrix[i][temp] += 1</div></pre></td></tr></table></figure>
<p><strong>L2训练</strong><br>然后我们可以用转换后的训练集特征和label训练我们的LR模型，并对测试集进行测试：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">lm = LogisticRegression(penalty=&apos;l2&apos;,C=0.05) # logestic model construction</div><div class="line">lm.fit(transformed_training_matrix,y_train)  # fitting the data</div><div class="line">y_pred_test = lm.predict_proba(transformed_testing_matrix)   # Give the probabilty on each label</div></pre></td></tr></table></figure>
<p>我们这里得到的不是简单的类别，而是每个类别的概率。</p>
<p><strong>效果评价</strong><br>在Facebook的paper中，模型使用NE(Normalized Cross-Entropy)，进行评价，计算公式如下:</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-9587957913f5ad4e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>代码如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">NE = (-1) / len(y_pred_test) * sum(((1+y_test)/2 * np.log(y_pred_test[:,1]) +  (1-y_test)/2 * np.log(1 - y_pred_test[:,1])))</div><div class="line">print(&quot;Normalized Cross Entropy &quot; + str(NE))</div></pre></td></tr></table></figure>
<h2 id="4、反思"><a href="#4、反思" class="headerlink" title="4、反思"></a>4、反思</h2><p>现在的GBDT和LR的融合方案真的适合现在的大多数业务数据么？现在的业务数据是什么？是大量离散特征导致的高维度离散数据。而树模型对这样的离散特征，是不能很好处理的，要说为什么，因为这容易导致过拟合。下面的一段话来自知乎：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-387590265d8350a6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>用盖坤的话说，GBDT只是对历史的一个记忆罢了，没有推广性，或者说泛化能力。</p>
<p>但这并不是说对于大规模的离散特征，GBDT和LR的方案不再适用，感兴趣的话大家可以看一下参考文献2和3，这里就不再介绍了。</p>
<p>刚才提到了阿里的盖坤大神，他的团队在2017年提出了两个重要的用于CTR预估的模型，MLR和DIN，之后的系列中，我们会讲解这两种模型的理论和实战！欢迎大家继续关注！</p>
<p>#参考文献：<br>1、Facebook的paper：<a href="http://quinonero.net/Publications/predicting-clicks-facebook.pdf" target="_blank" rel="external">http://quinonero.net/Publications/predicting-clicks-facebook.pdf</a><br>2、<a href="http://www.cbdio.com/BigData/2015-08/27/content_3750170.htm" target="_blank" rel="external">http://www.cbdio.com/BigData/2015-08/27/content_3750170.htm</a><br>3、<a href="https://blog.csdn.net/shine19930820/article/details/71713680" target="_blank" rel="external">https://blog.csdn.net/shine19930820/article/details/71713680</a><br>4、<a href="https://www.zhihu.com/question/35821566" target="_blank" rel="external">https://www.zhihu.com/question/35821566</a><br>5、<a href="https://github.com/neal668/LightGBM-GBDT-LR/blob/master/GBFT%2BLR_simple.py" target="_blank" rel="external">https://github.com/neal668/LightGBM-GBDT-LR/blob/master/GBFT%2BLR_simple.py</a></p>
<h1 id="推荐系统遇上深度学习-十一-–神经协同过滤NCF原理及实战"><a href="#推荐系统遇上深度学习-十一-–神经协同过滤NCF原理及实战" class="headerlink" title="推荐系统遇上深度学习(十一)–神经协同过滤NCF原理及实战"></a>推荐系统遇上深度学习(十一)–神经协同过滤NCF原理及实战</h1><p>好久没更新该系列了，最近看到了一篇关于神经协同过滤的论文，感觉还不错，跟大家分享下。</p>
<p>论文地址：<a href="https://www.comp.nus.edu.sg/~xiangnan/papers/ncf.pdf" target="_blank" rel="external">https://www.comp.nus.edu.sg/~xiangnan/papers/ncf.pdf</a></p>
<h2 id="1、Neural-Collaborative-Filtering"><a href="#1、Neural-Collaborative-Filtering" class="headerlink" title="1、Neural Collaborative Filtering"></a>1、Neural Collaborative Filtering</h2><h4 id="1-1-背景"><a href="#1-1-背景" class="headerlink" title="1.1 背景"></a>1.1 背景</h4><p>本文讨论的主要是隐性反馈协同过滤解决方案，先来明确两个概念：显性反馈和隐性反馈：</p>
<p><strong>显性反馈</strong>行为包括用户明确表示对物品喜好的行为<br><strong>隐性反馈</strong>行为指的是那些不能明确反应用户喜好</p>
<p>举例来说：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-9cab8ff893c16d2a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>很多应用场景，并没有显性反馈的存在。因为大部分用户是沉默的用户，并不会明确给系统反馈“我对这个物品的偏好值是多少”。因此，推荐系统可以根据大量的隐性反馈来推断用户的偏好值。</p>
<p>根据已得到的隐性反馈数据，我们将用户-条目交互矩阵Y定义为：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-4cf5e3db52a1c7d3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>但是，Yui为1仅代表二者有交互记录，并不代表用户u真的喜欢项目i，同理，u和i没有交互记录也不能代表u不喜欢i。这对隐性反馈的学习提出了挑战，因为它提供了关于用户偏好的噪声信号。虽然观察到的条目至少反映了用户对项目的兴趣，但是未查看的条目可能只是丢失数据，并且这其中存在自然稀疏的负反馈。</p>
<p>在隐性反馈上的推荐问题可以表达为估算矩阵 Y中未观察到的条目的分数问题（这个分数被用来评估项目的排名）。形式上它可以被抽象为学习函数：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-875ca45170bf6cf9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>为了处理缺失数据，有两种常见的做法：要么将所有未观察到的条目视作负反馈，要么从没有观察到条目中抽样作为负反馈实例。</p>
<h4 id="1-2-矩阵分解及其缺陷"><a href="#1-2-矩阵分解及其缺陷" class="headerlink" title="1.2 矩阵分解及其缺陷"></a>1.2 矩阵分解及其缺陷</h4><p>传统的求解方法是矩阵分解(MF,Matrix Factorization)，为每个user和item找到一个隐向量，问题变为：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-4603109b9e326aa7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>这里的 K表示隐式空间（latent space）的维度。正如我们所看到的，MF模型是用户和项目的潜在因素的双向互动，它假设潜在空间的每一维都是相互独立的并且用相同的权重将它们线性结合。因此，MF可视为隐向量（latent factor）的线性模型。</p>
<p>论文中给出了一个例子来说明这种算法的局限性：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-b381c43ffb45efad.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>1(a)是user-item交互矩阵，1(b)是用户的隐式空间，论文中强调了两点来理解这张图片：<br>1）MF将user和item分布到同样的隐式空间中，那么两个用户之间的相似性也可以用二者在隐式空间中的向量夹角来确定。<br>2）使用Jaccard系数来作为真实的用户相似性。<br>通过MF计算的相似性与Jaccard系数计算的相似性也可以用来评判MF的性能。我们先来看看Jaccard系数</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-c104fa23f3d69c0b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-7448a3708977e2ca.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>上面的示例显示了MF因为使用一个简单的和固定的内积，来估计在低维潜在空间中用户-项目的复杂交互，从而所可能造成的限制。解决该问题的方法之一是使用大量的潜在因子 K (就是隐式空间向量的维度)。然而这可能对模型的泛化能力产生不利的影响（e.g. 数据的过拟合问题），特别是在稀疏的集合上。论文通过使用DNNs从数据中学习交互函数，突破了这个限制。</p>
<h4 id="1-3-NCF"><a href="#1-3-NCF" class="headerlink" title="1.3 NCF"></a>1.3 NCF</h4><p>本文先提出了一种通用框架：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-9c075241d934afaa.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>针对这个通用框架，论文提出了三种不同的实现，三种实现可以用一张图来说明：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-0cc3550bf20a608c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p><strong>GMF</strong>：<br>上图中仅使用GMF layer，就得到了第一种实现方式GMF，GMF被称为广义矩阵分解，输出层的计算公式为：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-f33f0e781fce1331.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p><strong>MLP</strong><br>上图中仅使用右侧的MLP Layers，就得到了第二种学习方式，通过多层神经网络来学习user和item的隐向量。这样，输出层的计算公式为：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-cd8744d945dab6c9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p><strong>NeuMF</strong><br>结合GMF和MLP，得到的就是第三种实现方式，上图是该方式的完整实现，输出层的计算公式为：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-4f43b020b149514f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<h4 id="1-4-模型实验"><a href="#1-4-模型实验" class="headerlink" title="1.4 模型实验"></a>1.4 模型实验</h4><p>论文通过三个角度进行了试验：</p>
<p> <strong>RQ1</strong> 我们提出的NCF方法是否胜过 state-of-the-art 的隐性协同过滤方法？<br> <strong>RQ2</strong> 我们提出的优化框架（消极样本抽样的logloss）怎样为推荐任务服务？<br> <strong>RQ3</strong> 更深的隐藏单元是不是有助于对用户项目交互数据的学习？</p>
<p><strong>使用的数据集</strong>：MovieLens 和 Pinterest 两个数据集</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-c5c67c9c159e7820.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p><strong>评估方案</strong>：为了评价项目推荐的性能，论文采用了leave-one-out方法评估，即：对于每个用户，我们将其<strong>最近的一次交互作为测试集</strong>（数据集一般都有时间戳），并利用余下的培训作为训练集。由于在评估过程中为每个用户排列所有项目花费的时间太多，所以遵循一般的策略，随机抽取100个不与用户进行交互的项目，将测试项目排列在这100个项目中。排名列表的性能由<strong>命中率（HR）</strong>和<strong>归一化折扣累积增益（NDCG）</strong>来衡量。同时，论文将这两个指标的排名列表截断为10。如此一来，HR直观地衡量测试项目是否存在于前10名列表中，而NDCG通过将较高分数指定为顶级排名来计算命中的位置。本文计算每个测试用户的这两个指标，并求取了平均分。</p>
<p><strong>Baselines</strong>，论文将NCF方法与下列方法进行了比较：ItemPop，ItemKNN，BPR，eALS。</p>
<p>以下是三个结果的贴图，关于试验结果的解读，由于篇幅的原因，大家可以查看原论文。</p>
<p><strong>RQ1试验结果</strong></p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-f625ccafecdd8070.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>简单的结论，即NCF效果好于BaseLine模型，如果不好的话论文也不用写了，哈哈。</p>
<p><strong>RQ2试验结果</strong></p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-619cfe110711228a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>Figure 6 表示将模型看作一个二分类任务并使用logloss作为损失函数时的训练效果。<br>Figure7 表示采样率对模型性能的影响（横轴是采样率，即负样本与正样本的比例）。</p>
<p><strong>RQ3试验结果</strong></p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-fc499022ad1423cf.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>上面的表格设置了两个变量，分别是Embedding的长度K和神经网络的层数，使用类似网格搜索的方式展示了在两个数据集上的结果。增加Embedding的长度和神经网络的层数是可以提升训练效果的。</p>
<h2 id="2、NCF实战"><a href="#2、NCF实战" class="headerlink" title="2、NCF实战"></a>2、NCF实战</h2><p>本文的github地址为：<a href="https://github.com/princewen/tensorflow_practice/tree/master/recommendation/Basic-NCF-Demo" target="_blank" rel="external">https://github.com/princewen/tensorflow_practice/tree/master/recommendation/Basic-NCF-Demo</a></p>
<p>本文仅介绍模型相关细节，数据处理部分就不介绍啦。</p>
<p>项目结构如下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-abf4284380169268.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p><strong>数据输入</strong><br>本文使用了一种新的数据处理方式，不过我们的输入就是三个：userid，itemid以及label，对训练集来说，label是0-1值，对测试集来说，是具体的itemid</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">def get_data(self):</div><div class="line">    sample = self.iterator.get_next()</div><div class="line">    self.user = sample[&apos;user&apos;]</div><div class="line">    self.item = sample[&apos;item&apos;]</div><div class="line">    self.label = tf.cast(sample[&apos;label&apos;],tf.float32)</div></pre></td></tr></table></figure>
<p><strong>定义初始化方式、损失函数、优化器</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div></pre></td><td class="code"><pre><div class="line">def inference(self):</div><div class="line">    &quot;&quot;&quot; Initialize important settings &quot;&quot;&quot;</div><div class="line">    self.regularizer = tf.contrib.layers.l2_regularizer(self.regularizer_rate)</div><div class="line"></div><div class="line">    if self.initializer == &apos;Normal&apos;:</div><div class="line">        self.initializer = tf.truncated_normal_initializer(stddev=0.01)</div><div class="line">    elif self.initializer == &apos;Xavier_Normal&apos;:</div><div class="line">        self.initializer = tf.contrib.layers.xavier_initializer()</div><div class="line">    else:</div><div class="line">        self.initializer = tf.glorot_uniform_initializer()</div><div class="line"></div><div class="line">    if self.activation_func == &apos;ReLU&apos;:</div><div class="line">        self.activation_func = tf.nn.relu</div><div class="line">    elif self.activation_func == &apos;Leaky_ReLU&apos;:</div><div class="line">        self.activation_func = tf.nn.leaky_relu</div><div class="line">    elif self.activation_func == &apos;ELU&apos;:</div><div class="line">        self.activation_func = tf.nn.elu</div><div class="line"></div><div class="line">    if self.loss_func == &apos;cross_entropy&apos;:</div><div class="line">        # self.loss_func = lambda labels, logits: -tf.reduce_sum(</div><div class="line">        # 		(labels * tf.log(logits) + (</div><div class="line">        # 		tf.ones_like(labels, dtype=tf.float32) - labels) *</div><div class="line">        # 		tf.log(tf.ones_like(logits, dtype=tf.float32) - logits)), 1)</div><div class="line">        self.loss_func = tf.nn.sigmoid_cross_entropy_with_logits</div><div class="line"></div><div class="line">    if self.optim == &apos;SGD&apos;:</div><div class="line">        self.optim = tf.train.GradientDescentOptimizer(self.lr,</div><div class="line">                                                       name=&apos;SGD&apos;)</div><div class="line">    elif self.optim == &apos;RMSProp&apos;:</div><div class="line">        self.optim = tf.train.RMSPropOptimizer(self.lr, decay=0.9,</div><div class="line">                                               momentum=0.0, name=&apos;RMSProp&apos;)</div><div class="line">    elif self.optim == &apos;Adam&apos;:</div><div class="line">        self.optim = tf.train.AdamOptimizer(self.lr, name=&apos;Adam&apos;)</div></pre></td></tr></table></figure>
<p><strong>得到embedding值</strong><br>分别得到GMF和MLP的embedding向量，当然也可以使用embedding_lookup方法：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div></pre></td><td class="code"><pre><div class="line">with tf.name_scope(&apos;input&apos;):</div><div class="line">    self.user_onehot = tf.one_hot(self.user,self.user_size,name=&apos;user_onehot&apos;)</div><div class="line">    self.item_onehot = tf.one_hot(self.item,self.item_size,name=&apos;item_onehot&apos;)</div><div class="line"></div><div class="line">with tf.name_scope(&apos;embed&apos;):</div><div class="line">    self.user_embed_GMF = tf.layers.dense(inputs = self.user_onehot,</div><div class="line">                                          units = self.embed_size,</div><div class="line">                                          activation = self.activation_func,</div><div class="line">                                          kernel_initializer=self.initializer,</div><div class="line">                                          kernel_regularizer=self.regularizer,</div><div class="line">                                          name=&apos;user_embed_GMF&apos;)</div><div class="line"></div><div class="line">    self.item_embed_GMF = tf.layers.dense(inputs=self.item_onehot,</div><div class="line">                                          units=self.embed_size,</div><div class="line">                                          activation=self.activation_func,</div><div class="line">                                          kernel_initializer=self.initializer,</div><div class="line">                                          kernel_regularizer=self.regularizer,</div><div class="line">                                          name=&apos;item_embed_GMF&apos;)</div><div class="line"></div><div class="line">    self.user_embed_MLP = tf.layers.dense(inputs=self.user_onehot,</div><div class="line">                                          units=self.embed_size,</div><div class="line">                                          activation=self.activation_func,</div><div class="line">                                          kernel_initializer=self.initializer,</div><div class="line">                                          kernel_regularizer=self.regularizer,</div><div class="line">                                          name=&apos;user_embed_MLP&apos;)</div><div class="line">    self.item_embed_MLP = tf.layers.dense(inputs=self.item_onehot,</div><div class="line">                                          units=self.embed_size,</div><div class="line">                                          activation=self.activation_func,</div><div class="line">                                          kernel_initializer=self.initializer,</div><div class="line">                                          kernel_regularizer=self.regularizer,</div><div class="line">                                          name=&apos;item_embed_MLP&apos;)</div></pre></td></tr></table></figure>
<p><strong>GMF</strong><br>GMF部分就是求两个embedding的内积：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">with tf.name_scope(&quot;GMF&quot;):</div><div class="line">    self.GMF = tf.multiply(self.user_embed_GMF,self.item_embed_GMF,name=&apos;GMF&apos;)</div></pre></td></tr></table></figure>
<p><strong>MLP</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line">with tf.name_scope(&quot;MLP&quot;):</div><div class="line">    self.interaction = tf.concat([self.user_embed_MLP, self.item_embed_MLP],</div><div class="line">                                 axis=-1, name=&apos;interaction&apos;)</div><div class="line"></div><div class="line">    self.layer1_MLP = tf.layers.dense(inputs=self.interaction,</div><div class="line">                                      units=self.embed_size * 2,</div><div class="line">                                      activation=self.activation_func,</div><div class="line">                                      kernel_initializer=self.initializer,</div><div class="line">                                      kernel_regularizer=self.regularizer,</div><div class="line">                                      name=&apos;layer1_MLP&apos;)</div><div class="line">    self.layer1_MLP = tf.layers.dropout(self.layer1_MLP, rate=self.dropout)</div><div class="line"></div><div class="line">    self.layer2_MLP = tf.layers.dense(inputs=self.layer1_MLP,</div><div class="line">                                      units=self.embed_size,</div><div class="line">                                      activation=self.activation_func,</div><div class="line">                                      kernel_initializer=self.initializer,</div><div class="line">                                      kernel_regularizer=self.regularizer,</div><div class="line">                                      name=&apos;layer2_MLP&apos;)</div><div class="line">    self.layer2_MLP = tf.layers.dropout(self.layer2_MLP, rate=self.dropout)</div><div class="line"></div><div class="line">    self.layer3_MLP = tf.layers.dense(inputs=self.layer2_MLP,</div><div class="line">                                      units=self.embed_size // 2,</div><div class="line">                                      activation=self.activation_func,</div><div class="line">                                      kernel_initializer=self.initializer,</div><div class="line">                                      kernel_regularizer=self.regularizer,</div><div class="line">                                      name=&apos;layer3_MLP&apos;)</div><div class="line">    self.layer3_MLP = tf.layers.dropout(self.layer3_MLP, rate=self.dropout)</div></pre></td></tr></table></figure>
<p><strong>得到预测值</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">with tf.name_scope(&apos;concatenation&apos;):</div><div class="line">    self.concatenation = tf.concat([self.GMF,self.layer3_MLP],axis=-1,name=&apos;concatenation&apos;)</div><div class="line"></div><div class="line"></div><div class="line">    self.logits = tf.layers.dense(inputs= self.concatenation,</div><div class="line">                                  units = 1,</div><div class="line">                                  activation=None,</div><div class="line">                                  kernel_initializer=self.initializer,</div><div class="line">                                  kernel_regularizer=self.regularizer,</div><div class="line">                                  name=&apos;predict&apos;)</div><div class="line"></div><div class="line">    self.logits_dense = tf.reshape(self.logits,[-1])</div></pre></td></tr></table></figure>
<p><strong>测试集构建</strong><br>这里只介绍几行关键的测试集构建代码，整个流程希望大家可以看一下完整的代码。<br>需要明确的一点是，对于测试集，我们的评价不只是对错，还要关注排名，所以测试集的label不是0-1，而是具体的itemid<br>首先，对每个user取最后一行作为测试集的正样本：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">split_train_test = []</div><div class="line"></div><div class="line">for i in range(len(user_set)):</div><div class="line">    for _ in range(user_length[i] - 1):</div><div class="line">        split_train_test.append(&apos;train&apos;)</div><div class="line">    split_train_test.append(&apos;test&apos;)</div><div class="line"></div><div class="line">full_data[&apos;split&apos;] = split_train_test</div><div class="line"></div><div class="line">train_data = full_data[full_data[&apos;split&apos;] == &apos;train&apos;].reset_index(drop=True)</div><div class="line">test_data = full_data[full_data[&apos;split&apos;] == &apos;test&apos;].reset_index(drop=True)</div></pre></td></tr></table></figure>
<p>添加一些负采样的样本， 这里顺序是，1正样本-n负样本-1正样本-n负样本….，每个用户有n+1条数据，便于计算HR和NDCG：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">feature_user.append(user)</div><div class="line">feature_item.append(item)</div><div class="line">labels_add.append(label)</div><div class="line"></div><div class="line">for k in neg_samples:</div><div class="line">    feature_user.append(user)</div><div class="line">    feature_item.append(k)</div><div class="line">    labels_add.append(k)</div></pre></td></tr></table></figure>
<p>不打乱测试集的顺序，设置batch的大小为1+n:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">dataset = tf.data.Dataset.from_tensor_slices(data)</div><div class="line">dataset = dataset.batch(test_neg + 1)</div></pre></td></tr></table></figure>
<p><strong>计算HR和NDCG</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">def hr(gt_item, pred_items):</div><div class="line">    if gt_item in pred_items:</div><div class="line">        return 1</div><div class="line">    return 0</div><div class="line"></div><div class="line"></div><div class="line">def ndcg(gt_item, pred_items):</div><div class="line">    if gt_item in pred_items:</div><div class="line">        index = np.where(pred_items == gt_item)[0][0]</div><div class="line">        return np.reciprocal(np.log2(index + 2))</div><div class="line">    return 0</div></pre></td></tr></table></figure>
<p>更详细的代码可以参考github，最好能够手敲一遍来理解其原理哟！</p>
<p>#参考文章</p>
<p><a href="https://www.comp.nus.edu.sg/~xiangnan/papers/ncf.pdf" target="_blank" rel="external">https://www.comp.nus.edu.sg/~xiangnan/papers/ncf.pdf</a><br><a href="https://www.cnblogs.com/HolyShine/p/6728999.html" target="_blank" rel="external">https://www.cnblogs.com/HolyShine/p/6728999.html</a></p>
<h1 id="推荐系统遇上深度学习-十二-–推荐系统中的EE问题及基本Bandit算法"><a href="#推荐系统遇上深度学习-十二-–推荐系统中的EE问题及基本Bandit算法" class="headerlink" title="推荐系统遇上深度学习(十二)–推荐系统中的EE问题及基本Bandit算法"></a>推荐系统遇上深度学习(十二)–推荐系统中的EE问题及基本Bandit算法</h1><h2 id="1、推荐系统中的EE问题"><a href="#1、推荐系统中的EE问题" class="headerlink" title="1、推荐系统中的EE问题"></a>1、推荐系统中的EE问题</h2><p>Exploration and Exploitation(EE问题，探索与开发)是计算广告和推荐系统里常见的一个问题，为什么会有EE问题？简单来说，是为了平衡推荐系统的准确性和多样性。</p>
<p>EE问题中的Exploitation就是：对用户比较确定的兴趣，当然要利用开采迎合，好比说已经挣到的钱，当然要花；而exploration就是：光对着用户已知的兴趣使用，用户很快会腻，所以要不断探索用户新的兴趣才行，这就好比虽然有一点钱可以花了，但是还得继续搬砖挣钱，不然花完了就得喝西北风。</p>
<h2 id="2、Bandit算法"><a href="#2、Bandit算法" class="headerlink" title="2、Bandit算法"></a>2、Bandit算法</h2><p>Bandit算法是解决EE问题的一种有效算法，我们先来了解一下Bandit算法的起源。<br>Bandit算法来源于历史悠久的赌博学，它要解决的问题是这样的：</p>
<p>一个赌徒，要去摇老虎机，走进赌场一看，一排老虎机，外表一模一样，但是每个老虎机吐钱的概率可不一样，他不知道每个老虎机吐钱的概率分布是什么，那么每次该选择哪个老虎机可以做到最大化收益呢？这就是多臂赌博机问题（Multi-armed bandit problem, K-armed bandit problem, MAB）。</p>
<p><img src="http://upload-images.jianshu.io/upload_images/4155986-85269ad91ee8ce57?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>怎么解决这个问题呢？最好的办法是去试一试，不是盲目地试，而是有策略地快速试一试，这些策略就是Bandit算法。</p>
<p>Bandit算法如何同推荐系统中的EE问题联系起来呢？假设我们已经经过一些试验，得到了当前每个老虎机的吐钱的概率，如果想要获得最大的收益，我们会一直摇哪个吐钱概率最高的老虎机，这就是Exploitation。但是，当前获得的信息并不是老虎机吐钱的真实概率，可能还有更好的老虎机吐钱概率更高，因此还需要进一步探索，这就是Exploration问题。</p>
<p>下面，我们就来看一下一些经典的Bandit算法实现吧，不过我们还需要补充一些基础知识。</p>
<h2 id="3、基础知识"><a href="#3、基础知识" class="headerlink" title="3、基础知识"></a>3、基础知识</h2><h4 id="3-1-累积遗憾"><a href="#3-1-累积遗憾" class="headerlink" title="3.1 累积遗憾"></a>3.1 累积遗憾</h4><p>Bandit算法需要量化一个核心问题：错误的选择到底有多大的遗憾？能不能遗憾少一些？所以我们便有了衡量Bandit算法的一个指标：累积遗憾：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-f2b84165d0c8cf63.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>这里 t 表示轮数, r表示回报。公式右边的第一项表示第t轮的期望最大收益，而右边的第二项表示当前选择的arm获取的收益，把每次差距累加起来就是总的遗憾。</p>
<p>对应同样的问题，采用不同bandit算法来进行实验相同的次数，那么看哪个算法的总regret增长最慢，那么哪个算法的效果就是比较好的。</p>
<h4 id="3-2-Beta分布"><a href="#3-2-Beta分布" class="headerlink" title="3.2 Beta分布"></a>3.2 Beta分布</h4><p>有关Beta分布，可以参考帖子：<a href="https://www.zhihu.com/question/30269898。这里只做一个简单的介绍。" target="_blank" rel="external">https://www.zhihu.com/question/30269898。这里只做一个简单的介绍。</a><br>beta分布可以看作一个概率的概率分布。它是对二项分布中成功概率p的概率分布的描述。它的形式如下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-e191ab28dde4b7d1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>其中，a和b分别代表在a+b次伯努利试验中成功和失败的次数。我们用下面的图来说明一下Beta分布的含义：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-af26e938fe5ef24a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>上图中一共有三条线，我们忽略中间的一条线，第一条线中a=81，b=219。也就是说在我们进行了300次伯努利试验中，成功81次，失败219次的情况下，成功概率p的一个分布，可以看到，p的概率在0.27左右概率最大，但我们不能说成功的概率就是0.27，这也就是频率派和贝叶斯派的区别，哈哈。此时，我们又做了300次试验，此时在总共600次伯努利试验中，成功了181次，失败了419次，此时成功概率p的概率分布变为了蓝色的线，在0.3左右概率最大。</p>
<h2 id="4、经典Bandit算法原理及实现"><a href="#4、经典Bandit算法原理及实现" class="headerlink" title="4、经典Bandit算法原理及实现"></a>4、经典Bandit算法原理及实现</h2><p>下文中的收益可以理解为老虎机吐钱的观测概率。</p>
<h4 id="4-1-朴素Bandit算法"><a href="#4-1-朴素Bandit算法" class="headerlink" title="4.1 朴素Bandit算法"></a>4.1 朴素Bandit算法</h4><p>先随机试若干次，计算每个臂的平均收益，一直选均值最大那个臂。</p>
<h4 id="4-2-Epsilon-Greedy算法"><a href="#4-2-Epsilon-Greedy算法" class="headerlink" title="4.2 Epsilon-Greedy算法"></a>4.2 Epsilon-Greedy算法</h4><p>选一个(0,1)之间较小的数epsilon，每次以epsilon的概率在所有臂中随机选一个。以1-epsilon的概率选择截止当前，平均收益最大的那个臂。根据选择臂的回报值来对回报期望进行更新。</p>
<p>这里epsilon的值可以控制对exploit和explore的偏好程度，每次决策以概率ε去勘探Exploration，1-ε的概率来开发Exploitation，基于选择的item及回报，更新item的回报期望。</p>
<p>对于Epsilon-Greedy算法来首，能够应对变化，即如果item的回报发生变化，能及时改变策略，避免卡在次优状态。同时Epsilon的值可以控制对Exploit和Explore的偏好程度。越接近0，越保守，只想花钱不想挣钱。但是策略运行一段时间后，我们已经对各item有了一定程度了解，但没用利用这些信息，仍然不做任何区分地随机Exploration，这是Epsilon-Greedy算法的缺点。</p>
<h4 id="4-3-Thompson-sampling算法"><a href="#4-3-Thompson-sampling算法" class="headerlink" title="4.3 Thompson sampling算法"></a>4.3 Thompson sampling算法</h4><p>Thompson sampling算法用到了Beta分布，该方法假设每个老虎机都有一个吐钱的概率p，同时该概率p的概率分布符合beta(wins, lose)分布，每个臂都维护一个beta分布的参数，即wins, lose。每次试验后，选中一个臂，摇一下，有收益则该臂的wins增加1，否则该臂的lose增加1。</p>
<p>每次选择臂的方式是：用每个臂现有的beta分布产生一个随机数b，选择所有臂产生的随机数中最大的那个臂去摇。</p>
<h4 id="4-4-UCB算法"><a href="#4-4-UCB算法" class="headerlink" title="4.4 UCB算法"></a>4.4 UCB算法</h4><p>前面提到了，Epsilon-Greedy算法在探索的时候，所有的老虎机都有同样的概率被选中，这其实没有充分利用历史信息，比如每个老虎机之前探索的次数，每个老虎机之前的探索中吐钱的频率。</p>
<p>那我们怎么能够充分利用历史信息呢？首先，根据当前老虎机已经探索的次数，以及吐钱的次数，我们可以计算出当前每个老虎机吐钱的观测概率p’。同时，由于观测次数有限，因此观测概率和真实概率p之间总会有一定的差值 ∆ ，即p’ -  ∆  &lt;= p &lt;= p’ +  ∆。</p>
<p>基于上面的讨论，我们得到了另一种常用的Bandit算法：UCB(Upper Confidence Bound)算法。该算法在每次推荐时，总是乐观的认为每个老虎机能够得到的收益是p’ +  ∆。</p>
<p>好了，接下来的问题就是观测概率和真实概率之间的差值∆如何计算了，我们首先有两个直观的理解：<br>1）对于选中的老虎机，多获得一次反馈会使∆变小，当反馈无穷多时，∆趋近于0，最终会小于其他没有被选中的老虎机的∆。<br>2）对于没有被选中的老虎机，∆会随着轮数的增大而增加，最终会大于其他被选中的老虎机。</p>
<p>因此，当进行了一定的轮数的时候，每个老虎机都有机会得到探索的机会。UCB算法中p’ +  ∆的计算公式如下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-282d39b8f1196a80.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>其中加号前面是第j个老虎机到目前的收益均值，后面的叫做bonus，本质上是均值的标准差，T是目前的试验次数，n是该老虎机被试次数。</p>
<p>为什么选择上面形式的∆呢，还得从Chernoff-Hoeffding Bound说起：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-7deb00fa956f9895.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>因此(下面的截图来自于知乎<a href="https://zhuanlan.zhihu.com/p/32356077)：" target="_blank" rel="external">https://zhuanlan.zhihu.com/p/32356077)：</a></p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-861e6f749bf24730.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<h2 id="5、代码实现"><a href="#5、代码实现" class="headerlink" title="5、代码实现"></a>5、代码实现</h2><p>接下来，我们来实现两个基本的Bandit算法，UCB和Thompson sampling算法。</p>
<p>##5.1 UCB算法<br>代码中有详细的注释，所以我直接贴完整的代码了：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div></pre></td><td class="code"><pre><div class="line">import numpy as np</div><div class="line"></div><div class="line">T = 1000  # T轮试验</div><div class="line">N = 10  # N个老虎机</div><div class="line"></div><div class="line">true_rewards = np.random.uniform(low=0, high=1, size=N)  # 每个老虎机真实的吐钱概率</div><div class="line">estimated_rewards = np.zeros(N)  # 每个老虎机吐钱的观测概率，初始都为0</div><div class="line">chosen_count = np.zeros(N)  # 每个老虎机当前已经探索的次数，初始都为0</div><div class="line">total_reward = 0</div><div class="line"></div><div class="line"></div><div class="line"># 计算delta</div><div class="line">def calculate_delta(T, item):</div><div class="line">    if chosen_count[item] == 0:</div><div class="line">        return 1</div><div class="line">    else:</div><div class="line">        return np.sqrt(2 * np.log(T) / chosen_count[item])</div><div class="line"></div><div class="line"># 计算每个老虎机的p+delta，同时做出选择</div><div class="line">def UCB(t, N):</div><div class="line">    upper_bound_probs = [estimated_rewards[item] + calculate_delta(t, item) for item in range(N)]</div><div class="line">    item = np.argmax(upper_bound_probs)</div><div class="line">    reward = np.random.binomial(n=1, p=true_rewards[item])</div><div class="line">    return item, reward</div><div class="line"></div><div class="line"></div><div class="line">for t in range(1, T):  # 依次进行T次试验</div><div class="line">    # 选择一个老虎机，并得到是否吐钱的结果</div><div class="line">    item, reward = UCB(t, N)</div><div class="line">    total_reward += reward  # 一共有多少客人接受了推荐</div><div class="line"></div><div class="line">    # 更新每个老虎机的吐钱概率</div><div class="line">    estimated_rewards[item] = ((t - 1) * estimated_rewards[item] + reward) / t</div><div class="line">    chosen_count[item] += 1</div></pre></td></tr></table></figure>
<p>##5.2 Thompson sampling算法<br>Thompson sampling算法涉及到了beta分布，因此我们使用pymc库来产生服从beta分布的随机数，只需要一行代码就能在选择合适的老虎机。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">np.argmax(pymc.rbeta(1 + successes, 1 + totals - successes))</div></pre></td></tr></table></figure>
<p>#参考文献<br><a href="https://blog.csdn.net/z1185196212/article/details/53374194" target="_blank" rel="external">https://blog.csdn.net/z1185196212/article/details/53374194</a><br><a href="https://blog.csdn.net/heyc861221/article/details/80129310" target="_blank" rel="external">https://blog.csdn.net/heyc861221/article/details/80129310</a><br><a href="http://baijiahao.baidu.com/s?id=1559186004007512&amp;wfr=spider&amp;for=pc" target="_blank" rel="external">http://baijiahao.baidu.com/s?id=1559186004007512&amp;wfr=spider&amp;for=pc</a><br><a href="https://www.zhihu.com/question/30269898" target="_blank" rel="external">https://www.zhihu.com/question/30269898</a><br><a href="https://zhuanlan.zhihu.com/p/32356077" target="_blank" rel="external">https://zhuanlan.zhihu.com/p/32356077</a></p>
<h1 id="推荐系统遇上深度学习-十三-–linUCB方法浅析及实现"><a href="#推荐系统遇上深度学习-十三-–linUCB方法浅析及实现" class="headerlink" title="推荐系统遇上深度学习(十三)–linUCB方法浅析及实现"></a>推荐系统遇上深度学习(十三)–linUCB方法浅析及实现</h1><p>上一篇中介绍了Bandit算法，并介绍了几种简单的实现，如 Epsilon-Greedy算法，Thompson sampling算法和UCB算法。</p>
<p>但是传统的实现方法存在很大的缺陷，主要是缺乏用附加信息刻画决策过程的机制。今天的文章就来介绍一种结合上下文信息的Bandit方法，LinUCB，它是Contextual bandits算法框架的一种。</p>
<p>本文的原文是雅虎的新闻推荐算法：<a href="https://arxiv.org/pdf/1003.0146.pdf。里面公式是真的挺多的，而且涉及到了两种linUCB算法，本文只介绍第一种方法。感兴趣的同学可以阅读原文。" target="_blank" rel="external">https://arxiv.org/pdf/1003.0146.pdf。里面公式是真的挺多的，而且涉及到了两种linUCB算法，本文只介绍第一种方法。感兴趣的同学可以阅读原文。</a></p>
<h2 id="1、LinUCB浅析"><a href="#1、LinUCB浅析" class="headerlink" title="1、LinUCB浅析"></a>1、LinUCB浅析</h2><p>这里只简单介绍一下LinUCB算法的流程，真的是浅析，浅析！</p>
<p>在推荐系统中，通常把待推荐的商品作为MAB问题的arm。UCB是context-free类的算法，没有充分利用推荐场景的上下文信息，为所有用户的选择展现商品的策略都是相同的，忽略了用户作为一个个活生生的个性本身的兴趣点、偏好、购买力等因素，因而，同一个商品在不同的用户、不同的情景下接受程度是不同的。故在实际的推荐系统中，context-free的MAB算法基本都不会被采用。</p>
<p>与context-free MAB算法对应的是Contextual Bandit算法，顾名思义，这类算法在实现E&amp;E时考虑了上下文信息，因而更加适合实际的个性化推荐场景。</p>
<p>在LinUCB中，每一个arm维护一组参数，用户和每一个arm的组合可以形成一个上下文特征（上下文特征的特征维度为d），那么对于一个用户来说，在每个arm上所能够获得的期望收益如下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-6efefe77e0374972.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>对于一个老虎机来说，假设收集到了m次反馈，特征向量可以写作Da(维度为m<em>d)，假设我们收到的反馈为Ca(维度为m</em>1)，那么通过求解下面的loss，我们可以得到当前每个老虎机的参数的最优解：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-98ae4c1ac8c0b953.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>这其实就是岭回归嘛，我们很容易得到最优解为：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-07f5daf6b8fd7cd7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>既然是UCB方法的扩展，我们除了得到期望值外，我们还需要一个置信上界，但是，我们没法继续用Chernoff-Hoeffding Bound的定理来量化这个上界，幸运的是，这个上界已经被人找到了：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-8dfc87ed3e41e2c4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>因此，我们推荐的item就能够确定了：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-a6fb06b46c5fe613.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>可以看到，我们在计算参数及最后推荐结果的时候，用到了以下几部分的信息：上下文特征x，用户的反馈c。而这些信息都是可以每次都存储下来的，因此在收集到了一定的信息之后，参数都可以动态更新，因此我们说LinUCB是一种<strong>在线学习方法</strong>。</p>
<p>什么是在线学习？个人简单的理解就是模型的训练和更新是在线进行的，能够实时的根据在线上的反馈更新模型的参数。</p>
<p>好了，我们来看一下linUCB算法的流程吧：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-8c2127f9ff3a7f65.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>上面的ba可以理解为特征向量x和反馈r的乘积。</p>
<p>是否觉得一头雾水，不用着急，我们通过代码来一步步解析上面的流程。</p>
<h2 id="2、linUCB代码实战"><a href="#2、linUCB代码实战" class="headerlink" title="2、linUCB代码实战"></a>2、linUCB代码实战</h2><p>本文的代码地址为：<a href="https://github.com/princewen/tensorflow_practice/blob/master/recommendation/Basic-Bandit-Demo/Basic-LinUCB.py" target="_blank" rel="external">https://github.com/princewen/tensorflow_practice/blob/master/recommendation/Basic-Bandit-Demo/Basic-LinUCB.py</a></p>
<p>###设定超参数和矩阵</p>
<p>首先我们设定一些超参数，比如α，正反馈和负反馈的奖励程度r1，r0，上下文特征的长度d</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">self.alpha = 0.25</div><div class="line">self.r1 = 0.6</div><div class="line">self.r0 = -16</div><div class="line">self.d = 6  # dimension of user features</div></pre></td></tr></table></figure>
<p>接下来，我们设定我们的几个矩阵，比如A和A的逆矩阵，b(x和r的乘积），以及参数矩阵：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">self.Aa = &#123;&#125; # Aa : collection of matrix to compute disjoint part for each article a, d*d</div><div class="line">self.AaI = &#123;&#125;  # AaI : store the inverse of all Aa matrix</div><div class="line"></div><div class="line">self.ba = &#123;&#125;  # ba : collection of vectors to compute disjoin part, d*1</div><div class="line">self.theta = &#123;&#125;</div></pre></td></tr></table></figure>
<p>###初始化矩阵<br>初始化矩阵对应上面的4-7步，A设置为单位矩阵，b设置为0矩阵，参数也设置为0矩阵，注意的是，每个arm都有这么一套矩阵：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">def set_articles(self,art):</div><div class="line">    for key in art:</div><div class="line">        self.Aa[key] = np.identity(self.d) # 创建单位矩阵</div><div class="line">        self.ba[key] = np.zeros((self.d,1))</div><div class="line"></div><div class="line">        self.AaI[key] = np.identity(self.d)</div><div class="line">        self.theta[key] = np.zeros((self.d,1))</div></pre></td></tr></table></figure>
<p>###计算推荐结果<br>计算推荐结果对应于上面的8-11步，我们直接根据公式计算当前的最优参数和置信上界，并选择最大的arm作为推荐结果。代码中有个小trick，及对所有的arm来说，共同使用一个特征，而不是每一个arm单独使用不同的特征：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">def recommend(self,timestamp,user_features,articles):</div><div class="line">    xaT = np.array([user_features]) # d * 1</div><div class="line">    xa = np.transpose(xaT)</div><div class="line"></div><div class="line">    AaI_tmp = np.array([self.AaI[article] for article in articles])</div><div class="line">    theta_tmp = np.array([self.theta[article] for article in articles])</div><div class="line">    art_max = articles[np.argmax(np.dot(xaT,theta_tmp) + self.alpha * np.sqrt(np.dot(np.dot(xaT,AaI_tmp),xa)))]</div><div class="line"></div><div class="line">    self.x = xa</div><div class="line">    self.xT = xaT</div><div class="line"></div><div class="line">    self.a_max = art_max</div><div class="line">    return self.a_max</div></pre></td></tr></table></figure>
<p>###更新矩阵信息<br>这对应于上面的12-13步，根据选择的最优arm，以及得到的用户反馈，我们更新A和b矩阵：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">def update(self,reward):</div><div class="line">    if reward == -1:</div><div class="line">        pass</div><div class="line">    elif reward == 1 or reward == 0:</div><div class="line">        if reward == 1:</div><div class="line">            r = self.r1</div><div class="line">        else:</div><div class="line">            r = self.r0</div><div class="line"></div><div class="line">        self.Aa[self.a_max] += np.dot(self.x,self.xT)</div><div class="line">        self.ba[self.a_max] += r * self.x</div><div class="line">        self.AaI[self.a_max] = np.linalg.inv(self.Aa[self.a_max])</div><div class="line">        self.theta[self.a_max] = np.dot(self.AaI[self.a_max],self.ba[self.a_max])</div><div class="line"></div><div class="line">    else:</div><div class="line">        # error</div></pre></td></tr></table></figure>
<p>写到这里，本来应该就要结束了，可是脑子里又想到一个问题，为什么可以直接通过加法来更新A矩阵？其实是个很简单的问题，试着写出A矩阵中每个元素的计算公式来，问题就迎刃而解了！</p>
<p>##结语<br>总结一下LinUCB算法，有以下优点（来自参考文献3，自己又增加了一条）：<br>1）由于加入了特征，所以收敛比UCB更快（论文有证明）；<br>2）特征构建是效果的关键，也是工程上最麻烦和值的发挥的地方；<br>3）由于参与计算的是特征，所以可以处理动态的推荐候选池，编辑可以增删文章；<br>4）特征降维很有必要，关系到计算效率。<br>5）是一种在线学习算法。</p>
<p>#参考文献<br>1、<a href="https://arxiv.org/pdf/1003.0146.pdf" target="_blank" rel="external">https://arxiv.org/pdf/1003.0146.pdf</a><br>2、<a href="https://zhuanlan.zhihu.com/p/35753281" target="_blank" rel="external">https://zhuanlan.zhihu.com/p/35753281</a><br>3、<a href="https://blog.csdn.net/legendavid/article/details/71082124" target="_blank" rel="external">https://blog.csdn.net/legendavid/article/details/71082124</a><br>4、<a href="https://zhuanlan.zhihu.com/p/32382432" target="_blank" rel="external">https://zhuanlan.zhihu.com/p/32382432</a></p>
<h1 id="推荐系统遇上深度学习-十四-–《DRN-A-Deep-Reinforcement-Learning-Framework-for-News-Recommendation》"><a href="#推荐系统遇上深度学习-十四-–《DRN-A-Deep-Reinforcement-Learning-Framework-for-News-Recommendation》" class="headerlink" title="推荐系统遇上深度学习(十四)–《DRN-A-Deep-Reinforcement-Learning-Framework-for-News-Recommendation》"></a>推荐系统遇上深度学习(十四)–《DRN-A-Deep-Reinforcement-Learning-Framework-for-News-Recommendation》</h1><p>之前学习了强化学习的一些内容以及推荐系统的一些内容，二者能否联系起来呢！今天阅读了一篇论文，题目叫《DRN: A Deep Reinforcement Learning Framework for News Recommendation》。该论文便是深度强化学习和推荐系统的一个结合，也算是提供了一个利用强化学习来做推荐的完整的思路和方法吧。本文便是对文章中的内容的一个简单的介绍，希望对大家有所启发。</p>
<h2 id="1、引言-2"><a href="#1、引言-2" class="headerlink" title="1、引言"></a>1、引言</h2><p>新闻领域的个性化推荐十分重要，传统的方法如基于内容的方法、协同过滤、深度学习方法在建模user-item交互关系时，经常面临以下三个问题：<br>1）难以处理新闻推荐的动态变化。这种动态变化体现在两个方面，首先新闻具有很强的时效性，其次是用户对于新闻阅读的兴趣是不断变化的，如下图所示：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-bfa2eb6125928f38.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>因此，在建模过程中，不仅要考虑用户对当前推荐的反馈，还要考虑长期的影响。就好比买股票，不能只考虑眼前的收益，而是要考虑未来的预期收益。<br>2）当前的推荐算法通常只考虑用户的点击／未点击 或者 用户的评分作为反馈，然而，用户隔多久会再次使用服务也能在一定程度上反映用户对推荐结果的满意度。<br>3）目前的推荐系统倾向于推荐用户重复或相似内容的东西，这也许会降低用户在同一个主题上的兴趣度。因此需要进行exploration。传统方法 e -greedy strategy 或者 Upper Con dence Bound (UCB) 都会在短期对推荐系统的效果造成一定的影响，需要更有效的exploration策略。</p>
<p>因此，本文提出了基于强化学习的推荐系统框架来解决上述提到的三个问题：<br>1）首先，使用DQN网络来有效建模新闻推荐的动态变化属性，DQN可以将短期回报和长期回报进行有效的模拟。<br>2）将用户活跃度（activeness score）作为一种新的反馈信息，用户活跃度在后面会详细介绍。<br>3）使用Dueling Bandit Gradient Descent方法来进行有效的探索。</p>
<p>算法的框架如下图所示：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-2c7dafc34881e607.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>本文的贡献主要有：<br>1）提出了一种强化学习的框架用于在线新闻的个性化推荐<br>2）使用用户活跃度作为一种新的反馈，来提高推荐的准确性<br>3）使用了一种更加高效的探索算法：Dueling Bandit Gra- dient Descent<br>4）模型可以进行在线学习和更新，在离线和在线实验上的表现都超过了传统的算法。</p>
<h2 id="2、问题定义"><a href="#2、问题定义" class="headerlink" title="2、问题定义"></a>2、问题定义</h2><p>下面是本文中的一些符号约定：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-3e50c9d6b97cf246.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<h2 id="3、模型详解"><a href="#3、模型详解" class="headerlink" title="3、模型详解"></a>3、模型详解</h2><h4 id="3-1-模型整体框架"><a href="#3-1-模型整体框架" class="headerlink" title="3.1 模型整体框架"></a>3.1 模型整体框架</h4><p>模型整体框架如下图所示：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-dea44a2b28da4d40.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>有几个关键的环节：<br><strong>PUSH</strong>：在每一个时刻，用户发送请求时，agent根据当前的state产生k篇新闻推荐给用户，这个推荐结果是exploitation和exploration的结合</p>
<p><strong>FEEDBACK</strong>：通过用户对推荐新闻的点击行为得到反馈结果。</p>
<p><strong>MINOR UPDATE</strong>：在每个时间点过后，根据用户的信息（state）和推荐的新闻（action）及得到的反馈（reward），agent会评估exploitation network Q 和 exploration network Q ̃ 的表现，如果exploitation network Q效果更好，则模型保持不动，如果 exploration network Q ̃ 的表现更好，exploitation network Q的参数将会向exploration network Q ̃变化。</p>
<p><strong>MAJOR UPDATE</strong>：在一段时间过后，根据DQN的经验池中存放的历史经验，对exploitation network Q 模型参数进行更新。</p>
<h4 id="3-2-特征设计"><a href="#3-2-特征设计" class="headerlink" title="3.2 特征设计"></a>3.2 特征设计</h4><p>DQN每次的输入有下面四部分的特征：</p>
<p><strong>新闻的特征</strong>：包括题目，作者，排名，类别等等，共417维<br><strong>用户的特征</strong>：包括用户在1小时，6小时，24小时，1周，1年内点击过的新闻的特征表示，共413<em>5=2065维。<br><strong>新闻和用户的交互特征</strong>：25维。<br><em>*上下文特征</em></em>：32维的上下文信息，如时间，周几，新闻的新鲜程度等。</p>
<p>在这四组特征中，用户特征和上下文特征用于表示当前的state，新闻特征和交互特征用语表示当前的一个action。</p>
<h4 id="3-3-深度强化学习作推荐"><a href="#3-3-深度强化学习作推荐" class="headerlink" title="3.3 深度强化学习作推荐"></a>3.3 深度强化学习作推荐</h4><p>这里深度强化学习用的是Dueling-Double-DQN。之前我们介绍过DQN的三大改进，包括Double-DQN，Dueling-DQN和优先经验回放，这里用到了两个。将用户特征和上下文特征用于表示当前的state，新闻特征和交互特征用语表示当前的一个action，经过模型可以输出当前状态state采取这个action的预测Q值。</p>
<p>Q现实值包含两个部分：立即获得的奖励和未来获得奖励的折现：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-977d7b5d7a2b4b88.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>立即的奖励可能包含两部分，即用户的点击奖励和用户活跃度奖励。由于采取了Double-DQN 的结构，Q现实值的计算变为：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-6e185841e4c9209f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>再加上Dueling的考虑，模型的网络结构如下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-f69372a3786abe8b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>文章中关于DQN的理论部分没有详细介绍，可以参考我之前写过的强化学习系列的文章进行理解。</p>
<h4 id="3-4-用户活跃度"><a href="#3-4-用户活跃度" class="headerlink" title="3.4 用户活跃度"></a>3.4 用户活跃度</h4><p>用户活跃度（User Activeness） 是本文提出的新的可以用作推荐结果反馈的指标。用户活跃度可以理解为使用app的频率，好的推荐结果可以增加用户使用该app的频率，因此可以作为一个反馈指标。</p>
<p>用户活跃度的图示如下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-d755ce22b36b0002.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>如果用户在一定时间内没有点击行为，活跃度会下降，但一旦有了点击行为，活跃度会上升。</p>
<p>在考虑了点击和活跃度之后，之前提到过的立即奖励变为：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-a7c554ed808879e6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<h4 id="3-5探索"><a href="#3-5探索" class="headerlink" title="3.5探索"></a>3.5探索</h4><p>本文的探索采取的是Dueling Bandit Gradient Descent 算法，算法的结构如下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-5f1bbdc0797fb6b0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>在DQN网络的基础上又多出来一个exploration network Q ̃ ，这个网络的参数是由当前的Q网络参数基础上加入一定的噪声产生的，具体来说：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-01d3ee4bbd46526c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>当一个用户请求到来时，由两个网络同时产生top-K的新闻列表，然后将二者产生的新闻进行一定程度的混合，然后得到用户的反馈。如果exploration network Q ̃的效果好的话，那么当前Q网络的参数向着exploration network Q ̃的参数方向进行更新，具体公式如下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-3655ed112493f3ea.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>否则的话，当前Q网络的参数不变。</p>
<p>总的来说，使用深度强化学习来进行推荐，同时考虑了用户活跃度和对多样性推荐的探索，可以说是一个很完备的推荐框架了！</p>
<h2 id="4、实验比较"><a href="#4、实验比较" class="headerlink" title="4、实验比较"></a>4、实验比较</h2><h4 id="4-1-数据集"><a href="#4-1-数据集" class="headerlink" title="4.1 数据集"></a>4.1 数据集</h4><p>使用的数据集是新闻app得到的数据：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-3601af8c6eccdd0c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>数据中存在明显的长尾特点：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-4e50f0f50332e38d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<h4 id="4-2-评估指标："><a href="#4-2-评估指标：" class="headerlink" title="4.2 评估指标："></a>4.2 评估指标：</h4><p>主要用的评估指标有CTR、top-K准确率，nDCG，三者的计算公式如下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-a117a20d3ea35d70.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-0e2cf2a24f5ba18e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-794df050c633aeb5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>在nDCG中，r是新闻的排序，n是推荐新闻列表的长度。</p>
<h4 id="4-3-实验设定"><a href="#4-3-实验设定" class="headerlink" title="4.3 实验设定"></a>4.3 实验设定</h4><p><img src="https://upload-images.jianshu.io/upload_images/4155986-acc7a474bc91e358.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<h4 id="4-4-对比模型"><a href="#4-4-对比模型" class="headerlink" title="4.4 对比模型"></a>4.4 对比模型</h4><p>本文选取了五个基准模型：LR，FM，Wide&amp;Deep，LinUCB，HLinUCB。同时根据组件的不同(U代表用户活跃度，EG代表e-greedy，DBGD代表Dueling Bandit Gradient De- scent ）强化学习模型又分为以下几种：</p>
<p><strong>DN</strong>：没有考虑未来收益的Double-DQN<br><strong>DDQN</strong>：考虑未来收益的Double-DQN<br><strong>DDON+U</strong>：考虑未来收益，同时考虑用户活跃度的Double-DQN<br><strong>DDQN+U+EG</strong>：采用e-greedy作为探索策略的Double-DQN<br><strong>DDQN+U+DBGD</strong>：采用DBGD作为探索模型的Double-DQN</p>
<h4 id="4-5-离线实验"><a href="#4-5-离线实验" class="headerlink" title="4.5 离线实验"></a>4.5 离线实验</h4><p>离线实验的结果如下所示：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-0cbcc9cd2e73ccef.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>本文提出的模型效果明显好于基准模型。</p>
<h4 id="4-6-在线实验"><a href="#4-6-在线实验" class="headerlink" title="4.6 在线实验"></a>4.6 在线实验</h4><p>在线实验的效果分两部分，准确率和Diversity。</p>
<p>准确率<br>准确率用CTR来表征，如果CTR高，我们认为模型准确率较好，实验结果如下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-912568b7cf7c7fb8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>多样性</p>
<p>多样性这里采用的指标是ILS，用来表示推荐列表中item的相似性，如果这个相似性较低，可以认为推荐结果的多样性较好，计算公式如下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-370bdc8452a29aa2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>实验结果如下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-69042c94745f5ddb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>可以看到DDQN+U+DBGD的多样性明显好于其他模型。</p>
<h1 id="推荐系统遇上深度学习-十五-–强化学习在京东推荐中的探索"><a href="#推荐系统遇上深度学习-十五-–强化学习在京东推荐中的探索" class="headerlink" title="推荐系统遇上深度学习(十五)–强化学习在京东推荐中的探索"></a>推荐系统遇上深度学习(十五)–强化学习在京东推荐中的探索</h1><p>强化学习在各个公司的推荐系统中已经有过探索，包括阿里、京东等。之前在美团做过的一个引导语推荐项目，背后也是基于强化学习算法。本文，我们先来看一下强化学习是如何在京东推荐中进行探索的。</p>
<p>本文来自于paper：《Deep Reinforcement Learning for List-wise Recommendations》</p>
<h2 id="1、引言-3"><a href="#1、引言-3" class="headerlink" title="1、引言"></a>1、引言</h2><p>传统的大多数推荐系统应用存在两个问题：<br>1）无法建模用户兴趣的动态变化<br>2）最大化立即收益，忽略了长期受益</p>
<p>因此，本文将推荐的过程定义为一个序列决策的问题，通过强化学习来进行 List-wise 的推荐，主要有以下几个部分。</p>
<p><strong>List-wise Recommendations</strong></p>
<p>本文提出的推荐是List-wise，这样更能提供给用户多样性的选择。现有的强化学习大多先计算每一个item的Q-value，然后通过排序得到最终的推荐结果，这样就忽略了推荐列表中商品本身的关联。</p>
<p>而List-wise的推荐，强化学习算法计算的是一整个推荐列表的Q-value，可以充分考虑列表中物品的相关性，从而提升推荐的性能。</p>
<p><strong>Architecture Selection</strong><br>对于深度强化学习的模型，主要有下面两种结构：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-1a7e60057b18469c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>左边的两个是经典的DQN结构，(a)这种结构只需要输入一个state，然后输出是所有动作的Q-value，当action太多时，这种结构明显的就不适用。(b)的输入时state和一个具体的action，然后模型的输出是一个具体的Q-value，但对于这个模型结构来说，时间复杂度非常高。</p>
<p>因此本文选择的深度强化学习结构是(c)，即Actor-Critic结构。Actor输入一个具体的state，输出一个action，然后Critic输入这个state和Actor输出的action，得到一个Q-value，Actor根据Critic的反馈来更新自身的策略。</p>
<p><strong>Online Environment Simulator</strong><br>在推荐系统上线之前，需要进行线下的训练和评估，训练和评估主要基于用户的历史行为数据，但是，我们只有ground-truth的数据和相应的反馈。因此，对于整个动作空间来说(也就是所有物品的可能组合)，这是非常稀疏的。这会造成两个问题，首先只能拿到部分的state-action对进行训练，无法对所有的情况进行建模(可能造成过拟合)，其次会造成线上线下环境的不一致性。因此，需要一个仿真器来仿真没有出现过的state-action的reward值，用于训练和评估线下模型。</p>
<p>仿真器的构建主要基于用户的历史数据，其基本思想是给定一个相似的state和action，不同的用户也会作出相似的feedback。</p>
<p>因此，本文的贡献主要有以下三点：<br>1）构建了一个线上环境仿真器，可以在线下对AC网络参数进行训练。<br>2）构建了基于强化学习的List-wise推荐系统。<br>3）在真实的电商环境中，本文提出的推荐系统框架的性能得到了证明。</p>
<h2 id="2、系统框架"><a href="#2、系统框架" class="headerlink" title="2、系统框架"></a>2、系统框架</h2><h4 id="2-1-问题描述"><a href="#2-1-问题描述" class="headerlink" title="2.1 问题描述"></a>2.1 问题描述</h4><p>本文的推荐系统基于强化学习方法，将推荐问题定义为一个马尔可夫决策过程，它的五个元素分别是：</p>
<p><strong>状态空间</strong><br>状态定义为用户的历史浏览行为，即在推荐之前，用户点击或购买过的最新的N个物品。<br><img src="https://upload-images.jianshu.io/upload_images/4155986-e10d69fa48205994.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p><strong>动作空间</strong><br>动作定义为要推荐给用户的商品列表。<br><img src="https://upload-images.jianshu.io/upload_images/4155986-0566ccbab3963044.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p><strong>奖励</strong><br>agent根据当前的state，采取相应的action即推荐K个物品列表给用户之后，根据用户对推荐列表的反馈(忽略、点击或购买)来得到当前state-action的即时奖励reward。<br><img src="https://upload-images.jianshu.io/upload_images/4155986-76d46fb6f51a2ae4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p><strong>转移概率</strong><br>在本文中，状态的转移定义如下定义，当前的state是用户最近浏览的N个物品，action是新推荐给用户的K个商品，如果用户忽略了全部的这些商品，那么下一个时刻的state和当前的state是一样的，如果用户点击了其中的两个物品，那么下一个时刻的state是在当前state的基础上，从前面剔除两个商品同时将点击的这两个物品放在最后得到的。<br><img src="https://upload-images.jianshu.io/upload_images/4155986-a00e2537e2dcec5c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-33878efd40cff99d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p><strong>折扣因子</strong><br><img src="https://upload-images.jianshu.io/upload_images/4155986-64bd6ed4a2d3bd2d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>这里还需要强调的一点是，本文中将物品当作一个单词，通过embedding的方式表示每一个物品，因此每一个state和action都是通过word embedding来表示的。</p>
<h4 id="2-2-线上User-Agent交互仿真环境构建"><a href="#2-2-线上User-Agent交互仿真环境构建" class="headerlink" title="2.2 线上User-Agent交互仿真环境构建"></a>2.2 线上User-Agent交互仿真环境构建</h4><p>仿真器主要基于历史数据，因此我们首先需要对历史真实数据的((state,action)-reward)对进行一个存储，这将作为仿真器的历史记忆：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-24ea9104c2946150.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>有了历史记忆之后，仿真器就可以输出没有见过的(state，action)对的奖励，该(state，action)定义为pt。首先需要计算pt和历史中状态-动作对的相似性，基于如下的公式：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-d7acbc028fa123e8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>上式中mi代表了历史记忆中的一条状态-动作对。因此pt获得mi对应的奖励ri的可能性定义如下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-e7121be601cb3954.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>但是，这种做法计算复杂度太高了，需要计算pt和历史记忆中每条记录的相似性，为了处理这个问题，本文的做法是按照奖励序列对历史记忆进行分组，来建模pt获得某个奖励序列的可能性。</p>
<p>奖励序列这里先解释一下，假设我们按一定的顺序推荐了两个商品，用户对每个商品的反馈可能有忽略／点击／下单，对应的奖励分别是0/1/5，那么我们推荐给用户这两个物品的反馈一共有九种可能的情况(0,0),(0,1),(0,5),(1,0),(1,1),(1,5),(5,0),(5,1),(5,5)。这九种情况就是我们刚才所说的奖励序列，定义为：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-9de3b313d6f36bdb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>因此，将历史记忆按照奖励序列进行分组，pt所能获得某个奖励序列的概率是：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-e04544fdaaedfdca.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>基于上面的公式，我们只是得到了pt所能获得的奖励序列的概率，就可以进行采样得到具体的奖励序列。得到奖励序列还没完事，实际中我们的奖励都是一个具体的值，而不是一个vector，那么按照如下的公式将奖励序列转化为一个具体的奖励值：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-6fb75530a0291d47.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>K是推荐列表的长度，可以看到，我们这里任务排在前面的商品，奖励的权重越高。</p>
<h4 id="2-3-模型结构"><a href="#2-3-模型结构" class="headerlink" title="2.3 模型结构"></a>2.3 模型结构</h4><p>使用强化学习里的AC模型结合刚才提到的仿真器，模型框架如下所示：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-ea0c8b302f0b0f61.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p><strong>Actor部分</strong></p>
<p>对Actor部分来说，输入是一个具体的state，输出一个K维的向量w，K对应推荐列表的长度：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-40eb640113d5ecfd.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>然后，用w和每个item对应的embedding进行线性相乘，计算每个item的得分，根据得分选择k个最高的物品作为推荐结果：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-a777c2c0dd3abea7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>Actor部分的过程如下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-052b331fc36e5cbf.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>推荐结果经过仿真器，计算出奖励序列和奖励值r。</p>
<p><strong>Critic部分</strong><br>Critic部分建模的是state-action对应的Q值，需要有Q-eval 和 Q-target来指导模型的训练，Q-eval通过Critic得到，而Q-target值通过下面的式子得到：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-df9dea8b1d13f7f0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<h2 id="3、实验评估"><a href="#3、实验评估" class="headerlink" title="3、实验评估"></a>3、实验评估</h2><p>论文中提到的实验主要想验证两方面的内容：<br>1）本文提出的框架与现有的推荐算法(如协同过滤，FM等)比，效果如何<br>2）List-Wise的推荐与item-wise推荐相比，效果是否更突出。</p>
<p>不过，文章中没有给出具体的实验结果，这部分的效果还不得而知。</p>
<p>#欢迎关注个人公众号：小小挖掘机</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-2a82a353433e8a07.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<h1 id="推荐系统遇上深度学习-十六-–详解推荐系统中的常用评测指标"><a href="#推荐系统遇上深度学习-十六-–详解推荐系统中的常用评测指标" class="headerlink" title="推荐系统遇上深度学习(十六)–详解推荐系统中的常用评测指标"></a>推荐系统遇上深度学习(十六)–详解推荐系统中的常用评测指标</h1><p>最近阅读论文的过程中，发现推荐系统中的评价指标真的是五花八门，今天我们就来系统的总结一下，这些指标有的适用于二分类问题，有的适用于对推荐列表topk的评价。</p>
<h2 id="1、精确率、召回率、F1值"><a href="#1、精确率、召回率、F1值" class="headerlink" title="1、精确率、召回率、F1值"></a>1、精确率、召回率、F1值</h2><p>我们首先来看一下混淆矩阵，对于二分类问题，真实的样本标签有两类，我们学习器预测的类别有两类，那么根据二者的类别组合可以划分为四组，如下表所示：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/4155986-5aba3ad4e656a9ca?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>上表即为<strong>混淆矩阵</strong>，其中，行表示预测的label值，列表示真实label值。TP，FP，FN，TN分别表示如下意思：</p>
<p><strong>TP（true positive）</strong>：表示样本的真实类别为正，最后预测得到的结果也为正；<br><strong>FP（false positive）</strong>：表示样本的真实类别为负，最后预测得到的结果却为正；<br><strong>FN（false negative）</strong>：表示样本的真实类别为正，最后预测得到的结果却为负；<br><strong>TN（true negative）</strong>：表示样本的真实类别为负，最后预测得到的结果也为负.</p>
<p>可以看到，TP和TN是我们预测准确的样本，而FP和FN为我们预测错误的样本。</p>
<p>基于混淆矩阵，我们可以得到如下的评测指标：</p>
<p><strong>准确率</strong></p>
<p>准确率表示的是分类正确的样本数占样本总数的比例，假设我们预测了10条样本，有8条的预测正确，那么准确率即为80%。</p>
<p>用混淆矩阵计算的话，准确率可以表示为：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/4155986-671a7f6cdb03cd25?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p><strong>精确率／召回率</strong></p>
<p><strong>精确率</strong>表示预测结果中，预测为正样本的样本中，正确预测为正样本的概率；<br><strong>召回率</strong>表示在原始样本的正样本中，最后被正确预测为正样本的概率；</p>
<p>二者用混淆矩阵计算如下：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/4155986-0e3eac49259844d8?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p><strong>F1值</strong></p>
<p>为了折中精确率和召回率的结果，我们又引入了F-1 Score，计算公式如下：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/4155986-3a296a23b09d71b3?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<h2 id="2、AUC"><a href="#2、AUC" class="headerlink" title="2、AUC"></a>2、AUC</h2><p>AUC定义为ROC曲线下方的面积：</p>
<p>ROC曲线的横轴为“假正例率”（False Positive Rate,FPR)，又称为“假阳率”；纵轴为“真正例率”(True Positive Rate,TPR)，又称为“真阳率”，</p>
<p>假阳率，简单通俗来理解就是预测为正样本但是预测错了的可能性，显然，我们不希望该指标太高。</p>
<p><img src="http://upload-images.jianshu.io/upload_images/4155986-49e446be096fe4a1?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>真阳率，则是代表预测为正样本但是预测对了的可能性，当然，我们希望真阳率越高越好。</p>
<p><img src="http://upload-images.jianshu.io/upload_images/4155986-be34a2e974da46f0?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>下图就是我们绘制的一张ROC曲线图，曲线下方的面积即为AUC的值：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/4155986-f1cf1e33db359f61?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>AUC还有另一种解释，就是<strong>测试任意给一个正类样本和一个负类样本，正类样本的score有多大的概率大于负类样本的score</strong>。</p>
<h2 id="3、Hit-Ratio-HR"><a href="#3、Hit-Ratio-HR" class="headerlink" title="3、Hit Ratio(HR)"></a>3、Hit Ratio(HR)</h2><p>在top-K推荐中，HR是一种常用的衡量召回率的指标，其计算公式如下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-0a2af3358b3fae95.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>分母是所有的测试集合，分子式每个用户top-K推荐列表中属于测试集合的个数的总和。举个简单的例子，三个用户在测试集中的商品个数分别是10，12，8，模型得到的top-10推荐列表中，分别有6个，5个，4个在测试集中，那么此时HR的值是 (6+5+4)/(10+12+8) = 0.5。</p>
<h2 id="4、Mean-Average-Precision-MAP"><a href="#4、Mean-Average-Precision-MAP" class="headerlink" title="4、Mean Average Precision(MAP)"></a>4、Mean Average Precision(MAP)</h2><p>在了解MAP(Mean Average Precision)之前，先来看一下AP(Average Precision), 即为平均准确率。</p>
<p>对于AP可以用这种方式理解: 假使当我们使用google搜索某个关键词，返回了10个结果。当然最好的情况是这10个结果都是我们想要的相关信息。但是假如只有部分是相关的，比如5个，那么这5个结果如果被显示的比较靠前也是一个相对不错的结果。但是如果这个5个相关信息从第6个返回结果才开始出现，那么这种情况便是比较差的。这便是AP所反映的指标，与recall的概念有些类似，不过是“顺序敏感的recall”。</p>
<p>比如对于用户 u, 我们给他推荐一些物品，那么 u 的平均准确率定义为：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-b823ba5c48118494.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>用一个例子来解释AP的计算过程：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-4a79c572c058b978.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>因此该user的AP为（1 + 0.66 + 0.5） ／ 3 = 0.72</p>
<p>那么对于MAP(Mean Average Precision)，就很容易知道即为所有用户 u 的AP再取均值(mean)而已。那么计算公式如下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-8a2e764a98d17eba.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<h2 id="5、Normalized-Discounted-Cummulative-Gain-NDCG"><a href="#5、Normalized-Discounted-Cummulative-Gain-NDCG" class="headerlink" title="5、Normalized Discounted Cummulative Gain(NDCG)"></a>5、Normalized Discounted Cummulative Gain(NDCG)</h2><p>对于NDCG，我们需要一步步揭开其神秘的面纱，先从CG说起：<br><strong>CG</strong><br>我们先从CG(Cummulative Gain)说起, 直接翻译的话叫做“累计增益”。 在推荐系统中，CG即将每个推荐结果相关性(relevance)的分值累加后作为整个推荐列表(list)的得分。即</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-e75f3bd1a822ea0f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>这里， rel-i 表示处于位置 i 的推荐结果的相关性，k 表示所要考察的推荐列表的大小。</p>
<p><strong>DCG</strong><br>CG的一个缺点是没有考虑每个推荐结果处于不同位置对整个推荐效果的影响，例如我们总是希望相关性高的结果应排在前面。显然，如果相关性低的结果排在靠前的位置会严重影响用户体验， 所以在CG的基础上引入位置影响因素，即DCG(Discounted Cummulative Gain), “Discounted”有打折，折扣的意思，这里指的是对于排名靠后推荐结果的推荐效果进行“打折处理”:</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-b631860a8264420a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>从上面的式子可以得到两个结论：<br>1）推荐结果的相关性越大，DCG越大。<br>2）相关性好的排在推荐列表的前面的话，推荐效果越好，DCG越大。</p>
<p><strong>NDCG</strong><br>DCG仍然有其局限之处，即不同的推荐列表之间，很难进行横向的评估。而我们评估一个推荐系统，不可能仅使用一个用户的推荐列表及相应结果进行评估， 而是对整个测试集中的用户及其推荐列表结果进行评估。 那么不同用户的推荐列表的评估分数就需要进行归一化，也即NDCG(Normalized Discounted Cummulative Gain)。</p>
<p>在介绍NDCG之前，还需要了解一个概念：IDCG. IDCG, 即Ideal DCG， 指推荐系统为某一用户返回的最好推荐结果列表， 即假设返回结果按照相关性排序， 最相关的结果放在最前面， 此序列的DCG为IDCG。因此DCG的值介于 (0,IDCG] ，故NDCG的值介于(0,1]，那么用户u的NDCG@K定义为：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-11a7a60a51d7c56e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>因此，平均NDCG计算为：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-5710b4ca602b8519.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p><strong>NDCG的完整案例</strong><br>看了上面的介绍，是不是感觉还是一头雾水，不要紧张，我们通过一个案例来具体介绍一下。</p>
<p>假设在Baidu搜索到一个词，得到5个结果。我们对这些结果进行3个等级的分区，对应的分值分别是3、2、1，等级越高，表示相关性越高。假设这5个结果的分值分别是3、1、2、3、2。</p>
<p>因此CG的计算结果为3+1+2+3+2 = 11。DCG的值为6.69，具体见下表：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-e19e155504234f93.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>理想状况下，我们的IDCG排序结果的相关性应该是3，3，2，2，1，因此IDCG为7.14(具体过程不再给出)，因此NDCG结果为6.69/7.14 = 0.94。</p>
<h2 id="6、Mean-Reciprocal-Rank-MRR"><a href="#6、Mean-Reciprocal-Rank-MRR" class="headerlink" title="6、Mean Reciprocal Rank (MRR)"></a>6、Mean Reciprocal Rank (MRR)</h2><p>MRR计算公式如下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-2c1cfa453ef67a4e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>其中|Q|是用户的个数，ranki是对于第i个用户，推荐列表中第一个在ground-truth结果中的item所在的排列位置。</p>
<p>举个例子，有三个用户，推荐列表中正例的最小rank值分别为3，2，1，那么MRR=(1 + 0.5 + 0.33) / 3 = 0.61</p>
<h2 id="7、ILS"><a href="#7、ILS" class="headerlink" title="7、ILS"></a>7、ILS</h2><p>ILS是衡量推荐列表多样性的指标，计算公式如下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-4cf62bc4715da3a7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>如果S(bi,bj)计算的是i和j两个物品的相似性，如果推荐列表中物品越不相似，ILS越小，那么推荐结果的多样性越好。</p>
<h2 id="8、代码实践"><a href="#8、代码实践" class="headerlink" title="8、代码实践"></a>8、代码实践</h2><p>本文实践了部分上面提到的评价指标，git地址为：<a href="https://github.com/princewen/tensorflow_practice/tree/master/recommendation/Basic-Evaluation-metrics" target="_blank" rel="external">https://github.com/princewen/tensorflow_practice/tree/master/recommendation/Basic-Evaluation-metrics</a></p>
<p>参考目录为：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-3baae7459c56d7d8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>#参考文献<br>1、<a href="https://blog.csdn.net/simple_the_best/article/details/52296608" target="_blank" rel="external">https://blog.csdn.net/simple_the_best/article/details/52296608</a><br>2、<a href="https://blog.csdn.net/u010670689/article/details/73196054" target="_blank" rel="external">https://blog.csdn.net/u010670689/article/details/73196054</a><br>3、<a href="https://www.cnblogs.com/wzyj/p/8976185.html" target="_blank" rel="external">https://www.cnblogs.com/wzyj/p/8976185.html</a><br>4、<a href="https://blog.csdn.net/u014313009/article/details/38944687" target="_blank" rel="external">https://blog.csdn.net/u014313009/article/details/38944687</a></p>
<p>#欢迎关注个人公众号：小小挖掘机</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-2a82a353433e8a07.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<h1 id="推荐系统遇上深度学习-十七-–探秘阿里之MLR算法浅析及实现"><a href="#推荐系统遇上深度学习-十七-–探秘阿里之MLR算法浅析及实现" class="headerlink" title="推荐系统遇上深度学习(十七)–探秘阿里之MLR算法浅析及实现"></a>推荐系统遇上深度学习(十七)–探秘阿里之MLR算法浅析及实现</h1><p>阿里近几年公开的推荐领域算法可真不少，既有传统领域的探索如MLR算法，还有深度学习领域的探索如entire -space multi-task model，Deep Interest Network等，同时跟清华大学合作展开了强化学习领域的探索，提出了MARDPG算法。从本篇开始，我们就一起来探秘这些算法。这里，我们只是大体了解一下每一个算法的思路，对于数学部分的介绍，我们不会过多的涉及。</p>
<h2 id="1、算法介绍"><a href="#1、算法介绍" class="headerlink" title="1、算法介绍"></a>1、算法介绍</h2><p><strong>现阶段各CTR预估算法的不足</strong><br>我们这里的现阶段，不是指的今时今日，而是阿里刚刚公开此算法的时间，大概就是去年的三四月份吧。</p>
<p>业界常用的CTR预估算法的不足如下表所示：</p>
<table>
<thead>
<tr>
<th>方法</th>
<th style="text-align:center">简介</th>
<th style="text-align:center">不足</th>
</tr>
</thead>
<tbody>
<tr>
<td>逻辑回归</td>
<td style="text-align:center">使用了Sigmoid函数将函数值映射到0~1区间作为CTR的预估值。LR这种线性模型很容易并行化，处理上亿条训练样本不是问题。</td>
<td style="text-align:center">线性模型的学习能力有限，需要引入大量的领域知识来人工设计特征以及特征之间的交叉组合来间接补充算法的非线性学习能力，非常消耗人力和机器资源，迁移性不够友好。</td>
</tr>
<tr>
<td>Kernel方法</td>
<td style="text-align:center">将低维特征映射到高维特征空间</td>
<td style="text-align:center">复杂度太高而不易实现</td>
</tr>
<tr>
<td>树模型</td>
<td style="text-align:center">如Facebook的GBDT+LR算法，有效地解决了LR模型的特征组合问题</td>
<td style="text-align:center">是对历史行为的记忆，缺乏推广性，树模型只能学习到历史数据中的特定规则，对于新规则缺乏推广性</td>
</tr>
<tr>
<td>FM模型</td>
<td style="text-align:center">自动学习高阶属性的权值，不用通过人工的方式选取特征来做交叉</td>
<td style="text-align:center">FM模型只能拟合特定的非线性模式，常用的就是二阶FM</td>
</tr>
<tr>
<td>深度神经网络</td>
<td style="text-align:center">使用神经网络拟合数据之间的高阶非线性关系，非线性拟合能力足够强</td>
<td style="text-align:center">适合数据规律的、具备推广性的网络结构业界依然在探索中，尤其是要做到端到端规模化上线，这里面的技术挑战依然很大</td>
</tr>
</tbody>
</table>
<p><strong>那么挑战来了，如何设计算法从大规模数据中挖掘出具有推广性的非线性模式？</strong></p>
<p><strong>MLR算法</strong></p>
<p>2011-2012年期间，阿里妈妈资深专家盖坤创新性地提出了MLR(mixed logistic regression)算法，引领了广告领域CTR预估算法的全新升级。MLR算法创新地提出并实现了直接在原始空间学习特征之间的非线性关系，基于数据自动发掘可推广的模式，相比于人工来说效率和精度均有了大幅提升。</p>
<p>MLR可以看做是对LR的一个自然推广，它采用分而治之的思路，用分片线性的模式来拟合高维空间的非线性分类面，其形式化表达如下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-d4572939999edaf3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>其中u是聚类参数，决定了空间的划分，w是分类参数，决定空间内的预测。这里面超参数分片数m可以较好地平衡模型的拟合与推广能力。当m=1时MLR就退化为普通的LR，m越大模型的拟合能力越强，但是模型参数规模随m线性增长，相应所需的训练样本也随之增长。因此实际应用中m需要根据实际情况进行选择。例如，在阿里的场景中，m一般选择为12。下图中MLR模型用4个分片可以完美地拟合出数据中的菱形分类面。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-d9da3d968a2fdc1c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>在实际中，MLR算法常用的形式如下，使用softmax作为分片函数：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-ab8a627ded650751.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>在这种情况下，MLR模型可以看作是一个FOE model：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-c217d227121fa10c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>关于损失函数的设计，阿里采用了 neg-likelihood loss function以及L1，L2正则，形式如下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-65a604bb693beebf.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>由于加入了正则项，MLR算法变的不再是平滑的凸函数，梯度下降法不再适用，因此模型参数的更新使用LBFGS和OWLQN的结合，具体的优化细节大家可以参考论文(<a href="https://arxiv.org/pdf/1704.05194.pdf" target="_blank" rel="external">https://arxiv.org/pdf/1704.05194.pdf</a>).</p>
<p>MLR算法适合于工业级的大规模稀疏数据场景问题，如广告CTR预估。背后的优势体现在两个方面：<br><strong>端到端的非线性学习</strong>：从模型端自动挖掘数据中蕴藏的非线性模式，省去了大量的人工特征设计，这 使得MLR算法可以端到端地完成训练，在不同场景中的迁移和应用非常轻松。<br><strong>稀疏性</strong>：MLR在建模时引入了L1和L2,1范数正则，可以使得最终训练出来的模型具有较高的稀疏度， 模型的学习和在线预测性能更好。当然，这也对算法的优化求解带来了巨大的挑战。</p>
<h2 id="2、算法简单实现"><a href="#2、算法简单实现" class="headerlink" title="2、算法简单实现"></a>2、算法简单实现</h2><p>我们这里只是简单实现一个tensorflow版本的MLR模型，通过代码来了解一下模型的思想。</p>
<p>代码的github地址为：<a href="https://github.com/princewen/tensorflow_practice/tree/master/recommendation/Basic-MLR-Demo" target="_blank" rel="external">https://github.com/princewen/tensorflow_practice/tree/master/recommendation/Basic-MLR-Demo</a></p>
<p>所使用的数据下载地址为：<a href="http://archive.ics.uci.edu/ml/datasets/Adult，该数据是一个二分类的数据，所预测的任务是判断一个人是否能够一年挣到50K的钱，数据介绍如下：" target="_blank" rel="external">http://archive.ics.uci.edu/ml/datasets/Adult，该数据是一个二分类的数据，所预测的任务是判断一个人是否能够一年挣到50K的钱，数据介绍如下：</a></p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-8ee65bfd7a611be0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p><strong>数据处理</strong></p>
<p>数据中存在连续特征和离散特征，所以我们先要对数据进行一个简单的处理，处理包括将离散特征转换为one-hot以及对连续特征进行标准化。有一个需要注意的地方，训练集和测试集中离散特征出现的个数可能不一样，因此需要先将数据合并，然后转换成one-hot，最后再分开，代码如下。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div></pre></td><td class="code"><pre><div class="line">import pandas as pd</div><div class="line">from sklearn.preprocessing import StandardScaler</div><div class="line"></div><div class="line">def get_data():</div><div class="line">    train_data = pd.read_table(&quot;data/adult.data.txt&quot;,header=None,delimiter=&apos;,&apos;)</div><div class="line">    test_data = pd.read_table(&quot;data/adult.test.txt&quot;,header=None,delimiter=&apos;,&apos;)</div><div class="line"></div><div class="line">    all_columns = [&apos;age&apos;,&apos;workclass&apos;,&apos;fnlwgt&apos;,&apos;education&apos;,&apos;education-num&apos;,</div><div class="line">                        &apos;marital-status&apos;,&apos;occupation&apos;,&apos;relationship&apos;,&apos;race&apos;,&apos;sex&apos;,</div><div class="line">                        &apos;capital-gain&apos;,&apos;capital-loss&apos;,&apos;hours-per-week&apos;,&apos;native-country&apos;,&apos;label&apos;,&apos;type&apos;]</div><div class="line"></div><div class="line">    continus_columns = [&apos;age&apos;,&apos;fnlwgt&apos;,&apos;education-num&apos;,&apos;capital-gain&apos;,&apos;capital-loss&apos;,&apos;hours-per-week&apos;]</div><div class="line">    dummy_columns = [&apos;workclass&apos;,&apos;education&apos;,&apos;marital-status&apos;,&apos;occupation&apos;,&apos;relationship&apos;,&apos;race&apos;,&apos;sex&apos;,&apos;native-country&apos;]</div><div class="line"></div><div class="line">    train_data[&apos;type&apos;] = 1</div><div class="line">    test_data[&apos;type&apos;] = 2</div><div class="line"></div><div class="line">    all_data = pd.concat([train_data,test_data],axis=0)</div><div class="line">    all_data.columns = all_columns</div><div class="line"></div><div class="line">    all_data = pd.get_dummies(all_data,columns=dummy_columns)</div><div class="line">    test_data = all_data[all_data[&apos;type&apos;]==2].drop([&apos;type&apos;],axis=1)</div><div class="line">    train_data = all_data[all_data[&apos;type&apos;]==1].drop([&apos;type&apos;],axis=1)</div><div class="line"></div><div class="line">    train_data[&apos;label&apos;] = train_data[&apos;label&apos;].map(lambda x: 1 if x.strip() == &apos;&gt;50K&apos; else 0)</div><div class="line">    test_data[&apos;label&apos;] = test_data[&apos;label&apos;].map(lambda x: 1 if x.strip() == &apos;&gt;50K.&apos; else 0)</div><div class="line"></div><div class="line">    for col in continus_columns:</div><div class="line">        ss = StandardScaler()</div><div class="line">        train_data[col] = ss.fit_transform(train_data[[col]])</div><div class="line">        test_data[col] = ss.transform(test_data[[col]])</div><div class="line"></div><div class="line">    train_y = train_data[&apos;label&apos;]</div><div class="line">    train_x = train_data.drop([&apos;label&apos;],axis=1)</div><div class="line">    test_y = test_data[&apos;label&apos;]</div><div class="line">    test_x = test_data.drop([&apos;label&apos;],axis=1)</div><div class="line"></div><div class="line">    return train_x,train_y,test_x,test_y</div></pre></td></tr></table></figure>
<p>数据处理完后，特征的维度是108维。</p>
<p><strong>MLR的实现</strong></p>
<p>MLR的实现需要两组参数，分别是聚类参数和分类参数：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">u = tf.Variable(tf.random_normal([108,m],0.0,0.5),name=&apos;u&apos;)</div><div class="line">w = tf.Variable(tf.random_normal([108,m],0.0,0.5),name=&apos;w&apos;)</div></pre></td></tr></table></figure>
<p>随后，我们要计算我们的预估值：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">U = tf.matmul(x,u)</div><div class="line">p1 = tf.nn.softmax(U)</div><div class="line"></div><div class="line">W = tf.matmul(x,w)</div><div class="line">p2 = tf.nn.sigmoid(W)</div><div class="line"></div><div class="line">pred = tf.reduce_sum(tf.multiply(p1,p2),1)</div></pre></td></tr></table></figure>
<p>损失函数我们刚才介绍过了，在tensorflow中，我们选择FtrlOptimizer作为优化器，可以给我们的损失函数加上正则项：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">cost1=tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=pred, labels=y))</div><div class="line">cost=tf.add_n([cost1])</div><div class="line">train_op = tf.train.FtrlOptimizer(learning_rate).minimize(cost)</div></pre></td></tr></table></figure>
<p>随后，我们就可以进行试验了。</p>
<p><strong>实验结果</strong><br>本文对比了在当前给出的数据集下，m=5，10，15，25 以及lr算法的效果，结果如下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-236632a5e412a2bb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-5fd20cafb4a3fef7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>可以看到，lr的效果是最好的，随着m的增加，模型的效果越来越差。当然，这并不能说明mlr效果不如lr好，只是我们的数据实在是太少了，哈哈。</p>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>1、<a href="https://mp.weixin.qq.com/s?__biz=MzIzOTU0NTQ0MA==&amp;mid=2247485097&amp;idx=1&amp;sn=6dbc197e67e8a2ba3ee78786b13d894d&amp;scene=21#wechat_redirect" target="_blank" rel="external">https://mp.weixin.qq.com/s?__biz=MzIzOTU0NTQ0MA==&amp;mid=2247485097&amp;idx=1&amp;sn=6dbc197e67e8a2ba3ee78786b13d894d&amp;scene=21#wechat_redirect</a><br>2、Learning Piece-wise Linear Models<br>from Large Scale Data for Ad Click Prediction</p>
<h1 id="推荐系统遇上深度学习-十八-–探秘阿里之深度兴趣网络-DIN-浅析及实现"><a href="#推荐系统遇上深度学习-十八-–探秘阿里之深度兴趣网络-DIN-浅析及实现" class="headerlink" title="推荐系统遇上深度学习(十八)–探秘阿里之深度兴趣网络(DIN)浅析及实现"></a>推荐系统遇上深度学习(十八)–探秘阿里之深度兴趣网络(DIN)浅析及实现</h1><p>阿里近几年公开的推荐领域算法有许多，既有传统领域的探索如MLR算法，还有深度学习领域的探索如entire -space multi-task model，Deep Interest Network等，同时跟清华大学合作展开了强化学习领域的探索，提出了MARDPG算法。</p>
<p>上一篇，我们介绍了MLR算法，通过分而治之的思想改进了传统的LR算法，使其能够拟合更复杂的线性关系。这一篇，我们来简单理解和实现一下阿里在去年提出的另一个重要的推荐系统模型-深度兴趣网络(DIN,Deep Interest Network). 该方法由盖坤大神领导的阿里妈妈的精准定向检索及基础算法团队提出，充分利用/挖掘用户历史行为数据中的信息来提高CTR预估的性能。</p>
<h2 id="1、背景-3"><a href="#1、背景-3" class="headerlink" title="1、背景"></a>1、背景</h2><p>深度学习在CTR预估领域已经有了广泛的应用，常见的算法比如Wide&amp;Deep，DeepFM等。这些方法一般的思路是：通过Embedding层，将高维离散特征转换为固定长度的连续特征，然后通过多个全联接层，最后通过一个sigmoid函数转化为0-1值，代表点击的概率。即<strong>Sparse Features -&gt; Embedding Vector -&gt; MLPs -&gt; Sigmoid -&gt; Output</strong>.</p>
<p>这种方法的优点在于：通过神经网络可以拟合高阶的非线性关系，同时减少了人工特征的工作量。</p>
<p>不过，阿里的研究者们通过观察收集到的线上数据，发现了用户行为数据中有两个很重要的特性：</p>
<p><strong>Diversity</strong>：用户在浏览电商网站的过程中显示出的兴趣是十分多样性的。<br><strong>Local activation</strong>: 由于用户兴趣的多样性，只有部分历史数据会影响到当次推荐的物品是否被点击，而不是所有的历史记录。</p>
<p>这两种特性是密不可分的。</p>
<p>举个简单的例子，观察下面的表格：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-376938a54c692cbb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p><strong>Diversity</strong>体现在年轻的母亲的历史记录中体现的兴趣十分广泛，涵盖羊毛衫、手提袋、耳环、童装、运动装等等。而爱好游泳的人同样兴趣广泛，历史记录涉及浴装、旅游手册、踏水板、马铃薯、冰激凌、坚果等等。</p>
<p><strong>Local activation</strong>体现在，当我们给爱好游泳的人推荐goggle(护目镜)时，跟他之前是否购买过薯片、书籍、冰激凌的关系就不大了，而跟他游泳相关的历史记录如游泳帽的关系就比较密切。</p>
<p>针对上面提到的用户行为中存在的两种特性，阿里将其运用于自身的推荐系统中，推出了深度兴趣网路DIN，接下来，我们就一起来看一下模型的一些实现细节，然后我们会给出一个简化版的tensorflow实现。</p>
<h2 id="2、模型设计"><a href="#2、模型设计" class="headerlink" title="2、模型设计"></a>2、模型设计</h2><p><strong>整体框架</strong><br>我们先来看一下推荐系统的整体框架：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-85567850816136ef.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>整个流程可以描述为：<br>1.检查用户历史行为数据<br>2.使用matching module产生候选ads。<br>3.通过ranking module做point-wise的排序，即得到每个候选ads的点击概率，并根据概率排序得到推荐列表。<br>4.记录下用户在当前展示广告下的反应(点击与否)，作为label。</p>
<p><strong>特征设计</strong></p>
<p>本文将所涉及到的特征分为四个部分：<strong>用户特征、用户行为特征、广告特征、上下文特征</strong>，具体如下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-11706f64ea66cf2e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>其中，用户行为特征是multi-hot的，即多值离散特征。针对这种特征，由于每个涉及到的非0值个数是不一样的，常见的做法就是将id转换成embedding之后，加一层pooling层，比如average-pooling，sum-pooling，max-pooling。DIN中使用的是weighted-sum，其实就是加权的sum-pooling，权重经过一个activation unit计算得到。这里我们后面还会再介绍到。</p>
<p><strong>BaseModel</strong></p>
<p>在介绍DIN之前，我们先来看一下一个基准模型，结构如下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-f73cc229c99980ae.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>这里element-wise的意思其实就是元素级别的加减，同时，可不要忽略广播的存在哟。一个元素和一个向量相乘，也可以看作element-wise的，因为这个元素会广播成和向量一样的长度嘛，嘻嘻。</p>
<p>可以看到，Base Model首先吧one-hot或multi-hot特征转换为特定长度的embedding，作为模型的输入，然后经过一个DNN的part，得到最终的预估值。特别地，针对multi-hot的特征，做了一次element-wise+的操作，这里其实就是sum-pooling，这样，不管特征中有多少个非0值，经过转换之后的长度都是一样的！</p>
<p><strong>Deep Interest Network</strong><br>Base Model有一个很大的问题，它对用户的历史行为是同等对待的，没有做任何处理，这显然是不合理的。一个很显然的例子，离现在越近的行为，越能反映你当前的兴趣。因此，对用户历史行为基于Attention机制进行一个加权，阿里提出了深度兴趣网络（Deep Interest Network)，先来看一下模型结构：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-4250d6869c6481a3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>Attention机制简单的理解就是，针对不同的广告，用户历史行为与该广告的权重是不同的。假设用户有ABC三个历史行为，对于广告D，那么ABC的权重可能是0.8、0.1、0.1；对于广告E，那么ABC的权重可能是0.3、0.6、0.1。这里的权重，就是Attention机制即上图中的Activation Unit所需要学习的。</p>
<p>为什么要引入这一个机制呢？难道仅仅是通过观察历史数据拍脑袋决定的么？当然不是，如果不用Local activation的话，将会出现下面的情况：假设用户的兴趣的Embedding是Vu，候选广告的Embedding是Va，用户兴趣和候选的广告的相关性可以写作F(U,A) = Va * Vu。如果没有Local activation机制的话，那么同一个用户对于不同的广告，Vu都是相同的。举例来说，如果有两个广告A和B，用户兴趣和A，B的相似性都很高，那么在Va和Vb连线上的广告都会有很高的相似性。这样的限制使得模型非常难学习到有效的用户和广告的embedidng表示。</p>
<p>在加入Activation Unit之后，用户的兴趣表示计算如下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-482bdce2c1bee47c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>其中，Vi表示behavior id i的嵌入向量，比如good_id,shop_id等。Vu是所有behavior ids的加权和，表示的是用户兴趣；Va是候选广告的嵌入向量；wi是候选广告影响着每个behavior id的权重，也就是Local Activation。wi通过Activation Unit计算得出，这一块用函数去拟合，表示为g(Vi,Va)。</p>
<h2 id="3、模型细节"><a href="#3、模型细节" class="headerlink" title="3、模型细节"></a>3、模型细节</h2><h4 id="3-1-评价指标GAUC"><a href="#3-1-评价指标GAUC" class="headerlink" title="3.1 评价指标GAUC"></a>3.1 评价指标GAUC</h4><p>模型使用的评价指标是GAUC，我们先来看一下GAUC的计算公式：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-358cc3c6656a5408.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>我们首先要肯定的是，AUC是要分用户看的，我们的模型的预测结果，只要能够保证对每个用户来说，他想要的结果排在前面就好了。</p>
<p>假设有两个用户A和B，每个用户都有10个商品，10个商品中有5个是正样本，我们分别用TA，TB，FA，FB来表示两个用户的正样本和负样本。也就是说，20个商品中有10个是正样本。假设模型预测的结果大小排序依次为TA，FA，TB，FB。如果把两个用户的结果混起来看，AUC并不是很高，因为有5个正样本排在了后面，但是分开看的话，每个用户的正样本都排在了负样本之前，AUC应该是1。显然，分开看更容易体现模型的效果，这样消除了用户本身的差异。</p>
<p>但是上文中所说的差异是在用户点击数即样本数相同的情况下说的。还有一种差异是用户的展示次数或者点击数，如果一个用户有1个正样本，10个负样本，另一个用户有5个正样本，50个负样本，这种差异同样需要消除。那么GAUC的计算，不仅将每个用户的AUC分开计算，同时根据用户的展示数或者点击数来对每个用户的AUC进行加权处理。进一步消除了用户偏差对模型的影响。通过实验证明，GAUC确实是一个更加合理的评价指标。</p>
<h4 id="3-2-Dice激活函数"><a href="#3-2-Dice激活函数" class="headerlink" title="3.2 Dice激活函数"></a>3.2 Dice激活函数</h4><p><strong>从Relu到PRelu</strong><br>Relu激活函数形式如下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-936e11da7a2ab0fd.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>Relu激活函数在值大于0时原样输出，小于0时输出为0。这样的话导致了许多网络节点的更新缓慢。因此又了PRelu，也叫Leaky Relu，形式如下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-111dfa0155c33845.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>这样，及时值小于0，网络的参数也得以更新，加快了收敛速度。</p>
<p><strong>从PReLU到Dice</strong></p>
<p>尽管对Relu进行了修正得到了PRelu，但是仍然有一个问题，即我们认为分割点都是0，但实际上，分割点应该由数据决定，因此文中提出了Dice激活函数</p>
<p>Dice激活函数的全称是Data Dependent Activation Function，形式如下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-2ff44bcf7d3b4054.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>其中，期望和方差的计算如下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-448758600f0d336f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>可也看到，每一个yi对应了一个概率值pi。pi的计算主要分为两步：将yi进行标准化和进行sigmoid变换。</p>
<h4 id="3-3-自适应正则-Adaptive-Regularization"><a href="#3-3-自适应正则-Adaptive-Regularization" class="headerlink" title="3.3 自适应正则 Adaptive Regularization"></a>3.3 自适应正则 Adaptive Regularization</h4><p>CTR中输入稀疏而且维度高，通常的做法是加入L1、L2、Dropout等防止过拟合。但是论文中尝试后效果都不是很好。用户数据符合长尾定律long-tail law，也就是说很多的feature id只出现了几次，而一小部分feature id出现很多次。这在训练过程中增加了很多噪声，并且加重了过拟合。</p>
<p>对于这个问题一个简单的处理办法就是：直接去掉出现次数比较少的feature id。但是这样就人为的丢掉了一些信息，导致模型更加容易过拟合，同时阈值的设定作为一个新的超参数，也是需要大量的实验来选择的。</p>
<p>因此，阿里提出了<strong>自适应正则</strong>的做法，即：<br>1.针对feature id出现的频率，来自适应的调整他们正则化的强度；<br>2.对于出现频率高的，给与较小的正则化强度；<br>3.对于出现频率低的，给予较大的正则化强度。</p>
<p>计算公式如下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-69a8d1e143b1953a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<h2 id="4、效果展示"><a href="#4、效果展示" class="headerlink" title="4、效果展示"></a>4、效果展示</h2><p>下图是对Local Activation效果的一个展示，可以看到，对于候选的广告是一件衣服的时候，用户历史行为中跟衣服相关的权重较高，而非衣服的部分，权重较低。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-ad1698c345e7df33.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>下图是对使用不同正则项的结果进行的展示，可以发现，使用自适应正则的情况下，模型的验证集误差和验证集GAUC均是最好的。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-aff9654a61c327ce.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>下图对比了Base Model和DIN的实验结果，可以看到，DIN模型在加入Dice激活函数以及自适应正则之后，模型的效果有了一定的提升：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-d4ed014c118ab98b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<h2 id="5、实战DIN"><a href="#5、实战DIN" class="headerlink" title="5、实战DIN"></a>5、实战DIN</h2><p>本文的github地址为：<a href="https://github.com/princewen/tensorflow_practice/tree/master/recommendation/Basic-DIN-Demo" target="_blank" rel="external">https://github.com/princewen/tensorflow_practice/tree/master/recommendation/Basic-DIN-Demo</a></p>
<p>参考的github地址为：<a href="https://github.com/zhougr1993/DeepInterestNetwork/tree/master/din" target="_blank" rel="external">https://github.com/zhougr1993/DeepInterestNetwork/tree/master/din</a></p>
<p>这里我们只给出一些模型细节的实现，具体的数据处理以及其他方面的内容大家可以根据上面两个地址进行学习：</p>
<p><strong>数据准备</strong><br>按照下面的方法下载数据：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-97f7101f2de74bb5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p><strong>Dice激活函数</strong></p>
<p>这里实现的Dice激活函数没有根据上一步的均值方差来计算这一步的均值方差，而是直接计算了这个batch的均值方差。我们可以根据计算出的均值方差对x进行标准化(代码中被注释掉了)，也可以直接调用batch_normalization来对输入进行标准化。</p>
<p>注意的一点是，alpha也是需要训练的一个参数。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line">import tensorflow as tf</div><div class="line"></div><div class="line">def dice(_x,axis=-1,epsilon=0.0000001,name=&apos;&apos;):</div><div class="line"></div><div class="line">    alphas = tf.get_variable(&apos;alpha&apos;+name,_x.get_shape()[-1],</div><div class="line">                             initializer = tf.constant_initializer(0.0),</div><div class="line">                             dtype=tf.float32)</div><div class="line"></div><div class="line">    input_shape = list(_x.get_shape())</div><div class="line">    reduction_axes = list(range(len(input_shape)))</div><div class="line"></div><div class="line">    del reduction_axes[axis] # [0]</div><div class="line"></div><div class="line">    broadcast_shape = [1] * len(input_shape)  #[1,1]</div><div class="line">    broadcast_shape[axis] = input_shape[axis] # [1 * hidden_unit_size]</div><div class="line"></div><div class="line">    # case: train mode (uses stats of the current batch)</div><div class="line">    mean = tf.reduce_mean(_x, axis=reduction_axes) # [1 * hidden_unit_size]</div><div class="line">    brodcast_mean = tf.reshape(mean, broadcast_shape)</div><div class="line">    std = tf.reduce_mean(tf.square(_x - brodcast_mean) + epsilon, axis=reduction_axes)</div><div class="line">    std = tf.sqrt(std)</div><div class="line">    brodcast_std = tf.reshape(std, broadcast_shape) #[1 * hidden_unit_size]</div><div class="line">    # x_normed = (_x - brodcast_mean) / (brodcast_std + epsilon)</div><div class="line">    x_normed = tf.layers.batch_normalization(_x, center=False, scale=False)  # a simple way to use BN to calculate x_p</div><div class="line">    x_p = tf.sigmoid(x_normed)</div><div class="line"></div><div class="line">    return alphas * (1.0 - x_p) * _x + x_p * _x</div></pre></td></tr></table></figure>
<p><strong>Activation Unit</strong></p>
<p>这里的输入有三个，候选广告queries，用户历史行为keys，以及Batch中每个行为的长度。这里为什么要输入一个keys_length呢，因为每个用户发生过的历史行为是不一样多的，但是输入的keys维度是固定的(都是历史行为最大的长度)，因此我们需要这个长度来计算一个mask，告诉模型哪些行为是没用的，哪些是用来计算用户兴趣分布的。</p>
<p>经过以下几个步骤得到用户的兴趣分布：</p>
<ol>
<li>将queries变为和keys同样的形状B <em> T </em> H(B指batch的大小，T指用户历史行为的最大长度，H指embedding的长度)</li>
<li>通过三层神经网络得到queries和keys中每个key的权重，并经过softmax进行标准化</li>
<li>通过weighted sum得到最终用户的历史行为分布</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div></pre></td><td class="code"><pre><div class="line">def attention(queries,keys,keys_length):</div><div class="line">    &apos;&apos;&apos;</div><div class="line">        queries:     [B, H]</div><div class="line">        keys:        [B, T, H]</div><div class="line">        keys_length: [B]</div><div class="line">    &apos;&apos;&apos;</div><div class="line"></div><div class="line">    queries_hidden_units = queries.get_shape().as_list()[-1]</div><div class="line">    queries = tf.tile(queries,[1,tf.shape(keys)[1]])</div><div class="line">    queries = tf.reshape(queries,[-1,tf.shape(keys)[1],queries_hidden_units])</div><div class="line"></div><div class="line">    din_all = tf.concat([queries,keys,queries-keys,queries * keys],axis=-1) # B*T*4H</div><div class="line">    # 三层全链接</div><div class="line">    d_layer_1_all = tf.layers.dense(din_all, 80, activation=tf.nn.sigmoid, name=&apos;f1_att&apos;)</div><div class="line">    d_layer_2_all = tf.layers.dense(d_layer_1_all, 40, activation=tf.nn.sigmoid, name=&apos;f2_att&apos;)</div><div class="line">    d_layer_3_all = tf.layers.dense(d_layer_2_all, 1, activation=None, name=&apos;f3_att&apos;) #B*T*1</div><div class="line"></div><div class="line">    outputs = tf.reshape(d_layer_3_all,[-1,1,tf.shape(keys)[1]]) #B*1*T</div><div class="line">    # Mask</div><div class="line">    key_masks = tf.sequence_mask(keys_length,tf.shape(keys)[1])</div><div class="line">    key_masks = tf.expand_dims(key_masks,1) # B*1*T</div><div class="line">    paddings = tf.ones_like(outputs) * (-2 ** 32 + 1) # 在补足的地方附上一个很小的值，而不是0</div><div class="line">    outputs = tf.where(key_masks,outputs,paddings) # B * 1 * T</div><div class="line">    # Scale</div><div class="line">    outputs = outputs / (keys.get_shape().as_list()[-1] ** 0.5)</div><div class="line">    # Activation</div><div class="line">    outputs = tf.nn.softmax(outputs) # B * 1 * T</div><div class="line">    # Weighted Sum</div><div class="line">    outputs = tf.matmul(outputs,keys) # B * 1 * H 三维矩阵相乘，相乘发生在后两维，即 B * (( 1 * T ) * ( T * H ))</div><div class="line">    return outputs</div></pre></td></tr></table></figure>
<p>#参考文献<br>1、盖坤演讲视频：<a href="http://www.itdks.com/dakalive/detail/3166" target="_blank" rel="external">http://www.itdks.com/dakalive/detail/3166</a><br>2、论文：Deep Interest Network for Click-Through Rate Prediction<br>3、github：<a href="https://github.com/zhougr1993/DeepInterestNetwork" target="_blank" rel="external">https://github.com/zhougr1993/DeepInterestNetwork</a></p>
<h1 id="推荐系统遇上深度学习-十九-–探秘阿里之完整空间多任务模型ESMM"><a href="#推荐系统遇上深度学习-十九-–探秘阿里之完整空间多任务模型ESMM" class="headerlink" title="推荐系统遇上深度学习(十九)–探秘阿里之完整空间多任务模型ESMM"></a>推荐系统遇上深度学习(十九)–探秘阿里之完整空间多任务模型ESMM</h1><p>阿里近几年公开的推荐领域算法有许多，既有传统领域的探索如MLR算法，还有深度学习领域的探索如entire-space multi-task model，Deep Interest Network等，同时跟清华大学合作展开了强化学习领域的探索，提出了MARDPG算法。</p>
<p>上一篇，我们介绍了深度兴趣网络(Deep Interest Network)，充分利用用户历史行为数据中的<strong>Diversity</strong>和<strong>Local Activation</strong>特性进行建模。再来回顾一下DIN的网络结构：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-3748528c2a5fc919.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>这一篇，我们将焦点转向CVR的预估，来看一下阿里提出的完整空间多任务模型ESMM是如何处理CVR预估中存在的样本选择偏差(sample selection bias)和数据稀疏(data sparsity)问题的。</p>
<p>论文地址：<a href="https://arxiv.org/abs/1804.07931" target="_blank" rel="external">https://arxiv.org/abs/1804.07931</a></p>
<p>在正式开篇之前，我们先介绍一下几个名词：<br><strong>impression</strong>：用户观察到曝光的产品<br><strong>click</strong>：用户对impression的点击行为<br><strong>conversion</strong>：用户点击之后对物品的购买行为<br><strong>CTR</strong>：从impression到click的比例<br><strong>CVR</strong>：从click到conversion的比例<br><strong>CTCVR</strong>：从impression到conversion的比例<br><strong>pCTR</strong>：p(click=1 | impression)<br><strong>pCVR</strong>: p(conversion=1 | click=1,impression)<br><strong>pCTCVR</strong>: p(conversion=1,click=1 |impression) = p(click=1 | impression) * p(conversion=1|click=1,impression)</p>
<h2 id="1、背景-4"><a href="#1、背景-4" class="headerlink" title="1、背景"></a>1、背景</h2><p>以电子商务平台为例，用户的购买行为一般遵循以下的顺序决策模式：impression-click-conversion，即用户先观察到系统推荐的产品，然后会对自己感兴趣的商品进行点击，进而产生购买行为。从点击到购买的转化我们称为post-click Conversion rate(以下简称CVR)，CVR的预估在信息检索、推荐系统、在线广告投放等工业级应用中是至关重要的。</p>
<p>不过传统的CVR预估问题存在着两个主要的问题：<strong>样本选择偏差</strong>和<strong>稀疏数据</strong>。我们来看下面的图，我们把给用户曝光过的产品看作是整个样本空间X的话，用户点击过的产品仅是中间灰色的部分，我们定义为Xc，而用户购买过的产品仅是图中黑色的部分。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-31ccd3226cc1426d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p><strong>样本选择偏差(sample selection bias,SSB)</strong>：传统的推荐系统仅用Xc中的样本来训练CVR预估模型，但训练好的模型是在整个样本空间X去做推断的。由于点击事件相对于曝光事件来说要少很多，因此只是样本空间X的一个很小的子集，从Xc上提取的特征相对于从X中提取的特征而言是有偏的，甚至是很不相同。从而，按这种方法构建的训练样本集相当于是从一个与真实分布不一致的分布中采样得到的，这一定程度上违背了机器学习中独立同分布的假设。这种训练样本从整体样本空间的一个较小子集中提取，而训练得到的模型却需要对整个样本空间中的样本做推断预测的现象称之为样本选择偏差。样本选择偏差会伤害学到的模型的泛化性能。</p>
<p><strong>数据稀疏(data sparsity,DS)</strong>:推荐系统展现给用户的商品数量要远远大于被用户点击的商品数量，同时有点击行为的用户也仅仅只占所有用户的一小部分，因此有点击行为的样本空间Xc相对于整个样本空间X来说是很小的，通常来讲，量级要少1~3个数量级。如下表所示，在淘宝公开的训练数据集上，Xc只占整个样本空间X的4%。这就是所谓的训练数据稀疏的问题，高度稀疏的训练数据使得模型的学习变得相当困难。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-570b012bc8af5139.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>为了解决上面的两个问题，阿里提出了完整空间多任务模型ESMM。下一章，我们将会来学习一下阿里是如何设计整个网络的。</p>
<h2 id="2、ESMM模型"><a href="#2、ESMM模型" class="headerlink" title="2、ESMM模型"></a>2、ESMM模型</h2><h4 id="2-1-模型结构"><a href="#2-1-模型结构" class="headerlink" title="2.1 模型结构"></a>2.1 模型结构</h4><p>阿里妈妈的算法同学提出的ESMM模型借鉴了多任务学习的思路，引入了两个辅助的学习任务，分别用来拟合pCTR和pCTCVR，从而同时消除了上文提到的两个挑战。ESMM模型能够充分利用用户行为的顺序性模式，其模型架构下图所示：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-e583e6dbf39b38d5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>可以看到，ESMM模型由两个子网络组成，左边的子网络用来拟合pCVR，右边的子网络用来拟合pCTR，同时，两个子网络的输出相乘之后可以得到pCTCVR。因此，该网络结构共有三个子任务，分别用于输出pCTR、pCVR和pCTCVR。</p>
<p>假设我们用x表示feature(即impression),y表示点击，z表示转化，那么根据pCTCVR = pCTR * pCVR，可以得到：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-148dbce6adc8dd64.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>将乘法转化为除法，我们可以得到pCVR的计算：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-716f554b649e1c43.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<h4 id="2-2-模型特点"><a href="#2-2-模型特点" class="headerlink" title="2.2 模型特点"></a>2.2 模型特点</h4><p>ESMM模型共有以下两个主要的特点：</p>
<p><strong>在整个样本空间中进行建模</strong></p>
<p>由上面提到的等式可以看出，pCVR是可以通过pCTR和pCTCVR的预估推导出来的。因此，我们只需要关注pCTR和pCTCVR两个任务即可。为什么是这两个任务呢？其实就是为了消除样本选择偏差嘛，因为CVR是从click到conversion，而CTCVR是从impression到conversion，CTR是从impression到conversion，所以CTR和CTCVR都可以从整个样本空间进行训练，一定程度的消除了样本选择偏差。</p>
<p>我们可以将有点击行为的曝光事件作为正样本，没有点击行为的曝光事件作为负样本，来做CTR预估的任务。将同时有点击行为和购买行为的曝光事件作为正样本，其他作为负样本来训练CTCVR的预估部分。</p>
<p>模型具体是怎么做的呢？可以看到，用来训练两个任务的输入x其实是相同的，但是label是不同的。CTR任务预估的是点击y，CTCVR预估的是转化z。因此，我们将(x,y)输入到CTR任务中，得到CTR的预估值，将(x,z)输入到CVR任务中，得到CVR的预估值，CTR和CVR的预估值相乘，便得到了CTCVR的预估值。因此，模型的损失函数可以定义为：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-3543355f3bed04dc.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>其中，θctr和θcvr分别是CTR网络和CVR网络的参数，l(⋅)是交叉熵损失函数。</p>
<p><strong>共享特征表示</strong></p>
<p>ESMM模型借鉴迁移学习的思路，在两个子网络的embedding层共享embedding向量（特征表示）词典。网络的embedding层把大规模稀疏的输入数据映射到低维的表示向量，该层的参数占了整个网络参数的绝大部分，需要大量的训练样本才能充分学习得到。由于CTR任务的训练样本量要大大超过CVR任务的训练样本量，ESMM模型中特征表示共享的机制能够使得CVR子任务也能够从只有展现没有点击的样本中学习，从而能够极大地有利于缓解训练数据稀疏性问题。</p>
<h2 id="3、实验效果"><a href="#3、实验效果" class="headerlink" title="3、实验效果"></a>3、实验效果</h2><h4 id="3-1-对比模型介绍"><a href="#3-1-对比模型介绍" class="headerlink" title="3.1 对比模型介绍"></a>3.1 对比模型介绍</h4><p>为了验证ESMM的效果，阿里团队与如下的算法进行了对比：</p>
<table>
<thead>
<tr>
<th>方法</th>
<th style="text-align:center">简介</th>
</tr>
</thead>
<tbody>
<tr>
<td>BASE</td>
<td style="text-align:center">ESMM模型中左边的子神经网络模型，只利用点击样本空间Xc</td>
</tr>
<tr>
<td>AMAN</td>
<td style="text-align:center">AMAN applies negative sampling strategy and best results are reported with sampling rate searched in {10%, 20%, 50%, 100%}</td>
</tr>
<tr>
<td>OVERSAMPLING</td>
<td style="text-align:center">OVERSAMPLINGcopies posi- tive examples to reduce di culty of training with sparse data, with sampling rate searched in {2, 3, 5, 10}.</td>
</tr>
<tr>
<td>UNBIAS</td>
<td style="text-align:center">UNBIAS follows  to  t the truly underlying distribution from observations via rejection sampling. pCTR is taken as the rejection probability.</td>
</tr>
<tr>
<td>DIVISION</td>
<td style="text-align:center">先分别训练出拟合CTR和CTCVR的模型，再拿CTCVR模型的预测结果除以CTR模型的预测结果得到对CVR模型的预估</td>
</tr>
<tr>
<td>ESMM-NS</td>
<td style="text-align:center">ESMM模型的基础上去掉了特征表示共享的机制</td>
</tr>
<tr>
<td>ESMM</td>
<td style="text-align:center">本文提出的模型n</td>
</tr>
</tbody>
</table>
<h4 id="3-2-公开数据集实验"><a href="#3-2-公开数据集实验" class="headerlink" title="3.2 公开数据集实验"></a>3.2 公开数据集实验</h4><p>下图展示了在公开实验数据集上模型的对比效果：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-b50a185fa9778b6a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>可以看到，ESMM模型相比于其他的模型，实验效果显著提升。</p>
<h4 id="3-3-淘宝数据集实验"><a href="#3-3-淘宝数据集实验" class="headerlink" title="3.3 淘宝数据集实验"></a>3.3 淘宝数据集实验</h4><p>下图展示了ESMM模型在淘宝生产环境数据集上的测试效果对比：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-bb500b1a936c380b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>可以看到，相对于BASE模型，ESMM模型在CVR任务中AUC指标提升了 2.18%，在CTCVR任务中AUC指标提升了2.32%。</p>
<h2 id="4、总结"><a href="#4、总结" class="headerlink" title="4、总结"></a>4、总结</h2><p>总结一下，ESMM模型利用用户行为序列数据在完整样本空间建模，避免了传统CVR模型经常遭遇的样本选择偏差和训练数据稀疏的问题，取得了显著的效果。另一方面，ESMM模型的贡献在于其提出的利用学习CTR和CTCVR的辅助任务，迂回地学习CVR的思路。ESMM模型中的BASE子网络可以替换为任意的学习模型，因此ESMM的框架可以非常容易地和其他学习模型集成，从而吸收其他学习模型的优势，进一步提升学习效果，想象空间巨大。</p>
<p>#参考文献<br>1、<a href="https://arxiv.org/abs/1804.07931" target="_blank" rel="external">https://arxiv.org/abs/1804.07931</a><br>2、<a href="http://xudongyang.coding.me/esmm/" target="_blank" rel="external">http://xudongyang.coding.me/esmm/</a></p>
<h1 id="推荐系统遇上深度学习-二十-–贝叶斯个性化排序-BPR-算法原理及实战"><a href="#推荐系统遇上深度学习-二十-–贝叶斯个性化排序-BPR-算法原理及实战" class="headerlink" title="推荐系统遇上深度学习(二十)–贝叶斯个性化排序(BPR)算法原理及实战"></a>推荐系统遇上深度学习(二十)–贝叶斯个性化排序(BPR)算法原理及实战</h1><p>排序推荐算法大体上可以分为三类，第一类排序算法类别是点对方法(Pointwise Approach)，这类算法将排序问题被转化为分类、回归之类的问题，并使用现有分类、回归等方法进行实现。第二类排序算法是成对方法(Pairwise Approach)，在序列方法中，排序被转化为对序列分类或对序列回归。所谓的pair就是成对的排序，比如(a,b)一组表明a比b排的靠前。第三类排序算法是列表方法(Listwise Approach)，它采用更加直接的方法对排序问题进行了处理。它在学习和预测过程中都将排序列表作为一个样本。排序的组结构被保持。</p>
<p>之前我们介绍的算法大都是Pointwise的方法，今天我们来介绍一种Pairwise的方法：贝叶斯个性化排序(Bayesian Personalized Ranking, 以下简称BPR)</p>
<h2 id="1、BPR算法简介"><a href="#1、BPR算法简介" class="headerlink" title="1、BPR算法简介"></a>1、BPR算法简介</h2><h4 id="1-1-基本思路"><a href="#1-1-基本思路" class="headerlink" title="1.1 基本思路"></a>1.1 基本思路</h4><p>在BPR算法中，我们将任意用户u对应的物品进行标记，如果用户u在同时有物品i和j的时候点击了i，那么我们就得到了一个三元组<u,i,j>，它表示对用户u来说，i的排序要比j靠前。如果对于用户u来说我们有m组这样的反馈，那么我们就可以得到m组用户u对应的训练样本。</u,i,j></p>
<p>这里，我们做出两个假设：</p>
<ol>
<li>每个用户之间的偏好行为相互独立，即用户u在商品i和j之间的偏好和其他用户无关。</li>
<li>同一用户对不同物品的偏序相互独立，也就是用户u在商品i和j之间的偏好和其他的商品无关。</li>
</ol>
<p>为了便于表述，我们用&gt;u符号表示用户u的偏好，上面的<u,i,j>可以表示为：i &gt;u j。</u,i,j></p>
<p>在BPR中，我们也用到了类似矩阵分解的思想，对于用户集U和物品集I对应的U*I的预测排序矩阵，我们期望得到两个分解后的用户矩阵W(|U|×k)和物品矩阵H(|I|×k)，满足：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-5cb991131b04fb8f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>那么对于任意一个用户u，对应的任意一个物品i，我们预测得出的用户对该物品的偏好计算如下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-d362e6c0d3411296.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>而模型的最终目标是寻找合适的矩阵W和H，让X^-^(公式打不出来，这里代表的是X上面有一个横线，即W和H矩阵相乘后的结果)和X(实际的评分矩阵)最相似。看到这里，也许你会说，BPR和矩阵分解没有什区别呀？是的，到目前为止的基本思想是一致的，但是具体的算法运算思路，确实千差万别的，我们慢慢道来。</p>
<h4 id="1-2-算法运算思路"><a href="#1-2-算法运算思路" class="headerlink" title="1.2 算法运算思路"></a>1.2 算法运算思路</h4><p>BPR 基于最大后验估计P(W,H|&gt;u)来求解模型参数W,H,这里我们用θ来表示参数W和H, &gt;u代表用户u对应的所有商品的全序关系,则优化目标是P(θ|&gt;u)。根据贝叶斯公式，我们有：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-deab3d8e2c561d13.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>由于我们求解假设了用户的排序和其他用户无关，那么对于任意一个用户u来说，P(&gt;u)对所有的物品一样，所以有：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-bf036c9c9f1298b7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>这个优化目标转化为两部分。第一部分和样本数据集D有关，第二部分和样本数据集D无关。</p>
<p><strong>第一部分</strong></p>
<p>对于第一部分，由于我们假设每个用户之间的偏好行为相互独立，同一用户对不同物品的偏序相互独立，所以有：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-c3282d4fea4535ea.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>上面的式子类似于极大似然估计，若用户u相比于j来说更偏向i，那么我们就希望P(i &gt;u j|θ)出现的概率越大越好。</p>
<p>上面的式子可以进一步改写成：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-47c5254c8ee023b0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>而对于P(i &gt;u j|θ)这个概率，我们可以使用下面这个式子来代替:</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-c515e9855fa8e08e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>其中，σ(x)是sigmoid函数，σ里面的项我们可以理解为用户u对i和j偏好程度的差异，我们当然希望i和j的差异越大越好，这种差异如何体现，最简单的就是差值：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-a03ff1a10d70f0dc.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>省略θ我们可以将式子简略的写为：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-298d24f7d518a3fa.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>因此优化目标的第一项可以写作：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-a5bc6ddc83bb3c46.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>哇，是不是很简单的思想，对于训练数据中的<u,i,j>，用户更偏好于i，那么我们当然希望在X^-^矩阵中ui对应的值比uj对应的值大，而且差距越大越好！</u,i,j></p>
<p><strong>第二部分</strong><br>回想之前我们通过贝叶斯角度解释正则化的文章：<a href="https://www.jianshu.com/p/4d562f2c06b8" target="_blank" rel="external">https://www.jianshu.com/p/4d562f2c06b8</a></p>
<p>当θ的先验分布是正态分布时，其实就是给损失函数加入了正则项，因此我们可以假定θ的先验分布是正态分布：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-a74d49dd7f79aefa.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>所以：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-7f45ccc66c59c2e7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>因此，最终的最大对数后验估计函数可以写作：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-e9010bc3e0e254a3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>剩下的我们就可以通过梯度上升法(因为是要让上式最大化)来求解了。我们这里就略过了，BPR的思想已经很明白了吧，哈哈！让我们来看一看如何实现吧。</p>
<h2 id="2、算法实现"><a href="#2、算法实现" class="headerlink" title="2、算法实现"></a>2、算法实现</h2><p>本文的github地址为：<a href="https://github.com/princewen/tensorflow_practice/tree/master/recommendation/Basic-BPR-Demo" target="_blank" rel="external">https://github.com/princewen/tensorflow_practice/tree/master/recommendation/Basic-BPR-Demo</a></p>
<p>所用到的数据集是movieslen 100k的数据集，下载地址为：<a href="http://grouplens.org/datasets/movielens/" target="_blank" rel="external">http://grouplens.org/datasets/movielens/</a></p>
<p><strong>数据预处理</strong></p>
<p>首先，我们需要处理一下数据，得到每个用户打分过的电影，同时，还需要得到用户的数量和电影的数量。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line">def load_data():</div><div class="line">    user_ratings = defaultdict(set)</div><div class="line">    max_u_id = -1</div><div class="line">    max_i_id = -1</div><div class="line">    with open(&apos;data/u.data&apos;,&apos;r&apos;) as f:</div><div class="line">        for line in f.readlines():</div><div class="line">            u,i,_,_ = line.split(&quot;\t&quot;)</div><div class="line">            u = int(u)</div><div class="line">            i = int(i)</div><div class="line">            user_ratings[u].add(i)</div><div class="line">            max_u_id = max(u,max_u_id)</div><div class="line">            max_i_id = max(i,max_i_id)</div><div class="line"></div><div class="line"></div><div class="line">    print(&quot;max_u_id:&quot;,max_u_id)</div><div class="line">    print(&quot;max_i_idL&quot;,max_i_id)</div><div class="line"></div><div class="line">    return max_u_id,max_i_id,user_ratings</div></pre></td></tr></table></figure>
<p>下面我们会对每一个用户u，在user_ratings中随机找到他评分过的一部电影i,保存在user_ratings_test，后面构造训练集和测试集需要用到。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">def generate_test(user_ratings):</div><div class="line">    &quot;&quot;&quot;</div><div class="line">    对每一个用户u，在user_ratings中随机找到他评分过的一部电影i,保存在user_ratings_test，我们为每个用户取出的这一个电影，是不会在训练集中训练到的，作为测试集用。</div><div class="line">    &quot;&quot;&quot;</div><div class="line">    user_test = dict()</div><div class="line">    for u,i_list in user_ratings.items():</div><div class="line">        user_test[u] = random.sample(user_ratings[u],1)[0]</div><div class="line">    return user_test</div></pre></td></tr></table></figure>
<p><strong>构建训练数据</strong><br>我们构造的训练数据是<u,i,j>的三元组，i可以根据刚才生成的用户评分字典得到，j可以利用负采样的思想，认为用户没有看过的电影都是负样本：</u,i,j></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line">def generate_train_batch(user_ratings,user_ratings_test,item_count,batch_size=512):</div><div class="line">    &quot;&quot;&quot;</div><div class="line">    构造训练用的三元组</div><div class="line">    对于随机抽出的用户u，i可以从user_ratings随机抽出，而j也是从总的电影集中随机抽出，当然j必须保证(u,j)不在user_ratings中</div><div class="line"></div><div class="line">    &quot;&quot;&quot;</div><div class="line">    t = []</div><div class="line">    for b in range(batch_size):</div><div class="line">        u = random.sample(user_ratings.keys(),1)[0]</div><div class="line">        i = random.sample(user_ratings[u],1)[0]</div><div class="line">        while i==user_ratings_test[u]:</div><div class="line">            i = random.sample(user_ratings[u],1)[0]</div><div class="line"></div><div class="line">        j = random.randint(1,item_count)</div><div class="line">        while j in user_ratings[u]:</div><div class="line">            j = random.randint(1,item_count)</div><div class="line"></div><div class="line">        t.append([u,i,j])</div><div class="line"></div><div class="line">    return np.asarray(t)</div></pre></td></tr></table></figure>
<p><strong>构造测试数据</strong><br>同样构造三元组，我们刚才给每个用户单独抽出了一部电影，这个电影作为i，而用户所有没有评分过的电影都是负样本j：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">def generate_test_batch(user_ratings,user_ratings_test,item_count):</div><div class="line">    &quot;&quot;&quot;</div><div class="line">    对于每个用户u，它的评分电影i是我们在user_ratings_test中随机抽取的，它的j是用户u所有没有评分过的电影集合，</div><div class="line">    比如用户u有1000部电影没有评分，那么这里该用户的测试集样本就有1000个</div><div class="line">    &quot;&quot;&quot;</div><div class="line">    for u in user_ratings.keys():</div><div class="line">        t = []</div><div class="line">        i = user_ratings_test[u]</div><div class="line">        for j in range(1,item_count + 1):</div><div class="line">            if not(j in user_ratings[u]):</div><div class="line">                t.append([u,i,j])</div><div class="line">        yield np.asarray(t)</div></pre></td></tr></table></figure>
<p><strong>模型构建</strong><br>首先回忆一下我们需要学习的参数θ，其实就是用户矩阵W(|U|×k)和物品矩阵H(|I|×k)对应的值，对于我们的模型来说，可以简单理解为由id到embedding的转化，因此有：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">u = tf.placeholder(tf.int32,[None])</div><div class="line">i = tf.placeholder(tf.int32,[None])</div><div class="line">j = tf.placeholder(tf.int32,[None])</div><div class="line"></div><div class="line">user_emb_w = tf.get_variable(&quot;user_emb_w&quot;, [user_count + 1, hidden_dim],</div><div class="line">                             initializer=tf.random_normal_initializer(0, 0.1))</div><div class="line">item_emb_w = tf.get_variable(&quot;item_emb_w&quot;, [item_count + 1, hidden_dim],</div><div class="line">                             initializer=tf.random_normal_initializer(0, 0.1))</div><div class="line"></div><div class="line">u_emb = tf.nn.embedding_lookup(user_emb_w, u)</div><div class="line">i_emb = tf.nn.embedding_lookup(item_emb_w, i)</div><div class="line">j_emb = tf.nn.embedding_lookup(item_emb_w, j)</div></pre></td></tr></table></figure>
<p>回想一下我们要优化的目标，第一部分是ui和uj对应的预测值的评分之差，再经由sigmoid变换得到的[0,1]值，我们希望这个值越大越好，对于损失来说，当然是越小越好。因此，计算如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">x = tf.reduce_sum(tf.multiply(u_emb,(i_emb-j_emb)),1,keep_dims=True)</div><div class="line">loss1 = - tf.reduce_mean(tf.log(tf.sigmoid(x)))</div></pre></td></tr></table></figure>
<p>第二部分是我们的正则项，参数就是我们的embedding值，所以正则项计算如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">l2_norm = tf.add_n([</div><div class="line">        tf.reduce_sum(tf.multiply(u_emb, u_emb)),</div><div class="line">        tf.reduce_sum(tf.multiply(i_emb, i_emb)),</div><div class="line">        tf.reduce_sum(tf.multiply(j_emb, j_emb))</div><div class="line">    ])</div></pre></td></tr></table></figure>
<p>因此，我们模型整个的优化目标可以写作：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">regulation_rate = 0.0001</div><div class="line">bprloss = regulation_rate * l2_norm - tf.reduce_mean(tf.log(tf.sigmoid(x)))</div><div class="line"></div><div class="line">train_op = tf.train.GradientDescentOptimizer(0.01).minimize(bprloss)</div></pre></td></tr></table></figure>
<p>至此，我们整个模型就介绍完了，如果大家想要了解完整的代码实现，可以参考github哟。</p>
<h2 id="3、总结"><a href="#3、总结" class="headerlink" title="3、总结"></a>3、总结</h2><p>1.BPR是基于矩阵分解的一种排序算法，它不是做全局的评分优化，而是针对每一个用户自己的商品喜好分贝做排序优化。<br>2.它是一种pairwise的排序算法，对于每一个三元组<u,i,j>，模型希望能够使用户u对物品i和j的差异更明显。<br>3.同时，引入了贝叶斯先验，假设参数服从正态分布，在转换后变为了L2正则，减小了模型的过拟合。</u,i,j></p>
<p>#参考文献<br>1、<a href="http://www.cnblogs.com/pinard/p/9128682.html" target="_blank" rel="external">http://www.cnblogs.com/pinard/p/9128682.html</a><br>2、<a href="http://www.cnblogs.com/pinard/p/9163481.html" target="_blank" rel="external">http://www.cnblogs.com/pinard/p/9163481.html</a></p>
<h1 id="推荐系统遇上深度学习-二十一-–阶段性回顾"><a href="#推荐系统遇上深度学习-二十一-–阶段性回顾" class="headerlink" title="推荐系统遇上深度学习(二十一)–阶段性回顾"></a>推荐系统遇上深度学习(二十一)–阶段性回顾</h1><p>本系列已经写了二十篇了，但推荐系统的东西还有很多值得探索和学习的地方。不过在这之前，我们先静下心来，一起回顾下之前学习到的东西！</p>
<p>由于是总结性质的文章，很多细节不会过多的涉及，有兴趣的同学可以点击文章中给出的链接进行学习。</p>
<p>本文中涉及的大多数算法是计算广告中点击率预估用到的模型，当然也会涉及pair-wise的模型如贝叶斯个性排序以及list-wise的如京东的强化学习推荐模型。</p>
<h2 id="1、推荐系统中常用评测指标"><a href="#1、推荐系统中常用评测指标" class="headerlink" title="1、推荐系统中常用评测指标"></a>1、推荐系统中常用评测指标</h2><p>评测指标并非我们的损失函数，对于CTR预估问题来说，我们可以当作回归问题而选择平方损失函数，也可以当作分类问题而选择对数损失函数。<br>不过评测指标，是对我们推荐效果的评价，用于评估推荐效果的好坏，不用于指导我们模型的训练。因此在一般的基于深度学习的模型中，常常面临模型训练和评估时<strong>指标不一致</strong>的问题。好了，我们先来回顾一下常用的评测指标。这些指标有的适用于二分类问题，有的适用于对推荐列表topk的评价。</p>
<h4 id="1-1精确率、召回率、F1值"><a href="#1-1精确率、召回率、F1值" class="headerlink" title="1.1精确率、召回率、F1值"></a>1.1精确率、召回率、F1值</h4><p>我们首先来看一下混淆矩阵，对于二分类问题，真实的样本标签有两类，我们学习器预测的类别有两类，那么根据二者的类别组合可以划分为四组，如下表所示：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/4155986-ba9b891cbfbc639d?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>基于混淆矩阵，我们可以得到如下的评测指标：</p>
<p><strong>精确率／召回率</strong></p>
<p><strong>精确率</strong>表示预测结果中，预测为正样本的样本中，正确预测为正样本的概率；<br><strong>召回率</strong>表示在原始样本的正样本中，最后被正确预测为正样本的概率；</p>
<p>二者用混淆矩阵计算如下：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/4155986-56ca5a89ff79d8c1?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p><strong>F1值</strong></p>
<p>为了折中精确率和召回率的结果，我们又引入了F-1 Score，计算公式如下：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/4155986-4b29b0298355ca4f?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<h4 id="1-2-AUC"><a href="#1-2-AUC" class="headerlink" title="1.2 AUC"></a>1.2 AUC</h4><p>AUC定义为ROC曲线下方的面积：</p>
<p>ROC曲线的横轴为“假正例率”（True Positive Rate,TPR)，又称为“假阳率”；纵轴为“真正例率”(False Positive Rate,FPR)，又称为“真阳率”，</p>
<p>下图就是我们绘制的一张ROC曲线图，曲线下方的面积即为AUC的值：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/4155986-d0f6bd99198a0531?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>AUC还有另一种解释，就是<strong>测试任意给一个正类样本和一个负类样本，正类样本的score有多大的概率大于负类样本的score</strong>。</p>
<h4 id="1-3-Hit-Ratio-HR"><a href="#1-3-Hit-Ratio-HR" class="headerlink" title="1.3 Hit Ratio(HR)"></a>1.3 Hit Ratio(HR)</h4><p>在top-K推荐中，HR是一种常用的衡量召回率的指标，其计算公式如下：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/4155986-e15af599d004bd42?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="e"></p>
<p>分母是所有的测试集合，分子式每个用户top-K推荐列表中属于测试集合的个数的总和。举个简单的例子，三个用户在测试集中的商品个数分别是10，12，8，模型得到的top-10推荐列表中，分别有6个，5个，4个在测试集中，那么此时HR的值是 (6+5+4)/(10+12+8) = 0.5。</p>
<h4 id="1-4-Mean-Average-Precision-MAP"><a href="#1-4-Mean-Average-Precision-MAP" class="headerlink" title="1.4 Mean Average Precision(MAP)"></a>1.4 Mean Average Precision(MAP)</h4><p>在了解MAP(Mean Average Precision)之前，先来看一下AP(Average Precision), 即为平均准确率。比如对于用户 u, 我们给他推荐一些物品，那么 u 的平均准确率定义为：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/4155986-530acc1b9a22c86d?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>用一个例子来解释AP的计算过程：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/4155986-a280ce81f900d1a2?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>因此该user的AP为（1 + 0.66 + 0.5） ／ 3 = 0.72</p>
<p>那么对于MAP(Mean Average Precision)，就很容易知道即为所有用户 u 的AP再取均值(mean)而已。那么计算公式如下：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/4155986-960efa15cfe279c6?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<h4 id="1-5-Normalized-Discounted-Cummulative-Gain-NDCG"><a href="#1-5-Normalized-Discounted-Cummulative-Gain-NDCG" class="headerlink" title="1.5 Normalized Discounted Cummulative Gain(NDCG)"></a>1.5 Normalized Discounted Cummulative Gain(NDCG)</h4><p>对于NDCG，我们需要一步步揭开其神秘的面纱，先从CG说起：<br><strong>CG</strong><br>我们先从CG(Cummulative Gain)说起, 直接翻译的话叫做“累计增益”。 在推荐系统中，CG即将每个推荐结果相关性(relevance)的分值累加后作为整个推荐列表(list)的得分。即</p>
<p><img src="http://upload-images.jianshu.io/upload_images/4155986-9d64ec1c08aa889f?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>这里， rel-i 表示处于位置 i 的推荐结果的相关性，k 表示所要考察的推荐列表的大小。</p>
<p><strong>DCG</strong><br>CG的一个缺点是没有考虑每个推荐结果处于不同位置对整个推荐效果的影响，例如我们总是希望相关性高的结果应排在前面。显然，如果相关性低的结果排在靠前的位置会严重影响用户体验， 所以在CG的基础上引入位置影响因素，即DCG(Discounted Cummulative Gain), “Discounted”有打折，折扣的意思，这里指的是对于排名靠后推荐结果的推荐效果进行“打折处理”:</p>
<p><img src="http://upload-images.jianshu.io/upload_images/4155986-1a568e1198813514?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image"></p>
<p>从上面的式子可以得到两个结论：<br>1）推荐结果的相关性越大，DCG越大。<br>2）相关性好的排在推荐列表的前面的话，推荐效果越好，DCG越大。</p>
<p><strong>NDCG</strong><br>DCG仍然有其局限之处，即不同的推荐列表之间，很难进行横向的评估。而我们评估一个推荐系统，不可能仅使用一个用户的推荐列表及相应结果进行评估， 而是对整个测试集中的用户及其推荐列表结果进行评估。 那么不同用户的推荐列表的评估分数就需要进行归一化，也即NDCG(Normalized Discounted Cummulative Gain)。</p>
<p>在介绍NDCG之前，还需要了解一个概念：IDCG. IDCG, 即Ideal DCG， 指推荐系统为某一用户返回的最好推荐结果列表， 即假设返回结果按照相关性排序， 最相关的结果放在最前面， 此序列的DCG为IDCG。因此DCG的值介于 (0,IDCG] ，故NDCG的值介于(0,1]，那么用户u的NDCG@K定义为：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/4155986-039b14d4d3e620ea?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>因此，平均NDCG计算为：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/4155986-21ffa3238643a88d?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<h4 id="1-6-Mean-Reciprocal-Rank-MRR"><a href="#1-6-Mean-Reciprocal-Rank-MRR" class="headerlink" title="1.6 Mean Reciprocal Rank (MRR)"></a>1.6 Mean Reciprocal Rank (MRR)</h4><p>MRR计算公式如下：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/4155986-7b02b04a2d1d205a?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>其中|Q|是用户的个数，ranki是对于第i个用户，推荐列表中第一个在ground-truth结果中的item所在的排列位置。</p>
<p>举个例子，有三个用户，推荐列表中正例的最小rank值分别为3，2，1，那么MRR=(1 + 0.5 + 0.33) / 3 = 0.61</p>
<h4 id="1-7-ILS"><a href="#1-7-ILS" class="headerlink" title="1.7 ILS"></a>1.7 ILS</h4><p>ILS是衡量推荐列表多样性的指标，计算公式如下：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/4155986-4e51511c195adf3b?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>如果S(bi,bj)计算的是i和j两个物品的相似性，如果推荐列表中物品越不相似，ILS越小，那么推荐结果的多样性越好。</p>
<p>关于推荐系统评价指标更多的知识，可以看之前总结的两篇文章：<br>推荐系统遇上深度学习(九)–评价指标AUC原理及实践：<a href="https://www.jianshu.com/p/4dde15a56d44" target="_blank" rel="external">https://www.jianshu.com/p/4dde15a56d44</a><br>推荐系统遇上深度学习(十六)–详解推荐系统中的常用评测指标：<a href="https://www.jianshu.com/p/665f9f168eff" target="_blank" rel="external">https://www.jianshu.com/p/665f9f168eff</a></p>
<p>相关的代码实现在这里：<a href="https://github.com/princewen/tensorflow_practice/tree/master/recommendation/Basic-Evaluation-metrics" target="_blank" rel="external">https://github.com/princewen/tensorflow_practice/tree/master/recommendation/Basic-Evaluation-metrics</a></p>
<p>#2、点击率预估问题中的数据<br>点击率预估问题中的数据主要分为离散变量和连续变量，对于连续变量，直接带入计算即可，对于离散(类别)变量，我们往往采用one-hot形式，比如对于下面的数据：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/4155986-8a6b13fe3e47f6eb?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>将上面的数据进行one-hot编码以后，就变成了下面这样：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/4155986-ba4c70369248eed0?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>过one-hot编码以后，不可避免的样本的数据就变得很稀疏。举个非常简单的例子，假设淘宝或者京东上的item为100万，如果对item这个维度进行one-hot编码，光这一个维度数据的稀疏度就是百万分之一。由此可见，数据的稀疏性，是我们在实际应用场景中面临的一个非常常见的挑战与问题。</p>
<h2 id="3、传统方法"><a href="#3、传统方法" class="headerlink" title="3、传统方法"></a>3、传统方法</h2><h4 id="3-1-线性模型"><a href="#3-1-线性模型" class="headerlink" title="3.1 线性模型"></a>3.1 线性模型</h4><p>一般的线性模型为：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/4155986-e302cdf57ffb0f70?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>从上面的式子很容易看出，一般的线性模型没有考虑特征间的关联。为了表述特征间的相关性，我们采用多项式模型。在多项式模型中，特征xi与xj的组合用xixj表示。为了简单起见，我们讨论二阶多项式模型。具体的模型表达式如下：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/4155986-27dad6f0bb52cb33?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>上式中，n表示样本的特征数量,xi表示第i个特征。</p>
<p>但是对于线性模型来说，泛化能力较弱，特别是对于同一个离散特征展开的one-hot特征来说，两两之间的乘积总是为0。</p>
<h4 id="3-2-FM模型"><a href="#3-2-FM模型" class="headerlink" title="3.2 FM模型"></a>3.2 FM模型</h4><p>FM为每一个特征引入了一个隐变量，并且用隐变量的乘积来作为特征交叉的权重：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-21e01cfc409cf299.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-26eb5603bd1f3234.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>FM的特征交叉部分可以通过化简来简化计算，过程如下；</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-252e185cf5abe1d1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>有关FM的更多细节，参考文章：推荐系统遇上深度学习(一)–FM模型理论和实践：<a href="https://www.jianshu.com/p/152ae633fb00" target="_blank" rel="external">https://www.jianshu.com/p/152ae633fb00</a><br>代码地址：<a href="https://github.com/princewen/tensorflow_practice/tree/master/recommendation/recommendation-FM-demo" target="_blank" rel="external">https://github.com/princewen/tensorflow_practice/tree/master/recommendation/recommendation-FM-demo</a></p>
<h4 id="3-3-FFM模型"><a href="#3-3-FFM模型" class="headerlink" title="3.3 FFM模型"></a>3.3 FFM模型</h4><p>FFM模型在FM的基础上，中引入了类别的概念，即field。还是拿上一讲中的数据来讲，先看下图：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/4155986-c3b894ee29565fe6?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>在上面的广告点击案例中，“Day=26/11/15”、“Day=1/7/14”、“Day=19/2/15”这三个特征都是代表日期的，可以放到同一个field中。同理，Country也可以放到一个field中。简单来说，同一个categorical特征经过One-Hot编码生成的数值特征都可以放到同一个field，包括用户国籍，广告类型，日期等等。</p>
<p>在FFM中，每一维特征 xi，针对其它特征的每一种field fj，都会学习一个隐向量 v_i,fj。因此，隐向量不仅与特征相关，也与field相关。也就是说，“Day=26/11/15”这个特征与“Country”特征和“Ad_type”特征进行关联的时候使用不同的隐向量，这与“Country”和“Ad_type”的内在差异相符，也是FFM中“field-aware”的由来。</p>
<p>假设样本的 n个特征属于 f个field，那么FFM的二次项有 nf个隐向量。而在FM模型中，每一维特征的隐向量只有一个。FM可以看作FFM的特例，是把所有特征都归属到一个field时的FFM模型。根据FFM的field敏感特性，可以导出其模型方程。</p>
<p><img src="http://upload-images.jianshu.io/upload_images/4155986-149fe07ecdbaf7d2?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>可以看到，如果隐向量的长度为 k，那么FFM的二次参数有 nfk 个，远多于FM模型的 nk个。此外，由于隐向量与field相关，FFM二次项并不能够化简，其预测复杂度是 O(kn^2)。</p>
<p>有关FFM的更多细节，参考文章：推荐系统遇上深度学习(二)–FFM模型理论和实践：<a href="https://www.jianshu.com/p/781cde3d5f3d" target="_blank" rel="external">https://www.jianshu.com/p/781cde3d5f3d</a><br>代码地址：<a href="https://github.com/princewen/tensorflow_practice/tree/master/recommendation/recommendation-FFM-Demo" target="_blank" rel="external">https://github.com/princewen/tensorflow_practice/tree/master/recommendation/recommendation-FFM-Demo</a></p>
<h4 id="3-4-GBDT-LR模型"><a href="#3-4-GBDT-LR模型" class="headerlink" title="3.4 GBDT+LR模型"></a>3.4 GBDT+LR模型</h4><p>Facebook 2014年的文章介绍了通过GBDT解决LR的特征组合问题，随后Kaggle竞赛也有实践此思路，GBDT与LR融合开始引起了业界关注。</p>
<p>GBDT和LR的融合方案，FaceBook的paper中有个例子：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/4155986-86038ca7349f6a8c?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>图中共有两棵树，x为一条输入样本，遍历两棵树后，x样本分别落到两颗树的叶子节点上，每个叶子节点对应LR一维特征，那么通过遍历树，就得到了该样本对应的所有LR特征。构造的新特征向量是取值0/1的。举例来说：上图有两棵树，左树有三个叶子节点，右树有两个叶子节点，最终的特征即为五维的向量。对于输入x，假设他落在左树第一个节点，编码[1,0,0]，落在右树第二个节点则编码[0,1]，所以整体的编码为[1,0,0,0,1]，这类编码作为特征，输入到LR中进行分类。</p>
<p>有关GBDT+LR的更多细节，参考文章：推荐系统遇上深度学习(十)–GBDT+LR融合方案实战：<a href="https://www.jianshu.com/p/96173f2c2fb4" target="_blank" rel="external">https://www.jianshu.com/p/96173f2c2fb4</a><br>代码地址：<a href="https://github.com/princewen/tensorflow_practice/tree/master/recommendation/GBDT%2BLR-Demo" target="_blank" rel="external">https://github.com/princewen/tensorflow_practice/tree/master/recommendation/GBDT%2BLR-Demo</a></p>
<h2 id="4、深度学习方法"><a href="#4、深度学习方法" class="headerlink" title="4、深度学习方法"></a>4、深度学习方法</h2><p>在CTR预估中，为了解决稀疏特征的问题，学者们提出了FM模型来建模特征之间的交互关系。但是FM模型只能表达特征之间两两组合之间的关系，无法建模两个特征之间深层次的关系或者说多个特征之间的交互关系，因此学者们通过Deep Network来建模更高阶的特征之间的关系。</p>
<p>因此 FM和深度网络DNN的结合也就成为了CTR预估问题中主流的方法。有关FM和DNN的结合有两种主流的方法，并行结构和串行结构。两种结构的理解以及实现如下表所示：</p>
<table>
<thead>
<tr>
<th>结构</th>
<th style="text-align:center">描述</th>
<th style="text-align:center">常见模型</th>
</tr>
</thead>
<tbody>
<tr>
<td>并行结构</td>
<td style="text-align:center">FM部分和DNN部分分开计算，只在输出层进行一次融合得到结果</td>
<td style="text-align:center">DeepFM，DCN，Wide&amp;Deep</td>
</tr>
<tr>
<td>串行结构</td>
<td style="text-align:center">将FM的一次项和二次项结果(或其中之一)作为DNN部分的输入，经DNN得到最终结果</td>
<td style="text-align:center">PNN,NFM,AFM</td>
</tr>
</tbody>
</table>
<p>两类结构的典型网络模型如下图：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/4155986-3e3163f1c4437ed7?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p><img src="http://upload-images.jianshu.io/upload_images/4155986-4008a1822870d64f?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>下面，我们回顾一下这两类结构的典型模型。</p>
<h4 id="4-1-并行结构"><a href="#4-1-并行结构" class="headerlink" title="4.1 并行结构"></a>4.1 并行结构</h4><h4 id="4-1-1-Wide-amp-Deep模型"><a href="#4-1-1-Wide-amp-Deep模型" class="headerlink" title="4.1.1 Wide &amp; Deep模型"></a>4.1.1 Wide &amp; Deep模型</h4><p>Wide &amp; Deep模型本系列还没有整理，不过可以简单介绍一下。Wide &amp; Deep模型结构如下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-5e300d9acebcbe67.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p><strong>Wide部分</strong><br>wide部分就是一个广义线性模型，输入主要由两部分，一部分是原始特征，另一部分是交互特征，我们可以通过cross-product transformation的形式来构造K组交互特征：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-520af2feeea172e0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p><strong>Deep部分</strong><br>Deep部分就是一个DNN的模型，每一层计算如下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-14c58d46b353670b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p><strong>联合训练</strong></p>
<p>Wide &amp; Deep模型采用的是联合训练的形式，而非集成。二者的区别就是联合训练公用一个损失函数，然后同时更新各个部分的参数，而集成方法是独立训练N个模型，然后进行融合。因此，模型的输出为：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-c4c5e5f40e25e652.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>有关Wide&amp;Deep模型更多的细节，大家可以阅读原论文，或者关注本系列后续的文章。</p>
<h4 id="4-1-2-DeepFM模型"><a href="#4-1-2-DeepFM模型" class="headerlink" title="4.1.2 DeepFM模型"></a>4.1.2 DeepFM模型</h4><p>我们先来看一下DeepFM的模型结构：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/4155986-87d500bcd2bb54ee?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>DeepFM包含两部分：神经网络部分与因子分解机部分，分别负责低阶特征的提取和高阶特征的提取。这两部分<strong>共享同样的输入</strong>。DeepFM的预测结果可以写为：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/4155986-a0500b565e02b00f?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p><strong>FM部分</strong></p>
<p>FM部分的详细结构如下：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/4155986-d39c0f2053f68099?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>FM部分是一个因子分解机。这里我们不再过多介绍。FM的输出公式为：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/4155986-18ca7f734ceea229?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p><strong>深度部分</strong></p>
<p><img src="http://upload-images.jianshu.io/upload_images/4155986-f51933a36def647b?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>深度部分是一个前馈神经网络。与图像或者语音这类输入不同，图像语音的输入一般是连续而且密集的，然而用于CTR的输入一般是及其稀疏的。因此需要重新设计网络结构。具体实现中为，在第一层隐含层之前，引入一个嵌入层来完成将输入向量压缩到低维稠密向量。</p>
<p><img src="http://upload-images.jianshu.io/upload_images/4155986-0122fd3309b4959e?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>嵌入层(embedding layer)的结构如上图所示。当前网络结构有两个有趣的特性，1）尽管不同field的输入长度不同，但是embedding之后向量的长度均为K。2)对同一个特征来说，FM的隐变量和Embedding之后的向量是相同的，这两部分<strong>共享同样的输入</strong>。</p>
<p>有关DeepFM的更多细节，参考文章：推荐系统遇上深度学习(三)–DeepFM模型理论和实践：<a href="https://www.jianshu.com/p/6f1c2643d31b" target="_blank" rel="external">https://www.jianshu.com/p/6f1c2643d31b</a><br>代码地址：<a href="https://github.com/princewen/tensorflow_practice/tree/master/recommendation/Basic-DeepFM-model" target="_blank" rel="external">https://github.com/princewen/tensorflow_practice/tree/master/recommendation/Basic-DeepFM-model</a></p>
<h4 id="4-1-3-Deep-Cross-Network"><a href="#4-1-3-Deep-Cross-Network" class="headerlink" title="4.1.3 Deep Cross Network"></a>4.1.3 Deep Cross Network</h4><p>一个DCN模型从嵌入和堆积层开始，接着是一个交叉网络和一个与之平行的深度网络，之后是最后的组合层，它结合了两个网络的输出。完整的网络模型如图：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/4155986-b1452b98b574f72f?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image"></p>
<p><strong>嵌入和堆叠层</strong><br>我们考虑具有离散和连续特征的输入数据。在网络规模推荐系统中，如CTR预测，输入主要是分类特征，如“country=usa”。这些特征通常是编码为独热向量如“[ 0,1,0 ]”；然而，这往往导致过度的高维特征空间大的词汇。</p>
<p>为了减少维数，我们采用嵌入过程将这些离散特征转换成实数值的稠密向量（通常称为嵌入向量）：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/4155986-63b4322397513921?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>然后，我们将嵌入向量与连续特征向量叠加起来形成一个向量：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/4155986-6bf081cf072223ef?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>拼接起来的向量X0将作为我们Cross Network和Deep Network的输入</p>
<p><strong>Cross Network</strong></p>
<p>交叉网络的核心思想是以有效的方式应用显式特征交叉。交叉网络由交叉层组成，每个层具有以下公式：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/4155986-fa3574d61d7760d5?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>一个交叉层的可视化如图所示:</p>
<p><img src="http://upload-images.jianshu.io/upload_images/4155986-ff7359020d1c3a99?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>可以看到，交叉网络的特殊结构使交叉特征的程度随着层深度的增加而增大。多项式的最高程度（就输入X0而言）为L层交叉网络L + 1。如果用Lc表示交叉层数，d表示输入维度。然后，参数的数量参与跨网络参数为：d <em> Lc </em> 2 (w和b)</p>
<p>交叉网络的少数参数限制了模型容量。为了捕捉高度非线性的相互作用，模型并行地引入了一个深度网络。</p>
<p><strong>Deep Network</strong></p>
<p>深度网络就是一个全连接的前馈神经网络，每个深度层具有如下公式：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/4155986-8927d17fcce09bbd?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p><strong>Combination Layer</strong></p>
<p>链接层将两个并行网络的输出连接起来，经过一层全链接层得到输出：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/4155986-e565ae61e140a7e2?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>如果采用的是对数损失函数，那么损失函数形式如下：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/4155986-ad12260191f657fc?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>有关DCN的更多细节，参考文章：推荐系统遇上深度学习(五)–Deep&amp;Cross Network模型理论和实践：<a href="https://www.jianshu.com/p/77719fc252fa" target="_blank" rel="external">https://www.jianshu.com/p/77719fc252fa</a><br>代码地址：<a href="https://github.com/princewen/tensorflow_practice/tree/master/recommendation/Basic-DCN-Demo" target="_blank" rel="external">https://github.com/princewen/tensorflow_practice/tree/master/recommendation/Basic-DCN-Demo</a></p>
<h4 id="4-2-串行结构"><a href="#4-2-串行结构" class="headerlink" title="4.2 串行结构"></a>4.2 串行结构</h4><h5 id="4-2-1-Product-based-Neural-Network"><a href="#4-2-1-Product-based-Neural-Network" class="headerlink" title="4.2.1 Product-based Neural Network"></a>4.2.1 Product-based Neural Network</h5><p>PNN，全称为Product-based Neural Network，认为在embedding输入到MLP之后学习的交叉特征表达并不充分，提出了一种product layer的思想，既基于乘法的运算来体现体征交叉的DNN网络结构，如下图：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/4155986-dcd4347c2d4435f5?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>我们这里主要来关注一下Product-Layer，product layer可以分成两个部分，一部分是线性部分lz，一部分是非线性部分lp。</p>
<p><img src="http://upload-images.jianshu.io/upload_images/4155986-ca2165d2efb641ff?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>看上面的公式，我们首先需要知道z和p，这都是由我们的embedding层得到的，其中z是线性信号向量，因此我们直接用embedding层得到：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/4155986-f6c4e1a55b1acf46?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>论文中使用的等号加一个三角形，其实就是相等的意思，你可以认为z就是embedding层的复制。</p>
<p>对于p来说，这里需要一个公式进行映射：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/4155986-19a4de0536baec52?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p><img src="http://upload-images.jianshu.io/upload_images/4155986-a9c0a7f30199f396?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>不同的g的选择使得我们有了两种PNN的计算方法，一种叫做Inner PNN，简称IPNN，一种叫做Outer PNN，简称OPNN。</p>
<p><strong>IPNN</strong></p>
<p>IPNN的示意图如下：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/4155986-544b94047fdd8c13?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>IPNN中p的计算方式如下，即使用内积来代表pij：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/4155986-00b6ab3db4a55a8f?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p><strong>OPNN</strong></p>
<p>OPNN的示意图如下：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/4155986-930910cd08f46c27?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>OPNN中p的计算方式如下：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/4155986-52b89501393c2b5e?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>有关PNN的更多细节，参考文章：推荐系统遇上深度学习(六)–PNN模型理论和实践：<a href="https://www.jianshu.com/p/be784ab4abc2" target="_blank" rel="external">https://www.jianshu.com/p/be784ab4abc2</a><br>代码地址：<a href="https://github.com/princewen/tensorflow_practice/tree/master/recommendation/Basic-PNN-Demo" target="_blank" rel="external">https://github.com/princewen/tensorflow_practice/tree/master/recommendation/Basic-PNN-Demo</a></p>
<h5 id="4-2-2-Neural-factorization-machines"><a href="#4-2-2-Neural-factorization-machines" class="headerlink" title="4.2.2 Neural factorization machines"></a>4.2.2 Neural factorization machines</h5><p>对于NFM模型，目标值的预测公式变为：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/4155986-e0162f0ab1c39cf3?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>其中，f(x)是用来建模特征之间交互关系的多层前馈神经网络模块，架构图如下所示：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/4155986-efad68ef04a968ae?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p><strong>Embedding Layer</strong>和我们之间几个网络是一样的，embedding 得到的vector其实就是我们在FM中要学习的隐变量v。</p>
<p><strong>Bi-Interaction Layer</strong>名字挺高大上的，其实它就是计算FM中的二次项的过程，因此得到的向量维度就是我们的Embedding的维度。最终的结果是：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/4155986-b190ee2503d04297?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>Hidden Layers就是我们的DNN部分，将Bi-Interaction Layer得到的结果接入多层的神经网络进行训练，从而捕捉到特征之间复杂的非线性关系。</p>
<p>在进行多层训练之后，将最后一层的输出求和同时加上一次项和偏置项，就得到了我们的预测输出：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/4155986-3b4a0e3268bb8edb?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>有关NFM的更多细节，参考文章：推荐系统遇上深度学习(七)–NFM模型理论和实践：<a href="https://www.jianshu.com/p/4e65723ee632" target="_blank" rel="external">https://www.jianshu.com/p/4e65723ee632</a><br>代码地址：<a href="https://github.com/princewen/tensorflow_practice/tree/master/recommendation/Basic-NFM-Demo" target="_blank" rel="external">https://github.com/princewen/tensorflow_practice/tree/master/recommendation/Basic-NFM-Demo</a></p>
<h5 id="4-2-3-Attention-Based-factorization-machines"><a href="#4-2-3-Attention-Based-factorization-machines" class="headerlink" title="4.2.3 Attention-Based factorization machines"></a>4.2.3 Attention-Based factorization machines</h5><p>在进行预测时，FM会让一个特征固定一个特定的向量，当这个特征与其他特征做交叉时，都是用同样的向量去做计算。这个是很不合理的，因为不同的特征之间的交叉，重要程度是不一样的。如何体现这种重要程度，之前介绍的FFM模型是一个方案。另外，结合了attention机制的AFM模型，也是一种解决方案。</p>
<p>关于什么是attention model？本文不打算详细赘述，我们这里只需要知道的是，attention机制相当于一个加权平均，attention的值就是其中权重，判断不同特征之间交互的重要性。</p>
<p>刚才提到了，attention相等于加权的过程，因此我们的预测公式变为：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/4155986-16a87a156e89e83b?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>圆圈中有个点的符号代表的含义是element-wise product，即：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/4155986-b5d125fbefffc44b?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>因此，我们在求和之后得到的是一个K维的向量，还需要跟一个向量p相乘，得到一个具体的数值。</p>
<p>可以看到，AFM的前两部分和FM相同，后面的一项经由如下的网络得到：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/4155986-cbc3fbdd1a969529?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>图中的前三部分：sparse iput，embedding layer，pair-wise interaction layer，都和FM是一样的。而后面的两部分，则是AFM的创新所在，也就是我们的Attention net。Attention背后的数学公式如下：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/4155986-eeb3fea23988b651?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>总结一下，不难看出AFM只是在FM的基础上添加了attention的机制，但是实际上，由于最后的加权累加，二次项并没有进行更深的网络去学习非线性交叉特征，所以AFM并没有发挥出DNN的优势，也许结合DNN可以达到更好的结果。</p>
<p>有关AFM的更多细节，参考文章：推荐系统遇上深度学习(八)–AFM模型理论和实践：<a href="https://www.jianshu.com/p/83d3b2a1e55d" target="_blank" rel="external">https://www.jianshu.com/p/83d3b2a1e55d</a><br>代码地址：<a href="https://github.com/princewen/tensorflow_practice/tree/master/recommendation/Basic-AFM-Demo" target="_blank" rel="external">https://github.com/princewen/tensorflow_practice/tree/master/recommendation/Basic-AFM-Demo</a></p>
<h2 id="5、强化学习方法"><a href="#5、强化学习方法" class="headerlink" title="5、强化学习方法"></a>5、强化学习方法</h2><p>在《DRN:A Deep Reinforcement Learning Framework for News Recommendation》提出了一种基于强化学习的新闻推荐模型，一起来回顾一下：</p>
<p><strong>问题及解决方案</strong><br>本文提出的方法主要针对三个问题：<br>1、使用DQN来建模用户兴趣的动态变化性<br>2、推荐算法通常只考虑用户的点击／未点击 或者 用户的评分作为反馈，本文将用户活跃度作为一种反馈信息。<br>3、目前的推荐系统倾向于推荐用户重复或相似内容的东西，本文使用Dueling Bandit Gradient Descent方法来进行有效的探索。</p>
<p>因此本文的框架如下：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/4155986-83066f007db3fa1a?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p><strong>模型整体框架</strong><br>模型整体框架如下图所示：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/4155986-a5bee70e9a4c5727?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>有几个关键的环节：<br><strong>PUSH</strong>：在每一个时刻，用户发送请求时，agent根据当前的state产生k篇新闻推荐给用户，这个推荐结果是exploitation和exploration的结合</p>
<p><strong>FEEDBACK</strong>：通过用户对推荐新闻的点击行为得到反馈结果。</p>
<p><strong>MINOR UPDATE</strong>：在每个时间点过后，根据用户的信息（state）和推荐的新闻（action）及得到的反馈（reward），agent会评估exploitation network Q 和 exploration network Q ̃ 的表现，如果exploitation network Q效果更好，则模型保持不动，如果 exploration network Q ̃ 的表现更好，exploitation network Q的参数将会向exploration network Q ̃变化。</p>
<p><strong>MAJOR UPDATE</strong>：在一段时间过后，根据DQN的经验池中存放的历史经验，对exploitation network Q 模型参数进行更新。</p>
<p><strong>强化学习模型</strong><br>本文的探索模型使用的是Double-Dueling结构，状态由用户特征和上下文特征组成，动作由新闻特征，用户-新闻交互特征组成：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/4155986-5213b5683f51b7a2?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p><strong>探索模型</strong></p>
<p>本文的探索采取的是Dueling Bandit Gradient Descent 算法，算法的结构如下：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/4155986-f827ea7a64fca997?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>在DQN网络的基础上又多出来一个exploration network Q ̃ ，这个网络的参数是由当前的Q网络参数基础上加入一定的噪声产生的。当一个用户请求到来时，由两个网络同时产生top-K的新闻列表，然后将二者产生的新闻进行一定程度的混合，然后得到用户的反馈。如果exploration network Q ̃的效果好的话，那么当前Q网络的参数向着exploration network Q ̃的参数方向进行更新。</p>
<p>有关本论文的更多细节，参考文章：推荐系统遇上深度学习(十四)–《DRN:A Deep Reinforcement Learning Framework for News Recommendation》：<a href="https://www.jianshu.com/p/c0384b213320" target="_blank" rel="external">https://www.jianshu.com/p/c0384b213320</a></p>
<h2 id="6、推荐系统的EE问题"><a href="#6、推荐系统的EE问题" class="headerlink" title="6、推荐系统的EE问题"></a>6、推荐系统的EE问题</h2><p><strong>Exploration and Exploitation(EE问题，探索与开发)</strong>是计算广告和推荐系统里常见的一个问题，为什么会有EE问题？简单来说，是为了平衡推荐系统的准确性和多样性。</p>
<p>EE问题中的Exploitation就是：对用户比较确定的兴趣，当然要利用开采迎合，好比说已经挣到的钱，当然要花；而exploration就是：光对着用户已知的兴趣使用，用户很快会腻，所以要不断探索用户新的兴趣才行，这就好比虽然有一点钱可以花了，但是还得继续搬砖挣钱，不然花完了就得喝西北风。</p>
<h4 id="6-1-Bandit算法"><a href="#6-1-Bandit算法" class="headerlink" title="6.1 Bandit算法"></a>6.1 Bandit算法</h4><p>Bandit算法是解决EE问题的一种有效算法，Bandit算法来源于历史悠久的赌博学，它要解决的问题是这样的：一个赌徒，要去摇老虎机，走进赌场一看，一排老虎机，外表一模一样，但是每个老虎机吐钱的概率可不一样，他不知道每个老虎机吐钱的概率分布是什么，那么每次该选择哪个老虎机可以做到最大化收益呢？这就是多臂赌博机问题（Multi-armed bandit problem, K-armed bandit problem, MAB）。</p>
<p>Bandit算法如何同推荐系统中的EE问题联系起来呢？假设我们已经经过一些试验，得到了当前每个老虎机的吐钱的概率，如果想要获得最大的收益，我们会一直摇哪个吐钱概率最高的老虎机，这就是Exploitation。但是，当前获得的信息并不是老虎机吐钱的真实概率，可能还有更好的老虎机吐钱概率更高，因此还需要进一步探索，这就是Exploration问题。</p>
<p>下面介绍几种经典的Bandit算法：</p>
<p><strong>朴素Bandit算法</strong>：先随机试若干次，计算每个臂的平均收益，一直选均值最大那个臂。</p>
<p><strong>Epsilon-Greedy算法</strong>：选一个(0,1)之间较小的数epsilon，每次以epsilon的概率在所有臂中随机选一个。以1-epsilon的概率选择截止当前，平均收益最大的那个臂。根据选择臂的回报值来对回报期望进行更新。</p>
<p><strong>Thompson sampling算法</strong>：Thompson sampling算法用到了Beta分布，该方法假设每个老虎机都有一个吐钱的概率p，同时该概率p的概率分布符合beta(wins, lose)分布，每个臂都维护一个beta分布的参数，即wins, lose。每次试验后，选中一个臂，摇一下，有收益则该臂的wins增加1，否则该臂的lose增加1。每次选择臂的方式是：用每个臂现有的beta分布产生一个随机数b，选择所有臂产生的随机数中最大的那个臂去摇。</p>
<p><strong>UCB算法</strong>：该算法在每次推荐时，总是乐观的认为每个老虎机能够得到的收益是p’ + ∆。p’ + ∆的计算公式如下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-a09ff98ee9c5061b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>其中加号前面是第j个老虎机到目前的收益均值，后面的叫做bonus，本质上是均值的标准差，T是目前的试验次数，n是该老虎机被试次数。</p>
<p>有关EE问题的更多细节，参考文章：推荐系统遇上深度学习(十二)–推荐系统中的EE问题及基本Bandit算法：<a href="https://www.jianshu.com/p/95b2de50ce44" target="_blank" rel="external">https://www.jianshu.com/p/95b2de50ce44</a></p>
<h4 id="6-2-LinUCB算法"><a href="#6-2-LinUCB算法" class="headerlink" title="6.2 LinUCB算法"></a>6.2 LinUCB算法</h4><p>上面提到的MAB都是context-free，即没有考虑到用户的个性化问题，因此实际中很少应用。现实中我们大都采用考虑上下文的Contextual Bandit算法。LinUCB便是其中之一。</p>
<p>既然是UCB算法的扩展，那我们还是根据p’ + ∆来选择合适的老虎机。p’的计算基于有监督的学习方法。我们为每个老虎机维护一个特征向量D，同时上下文特征我们写作θ，然后通过收集的反馈进行有监督学习：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-e06e90515ee229f2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>而置信上界基于下面的公式进行计算：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-d40c19bf4ee5dae5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>因此LinUCB算法的流程如下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-1ce3fe2781dac963.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>有关LinUCB的更多细节，参考文章：推荐系统遇上深度学习(十三)–linUCB方法浅析及实现：<a href="https://www.jianshu.com/p/e0e843d78e3c" target="_blank" rel="external">https://www.jianshu.com/p/e0e843d78e3c</a><br>代码地址：<a href="https://github.com/princewen/tensorflow_practice/tree/master/recommendation/Basic-Bandit-Demo" target="_blank" rel="external">https://github.com/princewen/tensorflow_practice/tree/master/recommendation/Basic-Bandit-Demo</a></p>
<h2 id="7、推荐系统在公司中的实战"><a href="#7、推荐系统在公司中的实战" class="headerlink" title="7、推荐系统在公司中的实战"></a>7、推荐系统在公司中的实战</h2><h4 id="7-1-阿里MLR算法"><a href="#7-1-阿里MLR算法" class="headerlink" title="7.1 阿里MLR算法"></a>7.1 阿里MLR算法</h4><p>MLR可以看做是对LR的一个自然推广，它采用分而治之的思路，用分片线性的模式来拟合高维空间的非线性分类面，其形式化表达如下：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/4155986-46e92f37f013a32e?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>其中u是聚类参数，决定了空间的划分，w是分类参数，决定空间内的预测。这里面超参数分片数m可以较好地平衡模型的拟合与推广能力。当m=1时MLR就退化为普通的LR，m越大模型的拟合能力越强，但是模型参数规模随m线性增长，相应所需的训练样本也随之增长。因此实际应用中m需要根据实际情况进行选择。例如，在阿里的场景中，m一般选择为12。下图中MLR模型用4个分片可以完美地拟合出数据中的菱形分类面。</p>
<p><img src="http://upload-images.jianshu.io/upload_images/4155986-a84eda06ad7f9216?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>在实际中，MLR算法常用的形式如下，使用softmax作为分片函数：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/4155986-f462bfa9b62e8a91?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>在这种情况下，MLR模型可以看作是一个FOE model：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/4155986-b4ce399c599794e2?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>关于损失函数的设计，阿里采用了 neg-likelihood loss function以及L1，L2正则，形式如下：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/4155986-1080c174ebef8089?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>由于加入了正则项，MLR算法变的不再是平滑的凸函数，梯度下降法不再适用，因此模型参数的更新使用LBFGS和OWLQN的结合，具体的优化细节大家可以参考论文<a href="https://arxiv.org/pdf/1704.05194.pdf" target="_blank" rel="external">https://arxiv.org/pdf/1704.05194.pdf</a>.</p>
<p>有关MLR的更多细节，参考文章：推荐系统遇上深度学习(十七)–探秘阿里之MLR算法浅析及实现：<a href="https://www.jianshu.com/p/627fc0d755b2" target="_blank" rel="external">https://www.jianshu.com/p/627fc0d755b2</a><br>代码地址：<a href="https://github.com/princewen/tensorflow_practice/tree/master/recommendation/Basic-MLR-Demo" target="_blank" rel="external">https://github.com/princewen/tensorflow_practice/tree/master/recommendation/Basic-MLR-Demo</a></p>
<h4 id="7-2-阿里Deep-Interest-Network"><a href="#7-2-阿里Deep-Interest-Network" class="headerlink" title="7.2 阿里Deep Interest Network"></a>7.2 阿里Deep Interest Network</h4><p>阿里的研究者们通过观察收集到的线上数据，发现了用户行为数据中有两个很重要的特性：<br><strong>Diversity</strong>：用户在浏览电商网站的过程中显示出的兴趣是十分多样性的。<br><strong>Local activation</strong>: 由于用户兴趣的多样性，只有部分历史数据会影响到当次推荐的物品是否被点击，而不是所有的历史记录。</p>
<p>针对上面的两种特性，阿里在推荐网络中增加了一个Attention机制，对用户的历史行为进行加权：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/4155986-1a71a28c081dd25b?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>还有两个值得注意的细节：</p>
<p><strong>评价指标GAUC</strong></p>
<p>模型使用的评价指标是GAUC，我们先来看一下GAUC的计算公式：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/4155986-580d573dc4e1e338?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>GAUC的计算，不仅将每个用户的AUC分开计算，同时根据用户的展示数或者点击数来对每个用户的AUC进行加权处理。进一步消除了用户偏差对模型的影响。通过实验证明，GAUC确实是一个更加合理的评价指标。</p>
<p><strong>Dice激活函数</strong></p>
<p>使用PRelu作为激活函数时，存在一个问题，即我们认为分割点都是0，但实际上，分割点应该由数据决定，因此文中提出了Dice激活函数。</p>
<p>Dice激活函数的全称是Data Dependent Activation Function，形式如下：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/4155986-5f3e41adecc774a3?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>其中，期望和方差的计算如下：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/4155986-77cb449f01b3b8ce?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>可也看到，每一个yi对应了一个概率值pi。pi的计算主要分为两步：将yi进行标准化和进行sigmoid变换。</p>
<p><strong>自适应正则 Adaptive Regularization</strong></p>
<p>针对用户数据中的长尾情况，阿里提出了<strong>自适应正则</strong>的做法，即：<br>1.针对feature id出现的频率，来自适应的调整他们正则化的强度；<br>2.对于出现频率高的，给与较小的正则化强度；<br>3.对于出现频率低的，给予较大的正则化强度。</p>
<p>计算公式如下：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/4155986-6c402afad4e5e5dd?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>有关DIN的更多细节，参考文章：推荐系统遇上深度学习(十八)–探秘阿里之深度兴趣网络(DIN)浅析及实现：<a href="https://www.jianshu.com/p/73b6f5d00f46" target="_blank" rel="external">https://www.jianshu.com/p/73b6f5d00f46</a></p>
<h4 id="7-3-阿里ESSM模型"><a href="#7-3-阿里ESSM模型" class="headerlink" title="7.3 阿里ESSM模型"></a>7.3 阿里ESSM模型</h4><p>该模型主要解决的是CVR预估中的两个主要问题：<strong>样本选择偏差</strong>和<strong>稀疏数据</strong>。<br><strong>样本选择偏差</strong>：大多数CVR预估问题是在用户点击过的样本空间上进行训练的，而预测的时候却要对整个样本空间的样本进行预测。这种训练样本从整体样本空间的一个较小子集中提取，而训练得到的模型却需要对整个样本空间中的样本做推断预测的现象称之为样本选择偏差。</p>
<p><strong>数据稀疏</strong>：用户点击过的物品只占整个样本空间的很小一部分，使得模型训练十分困难。</p>
<p>阿里妈妈的算法同学提出的ESMM模型借鉴了多任务学习的思路，引入了两个辅助的学习任务，分别用来拟合pCTR和pCTCVR，从而同时消除了上文提到的两个挑战。ESMM模型能够充分利用用户行为的顺序性模式，其模型架构下图所示：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/4155986-b6e9b6255fabc2ac?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>可以看到，ESSM模型由两个子网络组成，左边的子网络用来拟合pCVR，右边的子网络用来拟合pCTR，同时，两个子网络的输出相乘之后可以得到pCTCVR。因此，该网络结构共有三个子任务，分别用于输出pCTR、pCVR和pCTCVR。</p>
<p>假设我们用x表示feature(即impression),y表示点击，z表示转化，那么根据pCTCVR = pCTR * pCVR，可以得到：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/4155986-0fd41dfca9690b36?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>将乘法转化为除法，我们可以得到pCVR的计算：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/4155986-39d1aea425974886?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>我们将有点击行为的曝光事件作为正样本，没有点击行为的曝光事件作为负样本，来做CTR预估的任务。将同时有点击行为和购买行为的曝光事件作为正样本，其他作为负样本来训练CTCVR的预估部分。</p>
<p>模型具体是怎么做的呢？可以看到，用来训练两个任务的输入x其实是相同的，但是label是不同的。CTR任务预估的是点击y，CTCVR预估的是转化z。因此，我们将(x,y)输入到CTR任务中，得到CTR的预估值，将(x,z)输入到CVR任务中，得到CVR的预估值，CTR和CVR的预估值相乘，便得到了CTCVR的预估值。因此，模型的损失函数可以定义为：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/4155986-89dd4ba58d2205c4?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>其中，θctr和θcvr分别是CTR网络和CVR网络的参数，l(⋅)是交叉熵损失函数。</p>
<p>同时，还需要提醒的一点是，两个子网络的embedding层共享embedding向量。</p>
<p>有关ESSM模型的更多细节，参考文章：推荐系统遇上深度学习(十九)–探秘阿里之完整空间多任务模型ESSM：<a href="https://www.jianshu.com/p/35f00299c059" target="_blank" rel="external">https://www.jianshu.com/p/35f00299c059</a></p>
<h4 id="7-4-京东强化学习推荐模型"><a href="#7-4-京东强化学习推荐模型" class="headerlink" title="7.4 京东强化学习推荐模型"></a>7.4 京东强化学习推荐模型</h4><p>京东通过强化学习来进行 List-wise 的推荐。</p>
<p><strong>构建线上环境仿真器</strong><br>在推荐系统上线之前，需要进行线下的训练和评估，训练和评估主要基于用户的历史行为数据，但是，我们只有ground-truth的数据和相应的反馈。因此，对于整个动作空间来说(也就是所有物品的可能组合)，这是非常稀疏的。这会造成两个问题，首先只能拿到部分的state-action对进行训练，无法对所有的情况进行建模(可能造成过拟合)，其次会造成线上线下环境的不一致性。因此，需要一个仿真器来仿真没有出现过的state-action的reward值，用于训练和评估线下模型。</p>
<p>仿真器的构建主要基于用户的历史数据，其基本思想是给定一个相似的state和action，不同的用户也会作出相似的feedback。来建模state-action的reward值。</p>
<p><strong>模型结构</strong><br>该模型通过Actor-Critic方法，结合刚才建立的仿真器，进行训练：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-cfd89ba8439f1e28.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>有关该方法的具体细节，参考文章：推荐系统遇上深度学习(十五)–强化学习在京东推荐中的探索：<a href="https://www.jianshu.com/p/b9113332e33e" target="_blank" rel="external">https://www.jianshu.com/p/b9113332e33e</a></p>
<h2 id="9、论文整理"><a href="#9、论文整理" class="headerlink" title="9、论文整理"></a>9、论文整理</h2><p>1、GBDT+LR：<a href="http://quinonero.net/Publications/predicting-clicks-facebook.pdf" target="_blank" rel="external">http://quinonero.net/Publications/predicting-clicks-facebook.pdf</a><br>2、DeepFM：<a href="https://arxiv.org/abs/1703.04247" target="_blank" rel="external">https://arxiv.org/abs/1703.04247</a><br>3、Wide&amp;Deep：<a href="https://dl.acm.org/citation.cfm?id=2988454" target="_blank" rel="external">https://dl.acm.org/citation.cfm?id=2988454</a><br>4、DCN：<a href="https://arxiv.org/pdf/1708.05123" target="_blank" rel="external">https://arxiv.org/pdf/1708.05123</a><br>5、PNN：<a href="https://arxiv.org/pdf/1611.00144" target="_blank" rel="external">https://arxiv.org/pdf/1611.00144</a><br>6、AFM：<a href="https://www.comp.nus.edu.sg/~xiangnan/papers/ijcai17-afm.pdf" target="_blank" rel="external">https://www.comp.nus.edu.sg/~xiangnan/papers/ijcai17-afm.pdf</a><br>7、NFM：<a href="https://arxiv.org/abs/1708.05027" target="_blank" rel="external">https://arxiv.org/abs/1708.05027</a><br>8、LinUCB：<a href="https://arxiv.org/pdf/1003.0146.pdf" target="_blank" rel="external">https://arxiv.org/pdf/1003.0146.pdf</a><br>9、MLR：<a href="https://arxiv.org/pdf/1704.05194.pdf" target="_blank" rel="external">https://arxiv.org/pdf/1704.05194.pdf</a><br>10、DIN：<a href="https://arxiv.org/abs/1706.06978" target="_blank" rel="external">https://arxiv.org/abs/1706.06978</a><br>11、ESSM：<a href="https://arxiv.org/abs/1804.07931" target="_blank" rel="external">https://arxiv.org/abs/1804.07931</a><br>12、京东：<a href="https://arxiv.org/abs/1801.00209" target="_blank" rel="external">https://arxiv.org/abs/1801.00209</a></p>
<p>欢迎关注个人公众号：小小挖掘机</p>
<p><img src="http://upload-images.jianshu.io/upload_images/4155986-808d0609856fbc30?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<h1 id="推荐系统遇上深度学习-二十二-–DeepFM升级版XDeepFM模型强势来袭！"><a href="#推荐系统遇上深度学习-二十二-–DeepFM升级版XDeepFM模型强势来袭！" class="headerlink" title="推荐系统遇上深度学习(二十二)–DeepFM升级版XDeepFM模型强势来袭！"></a>推荐系统遇上深度学习(二十二)–DeepFM升级版XDeepFM模型强势来袭！</h1><p>秋招基本结束，让我们继续学习！长期有耐心！</p>
<p>今天我们要学习的模型是xDeepFM模型，论文地址为：<a href="https://arxiv.org/abs/1803.05170。文中包含我个人的一些理解，如有不对的地方，欢迎大家指正！废话不多说，我们进入正题！" target="_blank" rel="external">https://arxiv.org/abs/1803.05170。文中包含我个人的一些理解，如有不对的地方，欢迎大家指正！废话不多说，我们进入正题！</a></p>
<h2 id="1、引言-4"><a href="#1、引言-4" class="headerlink" title="1、引言"></a>1、引言</h2><p>对于预测性的系统来说，特征工程起到了至关重要的作用。特征工程中，挖掘交叉特征是至关重要的。交叉特征指的是两个或多个原始特征之间的交叉组合。例如，在新闻推荐场景中，一个三阶交叉特征为AND(user_organization=msra,item_category=deeplearning,time=monday_morning),它表示当前用户的工作单位为微软亚洲研究院，当前文章的类别是与深度学习相关的，并且推送时间是周一上午。</p>
<p>传统的推荐系统中，挖掘交叉特征主要依靠人工提取，这种做法主要有以下三种缺点：</p>
<p>1）重要的特征都是与应用场景息息相关的，针对每一种应用场景，工程师们都需要首先花费大量时间和精力深入了解数据的规律之后才能设计、提取出高效的高阶交叉特征，因此人力成本高昂；<br>2）原始数据中往往包含大量稀疏的特征，例如用户和物品的ID，交叉特征的维度空间是原始特征维度的乘积，因此很容易带来维度灾难的问题；<br>3）人工提取的交叉特征无法泛化到未曾在训练样本中出现过的模式中。</p>
<p>因此自动学习特征间的交互关系是十分有意义的。目前大部分相关的研究工作是基于因子分解机的框架，利用多层全连接神经网络去自动学习特征间的高阶交互关系，例如FNN、PNN和DeepFM等。其缺点是模型学习出的是隐式的交互特征，其形式是未知的、不可控的；同时它们的特征交互是发生在<strong>元素级（bit-wise）</strong>而不是<strong>特征向量之间（vector-wise）</strong>，这一点违背了因子分解机的初衷。来自Google的团队在KDD 2017 AdKDD&amp;TargetAD研讨会上提出了DCN模型，旨在显式（explicitly）地学习高阶特征交互，其优点是模型非常轻巧高效，但缺点是最终模型的表现形式是一种很特殊的向量扩张，同时特征交互依旧是发生在元素级上。</p>
<p>我们用下图来回顾一下DCN的实现：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/4155986-ecee2d8901361e99?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>下面是我对文中提到的两个重要概念的理解：</p>
<p><strong>bit-wise VS vector-wise</strong><br>假设隐向量的维度为3维，如果两个特征(对应的向量分别为(a1,b1,c1)和(a2,b2,c2)的话）在进行交互时，交互的形式类似于f(w1 <em> a1 </em> a2,w2 <em> b1 </em> b2 ,w3 <em> c1 </em> c2)的话，此时我们认为特征交互是发生在<strong>元素级（bit-wise）</strong>上。如果特征交互形式类似于 f(w <em> (a1 </em> a2 ,b1 <em> b2,c1 </em> c2))的话，那么我们认为特征交互是发生在<strong>特征向量级（vector-wise）</strong>。</p>
<p><strong>explicitly VS implicitly</strong><br>显式的特征交互和隐式的特征交互。以两个特征为例xi和xj，在经过一系列变换后，我们可以表示成 wij <em> (xi </em> xj)的形式，就可以认为是显式特征交互，否则的话，是隐式的特征交互。</p>
<p>微软亚洲研究院社会计算组提出了一种极深因子分解机模型（xDeepFM），不仅能同时以显式和隐式的方式自动学习高阶的特征交互，使特征交互发生在向量级，还兼具记忆与泛化的学习能力。</p>
<p>我们接下来就来看看xDeepFM这个模型是怎么做的吧！</p>
<h2 id="2、xDeepFM模型介绍"><a href="#2、xDeepFM模型介绍" class="headerlink" title="2、xDeepFM模型介绍"></a>2、xDeepFM模型介绍</h2><h4 id="2-1-Compressed-Interaction-Network"><a href="#2-1-Compressed-Interaction-Network" class="headerlink" title="2.1 Compressed Interaction Network"></a>2.1 Compressed Interaction Network</h4><p>为了实现自动学习显式的高阶特征交互，同时使得交互发生在向量级上，文中首先提出了一种新的名为压缩交互网络（Compressed Interaction Network，简称CIN）的神经模型。在CIN中，隐向量是一个单元对象，因此我们将输入的原特征和神经网络中的隐层都分别组织成一个矩阵，记为X<sup>0</sup> 和 X<sup>k</sup>。CIN中每一层的神经元都是根据前一层的隐层以及原特征向量推算而来，其计算公式如下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-f6fd001a226680f5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>其中点乘的部分计算如下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-1f1a75dc0c18ed69.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>我们来解释一下上面的过程，第k层隐层含有H<sup>k</sup>条神经元向量。隐层的计算可以分成两个步骤：（1）根据前一层隐层的状态X<sup>k</sup> 和原特征矩阵 X<sup>0</sup>，计算出一个中间结果 Z<sup>k+1</sup>，它是一个三维的张量，如下图所示：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-89974f1fc48762af.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>在这个中间结果上，我们用H<sup>k+1</sup> 个尺寸为 m*H<sup>k</sup> 的卷积核生成下一层隐层的状态，该过程如图2所示。这一操作与计算机视觉中最流行的卷积神经网络大体是一致的，唯一的区别在于卷积核的设计。CIN中一个神经元相关的接受域是垂直于特征维度D的整个平面，而CNN中的接受域是当前神经元周围的局部小范围区域，因此CIN中经过卷积操作得到的特征图（Feature Map）是一个向量，而不是一个矩阵。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-576be6de0a1dbeb5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>如果你觉得原文中的图不够清楚的话，希望下图可以帮助你理解整个过程：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-52d9f278c189208a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>CIN的宏观框架可以总结为下图：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-865e97a5b034c18a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>可以看出，它的特点是，最终学习出的特征交互的阶数是由网络的层数决定的，每一层隐层都通过一个池化操作连接到输出层，从而保证了输出单元可以见到不同阶数的特征交互模式。同时不难看出，CIN的结构与循环神经网络RNN是很类似的，即每一层的状态是由前一层隐层的值与一个额外的输入数据计算所得。不同的是，CIN中不同层的参数是不一样的，而在RNN中是相同的；RNN中每次额外的输入数据是不一样的，而CIN中额外的输入数据是固定的，始终是X<sup>0</sup>。</p>
<p>可以看到，CIN是通过（vector-wise）来学习特征之间的交互的，还有一个问题，就是它为什么是显式的进行学习？我们先从X<sup>1</sup> 来开始看，X<sup>1</sup> 的第h个神经元向量可以表示成：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-090532cfbf3593fc.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>进一步，X^2的第h个神经元向量可以表示成：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-f41cc8ab14a3d724.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>最后，第k层的第h个神经元向量可以表示成：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-173a66f80a7e4f4c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>因此，我们能够通过上面的式子对特征交互的形式进行一个很好的表示，它是显式的学习特征交叉。</p>
<h4 id="2-2-xDeepFM"><a href="#2-2-xDeepFM" class="headerlink" title="2.2 xDeepFM"></a>2.2 xDeepFM</h4><p>将CIN与线性回归单元、全连接神经网络单元组合在一起，得到最终的模型并命名为极深因子分解机xDeepFM，其结构如下图：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-99a5bcc517d40903.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>集成的CIN和DNN两个模块能够帮助模型同时以显式和隐式的方式学习高阶的特征交互，而集成的线性模块和深度神经模块也让模型兼具记忆与泛化的学习能力。值得一提的是，为了提高模型的通用性，xDeepFM中不同的模块共享相同的输入数据。而在具体的应用场景下，不同的模块也可以接入各自不同的输入数据，例如，线性模块中依旧可以接入很多根据先验知识提取的交叉特征来提高记忆能力，而在CIN或者DNN中，为了减少模型的计算复杂度，可以只导入一部分稀疏的特征子集。</p>
<h2 id="3、Tensorflow充电"><a href="#3、Tensorflow充电" class="headerlink" title="3、Tensorflow充电"></a>3、Tensorflow充电</h2><p>在介绍xDeepFM的代码之前，我们先来进行充电，学习几个tf的函数以及xDeepFM关键过程的实现。</p>
<p><strong>tf.split</strong><br>首先我们要实现第一步：<br><img src="https://upload-images.jianshu.io/upload_images/4155986-89974f1fc48762af.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>如何将两个二维的矩阵，相乘得到一个三维的矩阵？我们首先来看一下tf.split函数的原理:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">tf.split(</div><div class="line">    value,</div><div class="line">    num_or_size_splits,</div><div class="line">    axis=0,</div><div class="line">    num=None,</div><div class="line">    name=&apos;split&apos;</div><div class="line">)</div></pre></td></tr></table></figure>
<p>其中，value传入的就是需要切割的张量，axis是切割的维度，根据num_or_size_splits的不同形式，有两种切割方式： </p>
<ol>
<li>如果num_or_size_splits传入的是一个整数，这个整数代表这个张量最后会被切成几个小张量。此时，传入axis的数值就代表切割哪个维度（从0开始计数）。调用tf.split(my_tensor, 2，0)返回两个10 <em> 30 </em> 40的小张量。 </li>
<li>如果num_or_size_splits传入的是一个向量，那么向量有几个分量就分成几份，切割的维度还是由axis决定。比如调用tf.split(my_tensor, [10, 5, 25], 2)，则返回三个张量分别大小为 20 <em> 30 </em> 10、20 <em> 30 </em> 5、20 <em> 30 </em> 25。很显然，传入的这个向量各个分量加和必须等于axis所指示原张量维度的大小 (10 + 5 + 25 = 40)。</li>
</ol>
<p>好了，从实际需求出发，我们来体验一下，假设我们的batch为2，embedding的size是3，field数量为4。我们先来生成两个这样的tensor(假设X^k的field也是4 )：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">arr1 = tf.convert_to_tensor(np.arange(1,25).reshape(2,4,3),dtype=tf.int32)</div><div class="line">arr2 = tf.convert_to_tensor(np.arange(1,25).reshape(2,4,3),dtype=tf.int32)</div></pre></td></tr></table></figure>
<p>生成的矩阵如下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-573b9e5a2a17dc49.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>在经过CIN的第一步之后，我们目标的矩阵大小应该是2(batch) <em> 3(embedding Dimension) </em> 4(X^k的field数) <em> 4(X^0的field数)。如果只考虑batch中第一条数据的话，应该形成的是 1 </em> 3 <em> 4 </em> 4的矩阵。忽略第0维，想像成一个长宽为4，高为3的长方体，长方体横向切割，第一个横截面对应的数字应该如下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-f4815190d2e14ffc.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>那么想要做到这样的结果，我们首先按输入数据的axis=2进行split：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">split_arr1 = tf.split(arr1,[1,1,1],2)</div><div class="line">split_arr2 = tf.split(arr2,[1,1,1],2)</div><div class="line">print(split_arr1)</div><div class="line">print(sess.run(split_arr1))</div><div class="line">print(sess.run(split_arr2))</div></pre></td></tr></table></figure>
<p>分割后的结果如下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-7294179d54cd654b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>通过结果我们可以看到，我们现在对每一条数据，得到了3个4 <em> 1的tensor，可以理解为此时的tensor大小为 3(embedding Dimension) </em> 2(batch) <em> 4(X^k 或X^0的field数) </em> 1。</p>
<p>此时我们进行矩阵相乘：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">res = tf.matmul(split_arr1,split_arr2,transpose_b=True)</div></pre></td></tr></table></figure>
<p>这里我理解的，tensorflow对3维及以上矩阵相乘时，矩阵相乘只发生在最后两维。也就是说，3 <em> 2 </em> 4 <em> 1 和 3 </em> 2 <em> 1 </em> 4的矩阵相乘，最终的结果是3 <em> 2 </em> 4 * 4。我们来看看结果：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-4b557a605b0cc417.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>可以看到，不仅矩阵的形状跟我们预想的一样，同时结果也跟我们预想的一样。</p>
<p>最后，我们只需要进行transpose操作，把batch转换到第0维就可以啦。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">res = tf.transpose(res,perm=[1,0,2,3])</div></pre></td></tr></table></figure>
<p>这样，CIN中的第一步就大功告成了，明白了这一步如何用tensorflow实现，那么代码你也就能够顺其自然的看懂啦！</p>
<p>这一块完整的代码如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line">import tensorflow as tf</div><div class="line">import numpy as np</div><div class="line"></div><div class="line">arr1 = tf.convert_to_tensor(np.arange(1,25).reshape(2,4,3),dtype=tf.int32)</div><div class="line">arr2 = tf.convert_to_tensor(np.arange(1,25).reshape(2,4,3),dtype=tf.int32)</div><div class="line"></div><div class="line"></div><div class="line">with tf.Session() as sess:</div><div class="line">    sess.run(tf.global_variables_initializer())</div><div class="line">    split_arr1 = tf.split(arr1,[1,1,1],2)</div><div class="line">    split_arr2 = tf.split(arr2,[1,1,1],2)</div><div class="line">    print(split_arr1)</div><div class="line">    print(sess.run(split_arr1))</div><div class="line">    print(sess.run(split_arr2))</div><div class="line">    res = tf.matmul(split_arr1,split_arr2,transpose_b=True)</div><div class="line">    print(sess.run(res))</div><div class="line">    res = tf.transpose(res,perm=[1,0,2,3])</div><div class="line">    print(sess.run(res))</div></pre></td></tr></table></figure>
<h2 id="4、XDeepFM的TF实现"><a href="#4、XDeepFM的TF实现" class="headerlink" title="4、XDeepFM的TF实现"></a>4、XDeepFM的TF实现</h2><p>本文的代码来自github地址：<a href="https://github.com/Leavingseason/xDeepFM" target="_blank" rel="external">https://github.com/Leavingseason/xDeepFM</a><br>而我的github库中也偷偷把这里面的代码加进去啦：<a href="https://github.com/princewen/tensorflow_practice/tree/master/recommendation/Basic-XDeepFM-Demo" target="_blank" rel="external">https://github.com/princewen/tensorflow_practice/tree/master/recommendation/Basic-XDeepFM-Demo</a></p>
<p>真的是写的非常好的一段代码，希望大家可以比着自己敲一敲，相信你会有所收获。</p>
<p>具体的代码细节我们不展开进行讨论，我们只说一下数据的问题吧：<br>1、代码中的数据按照ffm的格式存储，格式如下：filed:n th dimension:value,即这个特征属于第几个field，在所有特征全部按one-hot展开后的第几维(而不是在这个field中是第几维)以及对应的特征值。<br>2、代码中使用到的数据属于多值的离散特征。</p>
<p>关于代码实现细节，我们这里只说一下CIN的实现：</p>
<p>由于X^0 在每一层都有用到，所以我们先对 X^0 进行一个处理：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">nn_input = tf.reshape(nn_input, shape=[-1, int(field_num), hparams.dim])</div><div class="line">split_tensor0 = tf.split(hidden_nn_layers[0], hparams.dim * [1], 2)</div></pre></td></tr></table></figure>
<p>在计算X^k 时，我们需要用到 X^k-1 的数据，代码中用hidden_nn_layers保存这些数据。对X^k-1 进行和X^0 同样的处理：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">split_tensor = tf.split(hidden_nn_layers[-1], hparams.dim * [1], 2)</div></pre></td></tr></table></figure>
<p>接下来就是我们之前讲过的，对两个split之后的tensor进行相乘再转置的过程啦：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">dot_result_m = tf.matmul(split_tensor0, split_tensor, transpose_b=True)</div><div class="line">dot_result_o = tf.reshape(dot_result_m, shape=[hparams.dim, -1, field_nums[0]*field_nums[-1]])</div><div class="line">dot_result = tf.transpose(dot_result_o, perm=[1, 0, 2])</div></pre></td></tr></table></figure>
<p>接下来，我们需要进行CIN的第二步，先回顾一下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-576be6de0a1dbeb5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>这里我们用1维卷积实现，假设X^K的field的数量我们起名为layer_size:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">filters = tf.get_variable(name=&quot;f_&quot;+str(idx),</div><div class="line">                     shape=[1, field_nums[-1]*field_nums[0], layer_size],</div><div class="line">                     dtype=tf.float32)</div><div class="line"></div><div class="line">curr_out = tf.nn.conv1d(dot_result, filters=filters, stride=1, padding=&apos;VALID&apos;)</div></pre></td></tr></table></figure>
<p>此时我们curr_out的大小就是 Batch <em> Embedding Size </em> Layer size，我们需要进行一下转置：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">curr_out = tf.transpose(curr_out, perm=[0, 2, 1])</div></pre></td></tr></table></figure>
<p>接下来就是最后一步，进行sumpooling，如下图：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-18bdea2ca62019ee.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>代码中有两种选择方式，direct方式和非direct方式，direct方式，直接把完整curr_out作为最后输出结果的一部分，同时把完整的curr_out作为计算下一个隐藏层向量的输入。非direct方式，把curr_out按照layer_size进行均分，前一半作为计算下一个隐藏层向量的输入，后一半作为最后输出结果的一部分。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line">if direct:</div><div class="line">    hparams.logger.info(&quot;all direct connect&quot;)</div><div class="line">    direct_connect = curr_out</div><div class="line">    next_hidden = curr_out</div><div class="line">    final_len += layer_size</div><div class="line">    field_nums.append(int(layer_size))</div><div class="line"></div><div class="line">else:</div><div class="line">    hparams.logger.info(&quot;split connect&quot;)</div><div class="line">    if idx != len(hparams.cross_layer_sizes) - 1:</div><div class="line">        next_hidden, direct_connect = tf.split(curr_out, 2 * [int(layer_size / 2)], 1)</div><div class="line">        final_len += int(layer_size / 2)</div><div class="line">    else:</div><div class="line">        direct_connect = curr_out</div><div class="line">        next_hidden = 0</div><div class="line">        final_len += layer_size</div><div class="line">    field_nums.append(int(layer_size / 2))</div><div class="line"></div><div class="line">final_result.append(direct_connect)</div><div class="line">hidden_nn_layers.append(next_hidden)</div></pre></td></tr></table></figure>
<p>最后 ，经过sum_pooling操作，再拼接一个输出层，我们就得到了CIN部分的输出：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">result = tf.concat(final_result, axis=1)</div><div class="line">result = tf.reduce_sum(result, -1)</div><div class="line"></div><div class="line">hparams.logger.info(&quot;no residual network&quot;)</div><div class="line">w_nn_output = tf.get_variable(name=&apos;w_nn_output&apos;,</div><div class="line">                              shape=[final_len, 1],</div><div class="line">                              dtype=tf.float32)</div><div class="line">b_nn_output = tf.get_variable(name=&apos;b_nn_output&apos;,</div><div class="line">                              shape=[1],</div><div class="line">                              dtype=tf.float32,</div><div class="line">                              initializer=tf.zeros_initializer())</div><div class="line">self.layer_params.append(w_nn_output)</div><div class="line">self.layer_params.append(b_nn_output)</div><div class="line">exFM_out = tf.nn.xw_plus_b(result, w_nn_output, b_nn_output)</div></pre></td></tr></table></figure>
<h2 id="5、总结"><a href="#5、总结" class="headerlink" title="5、总结"></a>5、总结</h2><p>我们今天介绍的xDeepFM模型，由linear、DNN、CIN三部分组成，其中CIN实现了自动学习显式的高阶特征交互，同时使得交互发生在向量级上。该模型在几个数据集上都取得了超过DeepFM模型的效果。</p>
<h2 id="参考文献-1"><a href="#参考文献-1" class="headerlink" title="参考文献"></a>参考文献</h2><p>1、论文：<a href="https://arxiv.org/abs/1803.05170" target="_blank" rel="external">https://arxiv.org/abs/1803.05170</a><br>2、特征交互：一种极深因子分解机模型（xDeepFM）：<a href="https://www.xianjichina.com/news/details_81731.html" target="_blank" rel="external">https://www.xianjichina.com/news/details_81731.html</a><br>3、<a href="https://blog.csdn.net/SangrealLilith/article/details/80272346" target="_blank" rel="external">https://blog.csdn.net/SangrealLilith/article/details/80272346</a><br>4、<a href="https://github.com/Leavingseason/xDeepFM" target="_blank" rel="external">https://github.com/Leavingseason/xDeepFM</a></p>
<h1 id="推荐系统遇上深度学习-二十三-–大一统信息检索模型IRGAN在推荐领域的应用"><a href="#推荐系统遇上深度学习-二十三-–大一统信息检索模型IRGAN在推荐领域的应用" class="headerlink" title="推荐系统遇上深度学习(二十三)–大一统信息检索模型IRGAN在推荐领域的应用"></a>推荐系统遇上深度学习(二十三)–大一统信息检索模型IRGAN在推荐领域的应用</h1><h2 id="1、引言-5"><a href="#1、引言-5" class="headerlink" title="1、引言"></a>1、引言</h2><p>信息检索领域的一个重要任务就是针对用户的一个请求query，返回一组排好序的召回列表。</p>
<p>经典的IR流派认为query和document之间存在着一种生成过程，即q -&gt; d 。举一个例子，搜索“哈登”，我们可以联想到“保罗”，“火箭”，“MVP”等等，每一个联想出来的document有一个生成概率p(d|q)，然后根据这个生成概率进行排序，这种模型被称作生成模型。人们在研究生成模型的时候，设计了一系列基于query和document的特征，比方说TF-IDF，BM25。这些特征能非常客观的描述query和document的相关性，但没有考虑document的质量，用户的反馈，pagerank等信息。</p>
<p>现代的IR流派则利用了机器学习，将query和document的特征放在一起，通过机器学习方法来计算query和document之间的匹配相关性: r=f(q,d)。举个现实的例子，我们知道“小白”更喜欢“吃鸡”而不是“王者荣耀”，pointwise会优化f(小白，吃鸡)=1，f(小白，王者荣耀)=0；pairwise会优化f(小白，吃鸡)&gt;f(小白，王者荣耀)；listwise会考虑很多其他游戏，一起进行优化。机器学习的判别模型能够很好地利用文本统计信息，用户点击信息等特征，但模型本身局限于标注数据的质量和大小，模型常常会在训练数据上过拟合，或陷入某一个局部最优解。</p>
<p>受到GAN的启发，将生成模型和判别模型结合在一起，学者们便提出了IRGAN模型。</p>
<h2 id="2、IRGAN介绍"><a href="#2、IRGAN介绍" class="headerlink" title="2、IRGAN介绍"></a>2、IRGAN介绍</h2><h4 id="定义问题"><a href="#定义问题" class="headerlink" title="定义问题"></a>定义问题</h4><p>假定我们又一些列的query{q1,…qN}并且有一系列的文档document结合{d1,…dM}，对于一个特定的query，我们有一系列标记的真实相关的文档，但是这个数量是远远小于文档总数量M的。query和document之间潜在的概率分布可以表示为条件概率分布p<sub>true</sub>(d|q,r)。给定一堆从真实条件分布p<sub>true</sub>(d|q,r)观察到的样本， 我们可以定义两种类型的IR model。</p>
<p><strong>生成式检索模型</strong>：该模型的目标是学习p<sub>θ</sub>(d|q,r)，使其更接近于p<sub>true</sub>(d|q,r)。</p>
<p><strong>判别式检索模型</strong>：该模型的目标是学习f<sub>Φ</sub>(q,d)，即尽量能够准确的判别q和d的相关程度</p>
<p>因此，受到GAN的启发，我们将上述的两种IR模型结合起来做一个最大最小化的博弈：生成式模型的任务是尽可能的产生和query相关的document，以此来混淆判别式模型；判别式模型的任务是尽可能准确区分真正相关的document和生成模型生成的document，因此，我们总体的目标就是：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-4abc289d2bddc4b4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>在上式中，生成式模型G为p<sub>θ</sub>(d|q<sub>n</sub>,r),生成式模型D对d是否与q相关进行判定，通过下面的式子给出相关性得分：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-451cca0e243381c5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<h4 id="优化判别模型D"><a href="#优化判别模型D" class="headerlink" title="优化判别模型D"></a>优化判别模型D</h4><p>判别器的主要目标是最大化我们的对数似然，即正确的区分真正相关的文档和生成器生成的文档。最优的参数通过下面的式子得到：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-5b76de4fa1a74cd5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<h4 id="优化生成模型G"><a href="#优化生成模型G" class="headerlink" title="优化生成模型G"></a>优化生成模型G</h4><p>生成器的主要目标是产生能够混淆判别器的document，判别器直接从给定的document池中选择document。在固定判别器参数f<sub>Φ</sub>(q,d)的情况下，生成器的学习目标是(第一项不包含θ,因此可以省略)：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-701056a21ce4ae32.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>我们把生成器的优化目标写作J<sup>G</sup>(q<sub>n</sub>)。</p>
<p>由于生成的document是离散的，无法直接通过梯度下降法进行优化，一种通常的做法是使用强化学习中的策略梯度方法，我们将q<sub>n</sub>作为state，p<sub>θ</sub>(d|q<sub>n</sub>,r)作为对应的策略，而log(1+exp(f<sub>Φ</sub>(d<sub></sub>，q<sub>n</sub>))作为对应的reward：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-a348ceb8cc4e48c5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>其中，第二步到第三步的变换利用了log函数求导的性质，而在最后一步则基于采样的document做了一个近似。</p>
<h4 id="总体流程"><a href="#总体流程" class="headerlink" title="总体流程"></a>总体流程</h4><p>IRGAN的整体训练流程如下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-e85258529eeb80c7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<h4 id="Pair-wise的情况"><a href="#Pair-wise的情况" class="headerlink" title="Pair-wise的情况"></a>Pair-wise的情况</h4><p>在很多IR问题中，我们的数据是对一个query的一系列排序文档对，因为相比判断一个文档的相关性，更容易判断用户对一对文档的相对偏好（比如说通过点击数据，如果两篇document同时展示给用户，用户点击了a而没有点击b，则可以说明用户对a的偏好大于对b的偏好），此外，如果我们使用相关性进行分级(用来表明不同文档对同一个query的匹配程度)而不是使用是否相关，训练数据也可以自然的表示成有序的文档对。</p>
<p>IRGAN在pairwise情况下是同样适用的，假设我有一堆带标记的document组合R<sub>n</sub> = {<d<sub>i，d<sub>j</sub>&gt;|d<sub>i</sub> &gt; d<sub>j</sub>}。生成器G的任务是尽量生成正确的排序组合来混淆判别器D，判别器D的任务是尽可能区分真正的排序组合和生成器生成的排序组合。基于下面的式子来进行最大最小化博弈：</d<sub></p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-de30a9f502d39855.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>其中，o=<d<sub>u,d<sub>v</sub>&gt;,o’=<d'<sub>u,d'<sub>v</sub>&gt;分别代表正确的组合和生成器生成的组合。而D(d<sub>u</sub>,d<sub>v</sub>|q)计算公式如下：</d'<sub></d<sub></p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-02427e0f63729eae.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>接下来我们就来讲一下生成器的生成策略。首先我们选择一个正确的组合 <d<sub>i,d<sub>j</sub>&gt;，我们首先选取d<sub>j</sub>，然后根据当前的生成器G的策略p<sub>θ</sub>(d|q,r)，选择比d<sub>j</sub>生成概率大的d<sub>k</sub>，组成一组<d<sub>k,d<sub>j</sub>&gt;。</d<sub></d<sub></p>
<p>有关更多的IRGAN的细节，大家可以阅读原论文，接下来，我们来看一个简单的Demo吧。</p>
<h2 id="3、IRGAN的TF实现"><a href="#3、IRGAN的TF实现" class="headerlink" title="3、IRGAN的TF实现"></a>3、IRGAN的TF实现</h2><p>本文的github代码参考：<br><a href="https://github.com/geek-ai/irgan/tree/master/item_recommendation" target="_blank" rel="external">https://github.com/geek-ai/irgan/tree/master/item_recommendation</a></p>
<p>源代码是python2.7版本的，修改为python3版本的代码之后存放地址为：<br><a href="https://github.com/princewen/tensorflow_practice/tree/master/recommendation/Basic-IRGAN-Demo" target="_blank" rel="external">https://github.com/princewen/tensorflow_practice/tree/master/recommendation/Basic-IRGAN-Demo</a></p>
<h4 id="数据"><a href="#数据" class="headerlink" title="数据"></a>数据</h4><p>先来说说数据吧，数据用的是ml-100k的数据，每一行的格式为“uid iid score”，我们把评分大于等于4分的电影作为用户真正感兴趣的电影。</p>
<p>###Generator<br>对于训练Generator，我们需要输入的有三部分：uid，iid以及reward，我们首先定义user和item的embedding，然后获取uid和iid的item。同时，我们这里还给每个item定义了一个特征值：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">self.user_embeddings = tf.Variable(tf.random_uniform([self.userNum,self.emb_dim],</div><div class="line">                                                     minval=-initdelta,maxval=self.initdelta,</div><div class="line">                                                     dtype =tf.float32))</div><div class="line">self.item_embeddings = tf.Variable(tf.random_uniform([self.itemNum,self.emb_dim],</div><div class="line">                                                     minval=-initdelta,maxval=self.initdelta,</div><div class="line">                                                     dtype=tf.float32))</div><div class="line">self.item_bias = tf.Variable(tf.zeros([self.itemNum]))</div><div class="line"></div><div class="line">self.u = tf.placeholder(tf.int32)</div><div class="line">self.i = tf.placeholder(tf.int32)</div><div class="line">self.reward = tf.placeholder(tf.float32)</div><div class="line"></div><div class="line">self.u_embedding = tf.nn.embedding_lookup(self.user_embeddings,self.u)</div><div class="line">self.i_embedding = tf.nn.embedding_lookup(self.item_embeddings,self.i)</div><div class="line">self.i_bias = tf.gather(self.item_bias,self.i)</div></pre></td></tr></table></figure>
<p>接下来，我们需要计算传入的user和item之间的相关性，并通过传入的reward来更新我们的策略：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">self.all_logits = tf.reduce_sum(tf.multiply(self.u_embedding,self.item_embeddings),1) + self.item_bias</div><div class="line">self.i_prob = tf.gather(</div><div class="line">    tf.reshape(tf.nn.softmax(tf.reshape(self.all_logits, [1, -1])), [-1]),</div><div class="line">    self.i)</div><div class="line"></div><div class="line">self.gan_loss = -tf.reduce_mean(tf.log(self.i_prob) * self.reward) + self.lamda * (</div><div class="line">    tf.nn.l2_loss(self.u_embedding) + tf.nn.l2_loss(self.i_embedding) + tf.nn.l2_loss(self.i_bias)</div><div class="line">)</div><div class="line"></div><div class="line">g_opt = tf.train.GradientDescentOptimizer(self.learning_rate)</div><div class="line">self.gan_updates = g_opt.minimize(self.gan_loss,var_list=self.g_params)</div></pre></td></tr></table></figure>
<h4 id="Discriminator"><a href="#Discriminator" class="headerlink" title="Discriminator"></a>Discriminator</h4><p>传入D的同样有三部分，分别是uid，iid以及label值，与G一样，我们也首先得到embedding值：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">self.user_embeddings = tf.Variable(tf.random_uniform([self.userNum,self.emb_dim],</div><div class="line">                                                    minval=-self.initdelta,maxval=self.initdelta,</div><div class="line">                                                    dtype=tf.float32))</div><div class="line">self.item_embeddings = tf.Variable(tf.random_uniform([self.itemNum,self.emb_dim],</div><div class="line">                                                    minval=-self.initdelta,maxval=self.initdelta,</div><div class="line">                                                    dtype=tf.float32))</div><div class="line">self.item_bias = tf.Variable(tf.zeros(self.itemNum))</div><div class="line"></div><div class="line">self.u = tf.placeholder(tf.int32)</div><div class="line">self.i = tf.placeholder(tf.int32)</div><div class="line">self.label = tf.placeholder(tf.float32)</div><div class="line"></div><div class="line">self.u_embedding = tf.nn.embedding_lookup(self.user_embeddings,self.u)</div><div class="line">self.i_embedding = tf.nn.embedding_lookup(self.item_embeddings,self.i)</div><div class="line">self.i_bias = tf.gather(self.item_bias,self.i)</div></pre></td></tr></table></figure>
<p>随后，我们通过对数损失函数来更新D：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">self.pre_logits = tf.reduce_sum(tf.multiply(self.u_embedding, self.i_embedding), 1) + self.i_bias</div><div class="line">self.pre_loss = tf.nn.sigmoid_cross_entropy_with_logits(labels = self.label,</div><div class="line">                                                        logits = self.pre_logits) + self.lamda * (</div><div class="line">    tf.nn.l2_loss(self.u_embedding) + tf.nn.l2_loss(self.i_embedding) + tf.nn.l2_loss(self.i_bias)</div><div class="line">)</div><div class="line"></div><div class="line">d_opt = tf.train.GradientDescentOptimizer(self.learning_rate)</div><div class="line">self.d_updates = d_opt.minimize(self.pre_loss,var_list=self.d_params)</div></pre></td></tr></table></figure>
<p>D中还有很重要的一步就是，计算reward：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">self.reward_logits = tf.reduce_sum(tf.multiply(self.u_embedding,self.i_embedding),1) + self.i_bias</div><div class="line">self.reward = 2 * (tf.sigmoid(self.reward_logits) - 0.5)</div></pre></td></tr></table></figure>
<h4 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h4><p>我们的G和D是交叉训练的，D的训练过程如下，每隔5轮，我们就要调用generate_for_d函数产生一批新的训练样本。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">for d_epoch in range(100):</div><div class="line">    if d_epoch % 5 == 0:</div><div class="line">        generate_for_d(sess,generator,DIS_TRAIN_FILE)</div><div class="line">        train_size = ut.file_len(DIS_TRAIN_FILE)</div><div class="line">    index = 1</div><div class="line">    while True:</div><div class="line">        if index &gt; train_size:</div><div class="line">            break</div><div class="line">        if index + BATCH_SIZE &lt;= train_size + 1:</div><div class="line">            input_user,input_item,input_label = ut.get_batch_data(DIS_TRAIN_FILE,index,BATCH_SIZE)</div><div class="line">        else:</div><div class="line">            input_user,input_item,input_label = ut.get_batch_data(DIS_TRAIN_FILE,index,train_size-index+1)</div><div class="line">        index += BATCH_SIZE</div><div class="line"></div><div class="line">        _ = sess.run(discriminator.d_updates,feed_dict=&#123;</div><div class="line">            discriminator.u:input_user,discriminator.i:input_item,discriminator.label:input_label</div><div class="line">        &#125;)</div></pre></td></tr></table></figure>
<p>generate_for_d函数形式如下，其根据G的策略，生成一批样本。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">def generate_for_d(sess,model,filename):</div><div class="line">    data = []</div><div class="line">    for u in user_pos_train:</div><div class="line">        pos = user_pos_train[u]</div><div class="line"></div><div class="line">        rating = sess.run(model.all_rating,&#123;model.u:[u]&#125;)</div><div class="line">        rating = np.array(rating[0]) / 0.2</div><div class="line">        exp_rating = np.exp(rating)</div><div class="line">        prob = exp_rating / np.sum(exp_rating)</div><div class="line"></div><div class="line">        neg = np.random.choice(np.arange(ITEM_NUM),size=len(pos),p=prob)</div><div class="line">        # 1:1 的正负样本</div><div class="line">        for i in range(len(pos)):</div><div class="line">            data.append(str(u) + &apos;\t&apos; + str(pos[i]) + &apos;\t&apos; + str(neg[i]))</div><div class="line"></div><div class="line">    with open(filename,&apos;w&apos;) as fout:</div><div class="line">        fout.write(&apos;\n&apos;.join(data))</div></pre></td></tr></table></figure>
<p>G的训练过程首先要通过D得到对应的reward，随后更新自己的策略：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line">for g_epoch in range(50):</div><div class="line">    for u in user_pos_train:</div><div class="line">        sample_lambda = 0.2</div><div class="line">        pos = user_pos_train[u]</div><div class="line"></div><div class="line">        rating = sess.run(generator.all_logits,&#123;generator.u:u&#125;)</div><div class="line">        exp_rating = np.exp(rating)</div><div class="line">        prob = exp_rating / np.sum(exp_rating)</div><div class="line"></div><div class="line">        pn = (1-sample_lambda) * prob</div><div class="line">        pn[pos] += sample_lambda * 1.0 / len(pos)</div><div class="line"></div><div class="line">        sample = np.random.choice(np.arange(ITEM_NUM), 2 * len(pos), p=pn)</div><div class="line"></div><div class="line">        reward = sess.run(discriminator.reward, &#123;discriminator.u: u, discriminator.i: sample&#125;)</div><div class="line">        reward = reward * prob[sample] / pn[sample]</div><div class="line"></div><div class="line">        _ = sess.run(generator.gan_updates,</div><div class="line">                     &#123;generator.u: u, generator.i: sample, generator.reward: reward&#125;)</div></pre></td></tr></table></figure>
<h2 id="参考文献："><a href="#参考文献：" class="headerlink" title="参考文献："></a>参考文献：</h2><p>1、论文地址：<a href="https://arxiv.org/abs/1705.10513" target="_blank" rel="external">https://arxiv.org/abs/1705.10513</a><br>2、<a href="https://github.com/geek-ai/irgan/tree/master/item_recommendation" target="_blank" rel="external">https://github.com/geek-ai/irgan/tree/master/item_recommendation</a></p>
<h1 id="推荐系统遇上深度学习-二十四-–深度兴趣进化网络DIEN原理及实战！"><a href="#推荐系统遇上深度学习-二十四-–深度兴趣进化网络DIEN原理及实战！" class="headerlink" title="推荐系统遇上深度学习(二十四)–深度兴趣进化网络DIEN原理及实战！"></a>推荐系统遇上深度学习(二十四)–深度兴趣进化网络DIEN原理及实战！</h1><p>在本系列的第十八篇(<a href="https://www.jianshu.com/p/73b6f5d00f46)中，我们介绍了阿里的深度兴趣网络(Deep" target="_blank" rel="external">https://www.jianshu.com/p/73b6f5d00f46)中，我们介绍了阿里的深度兴趣网络(Deep</a> Interest Network，以下简称DIN)，时隔一年，阿里再次升级其模型，提出了深度兴趣进化网络(Deep Interest Evolution Network,以下简称DIEN，论文地址：<a href="https://arxiv.org/pdf/1809.03672.pdf)，并将其应用于淘宝的广告系统中，获得了20.7%的CTR的提升。本篇，我们一同来探秘DIEN的原理及实现。" target="_blank" rel="external">https://arxiv.org/pdf/1809.03672.pdf)，并将其应用于淘宝的广告系统中，获得了20.7%的CTR的提升。本篇，我们一同来探秘DIEN的原理及实现。</a></p>
<h2 id="1、背景-5"><a href="#1、背景-5" class="headerlink" title="1、背景"></a>1、背景</h2><p>在大多数非搜索电商场景下，用户并不会实时表达目前的兴趣偏好。因此通过设计模型来捕获用户的动态变化的兴趣，是提升CTR预估效果的关键。阿里之前的DIN模型将用户的历史行为来表示用户的兴趣，并强调了用户兴趣的多样性和动态变化性，因此通过attention-based model来捕获和目标物品相关的兴趣。虽然DIN模型将用户的历史行为来表示兴趣，但存在两个缺点：<br>1）用户的兴趣是不断进化的，而DIN抽取的用户兴趣之间是独立无关联的，没有捕获到兴趣的动态进化性<br>2）通过用户的显式的行为来表达用户隐含的兴趣，这一准确性无法得到保证。</p>
<p>基于以上两点，阿里提出了深度兴趣演化网络DIEN来CTR预估的性能。DIEN模型的主要贡献点在于：<br>1）模型关注电商系统中兴趣演化的过程，并提出了新的网络结果来建模兴趣进化的过程，这个模型能够更精确的表达用户兴趣，同时带来更高的CTR预估准确率。<br>2）设计了兴趣抽取层，并通过计算一个辅助loss，来提升兴趣表达的准确性。<br>3）设计了兴趣进化层，来更加准确的表达用户兴趣的动态变化性。</p>
<p>接下来，我们来一起看一下DIEN模型的原理。</p>
<h2 id="2、DIEN模型原理"><a href="#2、DIEN模型原理" class="headerlink" title="2、DIEN模型原理"></a>2、DIEN模型原理</h2><h4 id="2-1-模型总体结构"><a href="#2-1-模型总体结构" class="headerlink" title="2.1 模型总体结构"></a>2.1 模型总体结构</h4><p>我们先来对比一下DIN和DIEN的结构。<br>DIN的模型结构如下：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/4155986-14586a6e6389134b?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="DIN"></p>
<p>DIEN的模型结构如下：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/4155986-df004503462d5103?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="DIEN"></p>
<p>可以看到，DIN和DIEN的最底层都是Embedding Layer，User profile， target AD和context feature的处理方式是一致的。不同的是，DIEN将user behavior组织成了序列数据的形式，并把简单的使用外积完成的activation unit变成了一个attention-based GRU网络。</p>
<h4 id="2-2-兴趣抽取层Interest-Extractor-Layer"><a href="#2-2-兴趣抽取层Interest-Extractor-Layer" class="headerlink" title="2.2 兴趣抽取层Interest Extractor Layer"></a>2.2 兴趣抽取层Interest Extractor Layer</h4><p>兴趣抽取层Interest Extractor Layer的主要目标是从embedding数据中提取出interest。但一个用户在某一时间的interest不仅与当前的behavior有关，也与之前的behavior相关，所以作者们使用GRU单元来提取interest。GRU单元的表达式如下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-9027fa8dc972d164.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="GRU表达式"></p>
<p>这里我们可以认为h<sub>t</sub>是提取出的用户兴趣，但是这个地方兴趣是否表示的合理呢？文中别出心裁的增加了一个辅助loss，来提升兴趣表达的准确性：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-bbb5a385306857b6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>这里，作者设计了一个二分类模型来计算兴趣抽取的准确性，我们将用户下一时刻真实的行为e(t+1)作为正例，负采样得到的行为作为负例e(t+1)’，分别与抽取出的兴趣h(t)结合输入到设计的辅助网络中，得到预测结果，并通过logloss计算一个辅助的损失：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-d3dd959e6239e3fa.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<h4 id="2-3-兴趣进化层Interest-Evolution-Layer"><a href="#2-3-兴趣进化层Interest-Evolution-Layer" class="headerlink" title="2.3 兴趣进化层Interest Evolution Layer"></a>2.3 兴趣进化层Interest Evolution Layer</h4><p>兴趣进化层Interest Evolution Layer的主要目标是刻画用户兴趣的进化过程。举个简单的例子：</p>
<p>以用户对衣服的interest为例，随着季节和时尚风潮的不断变化，用户的interest也会不断变化。这种变化会直接影响用户的点击决策。建模用户兴趣的进化过程有两方面的好处：<br>1）追踪用户的interest可以使我们学习final interest的表达时包含更多的历史信息。<br>2）可以根据interest的变化趋势更好地进行CTR预测。</p>
<p>而interest在变化过程中遵循如下规律：<br>1）<strong>interest drift</strong>：用户在某一段时间的interest会有一定的集中性。比如用户可能在一段时间内不断买书，在另一段时间内不断买衣服。<br>2）<strong>interest individual</strong>：一种interest有自己的发展趋势，不同种类的interest之间很少相互影响，例如买书和买衣服的interest基本互不相关。</p>
<p>为了利用这两个时序特征，我们需要再增加一层GRU的变种，并加上attention机制以找到与target AD相关的interest。</p>
<p>attention的计算方式如下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-b2316abcc1ebc474.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>而Attention和GRU结合起来的机制有很多，文中介绍了一下三种：</p>
<p><strong>GRU with attentional input (AIGRU)</strong><br>这种方式将attention直接作用于输入，无需修改GRU的结构：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-6f5c535a91e4261d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p><strong>Attention based GRU(AGRU)</strong><br>这种方式需要修改GRU的结构，此时hidden state的输出变为：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-a626f3e0480c45e9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p><strong>GRU with attentional update gate (AUGRU)</strong><br>这种方式需要修改GRU的结构，此时hidden state的输出变为:</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-970cccbe7a019632.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<h4 id="2-4-模型试验"><a href="#2-4-模型试验" class="headerlink" title="2.4 模型试验"></a>2.4 模型试验</h4><p>文章在公共数据和自己的数据集上都做了实验，并选取了不同的对比模型：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-2c96d542ec3fba69.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>离线实验的结果如下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-e7a48a07f72a4f99.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-fc6a2c7fe6282758.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-753463da29dba12f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>DIEN使用了辅助loss和AUGRU结构，而BaseModel + GRU + AUGRU与DIEN的不同之处就是没有增加辅助loss。可以看到，DIEN的实验效果远好于其他模型。</p>
<h2 id="3、DIEN模型实现"><a href="#3、DIEN模型实现" class="headerlink" title="3、DIEN模型实现"></a>3、DIEN模型实现</h2><p>本文模型的实现参考代码是：<a href="https://github.com/mouna99/dien" target="_blank" rel="external">https://github.com/mouna99/dien</a><br>本文代码的地址为：<a href="https://github.com/princewen/tensorflow_practice/tree/master/recommendation/Basic-DIEN-Demo" target="_blank" rel="external">https://github.com/princewen/tensorflow_practice/tree/master/recommendation/Basic-DIEN-Demo</a><br>本文数据的地址为：<a href="https://github.com/mouna99/dien" target="_blank" rel="external">https://github.com/mouna99/dien</a></p>
<p>###3.1 数据介绍<br>根据github中提供的数据，解压后的文件如下：<br><strong>uid_voc.pkl</strong>: 用户名对应的id<br><strong>mid_voc.pkl</strong>: item对应的id<br><strong>cat_voc.pkl</strong>:category对应的id<br><strong>item-info</strong>:item对应的category信息<br><strong>reviews-info</strong>：用于进行负采样的数据<br><strong>local_train_splitByUser</strong>:训练数据，一行格式为：label、用户名、目标item、 目标item类别、历史item、历史item对应类别。<br><strong>local_test_splitByUser</strong>:测试数据，格式同训练数据</p>
<p>###3.2 代码实现<br>本文的代码主要包含以下几个文件：<br><strong>rnn.py</strong>：对tensorflow中原始的rnn进行修改，目的是将attention同rnn进行结合。<br><strong>vecAttGruCell.py</strong>: 对GRU源码进行修改，将attention加入其中，设计AUGRU结构<br><strong>data_iterator.py</strong>:数据迭代器，用于数据的不断输入<br><strong>utils.py</strong>:一些辅助函数，如dice激活函数、attention score计算等<br><strong>model.py</strong>:DIEN模型文件<br><strong>train.py</strong>:模型的入口，用于训练数据、保存模型和测试数据</p>
<p>好了，接下来我们介绍一些关键的代码：</p>
<h4 id="输入数据介绍"><a href="#输入数据介绍" class="headerlink" title="输入数据介绍"></a>输入数据介绍</h4><p>输入的数据有用户id、target的item id、target item对应的cateid、用户历史行为的item id list、用户历史行为item对应的cate id list、历史行为的长度、历史行为的mask、目标值、负采样的数据。</p>
<p>对于每一个用户的历史行为，代码中选取了5个样本作为负样本。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">self.mid_his_batch_ph = tf.placeholder(tf.int32,[None,None],name=&apos;mid_his_batch_ph&apos;)</div><div class="line">self.cat_his_batch_ph = tf.placeholder(tf.int32,[None,None],name=&apos;cat_his_batch_ph&apos;)</div><div class="line">self.uid_batch_ph = tf.placeholder(tf.int32,[None,],name=&apos;uid_batch_ph&apos;)</div><div class="line">self.mid_batch_ph = tf.placeholder(tf.int32,[None,],name=&apos;mid_batch_ph&apos;)</div><div class="line">self.cat_batch_ph = tf.placeholder(tf.int32,[None,],name=&apos;cat_batch_ph&apos;)</div><div class="line">self.mask = tf.placeholder(tf.float32,[None,None],name=&apos;mask&apos;)</div><div class="line">self.seq_len_ph = tf.placeholder(tf.int32,[None],name=&apos;seq_len_ph&apos;)</div><div class="line">self.target_ph = tf.placeholder(tf.float32,[None,None],name=&apos;target_ph&apos;)</div><div class="line">self.lr = tf.placeholder(tf.float64,[])</div><div class="line">self.use_negsampling = use_negsampling</div><div class="line">if use_negsampling:</div><div class="line">    self.noclk_mid_batch_ph = tf.placeholder(tf.int32, [None, None, None], name=&apos;noclk_mid_batch_ph&apos;)</div><div class="line">    self.noclk_cat_batch_ph = tf.placeholder(tf.int32, [None, None, None], name=&apos;noclk_cat_batch_ph&apos;)</div></pre></td></tr></table></figure>
<h4 id="输入数据转换为对应的embedding"><a href="#输入数据转换为对应的embedding" class="headerlink" title="输入数据转换为对应的embedding"></a>输入数据转换为对应的embedding</h4><p>接下来，输入数据将转换为对应的embedding：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line">with tf.name_scope(&quot;Embedding_layer&quot;):</div><div class="line">    self.uid_embeddings_var = tf.get_variable(&quot;uid_embedding_var&quot;,[n_uid,EMBEDDING_DIM])</div><div class="line">    tf.summary.histogram(&apos;uid_embeddings_var&apos;, self.uid_embeddings_var)</div><div class="line">    self.uid_batch_embedded = tf.nn.embedding_lookup(self.uid_embeddings_var,self.uid_batch_ph)</div><div class="line"></div><div class="line">    self.mid_embeddings_var = tf.get_variable(&quot;mid_embedding_var&quot;,[n_mid,EMBEDDING_DIM])</div><div class="line">    tf.summary.histogram(&apos;mid_embeddings_var&apos;,self.mid_embeddings_var)</div><div class="line">    self.mid_batch_embedded = tf.nn.embedding_lookup(self.mid_embeddings_var,self.mid_batch_ph)</div><div class="line">    self.mid_his_batch_embedded = tf.nn.embedding_lookup(self.mid_embeddings_var,self.mid_his_batch_ph)</div><div class="line">    if self.use_negsampling:</div><div class="line">        self.noclk_mid_his_batch_embedded = tf.nn.embedding_lookup(self.mid_embeddings_var,</div><div class="line">                                                                   self.noclk_mid_batch_ph)</div><div class="line"></div><div class="line">    self.cat_embeddings_var = tf.get_variable(&quot;cat_embedding_var&quot;, [n_cat, EMBEDDING_DIM])</div><div class="line">    tf.summary.histogram(&apos;cat_embeddings_var&apos;, self.cat_embeddings_var)</div><div class="line">    self.cat_batch_embedded = tf.nn.embedding_lookup(self.cat_embeddings_var, self.cat_batch_ph)</div><div class="line">    self.cat_his_batch_embedded = tf.nn.embedding_lookup(self.cat_embeddings_var, self.cat_his_batch_ph)</div><div class="line">    if self.use_negsampling:</div><div class="line">        self.noclk_cat_his_batch_embedded = tf.nn.embedding_lookup(self.cat_embeddings_var,</div><div class="line">                                                                   self.noclk_cat_batch_ph)</div></pre></td></tr></table></figure>
<p>接下来，将item的id对应的embedding 以及 item对应的cateid的embedding进行拼接，共同作为item的embedding.：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">self.item_eb = tf.concat([self.mid_batch_embedded,self.cat_batch_embedded],1)</div><div class="line">self.item_his_eb = tf.concat([self.mid_his_batch_embedded,self.cat_his_batch_embedded],2)</div><div class="line"></div><div class="line">if self.use_negsampling:</div><div class="line">    self.noclk_item_his_eb = tf.concat(</div><div class="line">        [self.noclk_mid_his_batch_embedded[:, :, 0, :], self.noclk_cat_his_batch_embedded[:, :, 0, :]], -1)</div><div class="line">    self.noclk_item_his_eb = tf.reshape(self.noclk_item_his_eb,</div><div class="line">                                        [-1, tf.shape(self.noclk_mid_his_batch_embedded)[1], EMBEDDING_DIM * 2]) # 负采样的item选第一个</div><div class="line"></div><div class="line">    self.noclk_his_eb = tf.concat([self.noclk_mid_his_batch_embedded, self.noclk_cat_his_batch_embedded], -1)</div></pre></td></tr></table></figure>
<h4 id="第一层GRU"><a href="#第一层GRU" class="headerlink" title="第一层GRU"></a>第一层GRU</h4><p>接下来，我们要将用户行为历史的item embedding输入到dynamic rnn中，同时计算辅助loss：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">with tf.name_scope(&apos;rnn_1&apos;):</div><div class="line">    rnn_outputs,_ = dynamic_rnn(GRUCell(HIDDEN_SIZE),inputs = self.item_his_eb,sequence_length=self.seq_len_ph,dtype=tf.float32,scope=&apos;gru1&apos;)</div><div class="line">    tf.summary.histogram(&quot;GRU_outputs&quot;,rnn_outputs)</div><div class="line"></div><div class="line">aux_loss_1 = self.auxiliary_loss(rnn_outputs[:,:-1,:],self.item_his_eb[:,1:,:],self.noclk_item_his_eb[:,1:,:],self.mask[:,1:],stag=&quot;gru&quot;)</div><div class="line">self.aux_loss = aux_loss_1</div></pre></td></tr></table></figure>
<p>辅助loss的计算其实是一个二分类模型，代码如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line">def auxiliary_loss(self,h_states,click_seq,noclick_seq,mask,stag=None):</div><div class="line">    mask = tf.cast(mask,tf.float32)</div><div class="line">    click_input = tf.concat([h_states,click_seq],-1)</div><div class="line">    noclick_input = tf.concat([h_states,noclick_seq],-1)</div><div class="line">    click_prop_ = self.auxiliary_net(click_input,stag=stag)[:,:,0]</div><div class="line">    noclick_prop_ = self.auxiliary_net(noclick_input,stag=stag)[:,:,0]</div><div class="line">    click_loss_ = -tf.reshape(tf.log(click_prop_),[-1,tf.shape(click_seq)[1]]) * mask</div><div class="line">    noclick_loss_ = - tf.reshape(tf.log(1.0 - noclick_prop_), [-1, tf.shape(noclick_seq)[1]]) * mask</div><div class="line">    loss_ = tf.reduce_mean(click_loss_ + noclick_loss_)</div><div class="line">    return loss_</div><div class="line"></div><div class="line">def auxiliary_net(self,input,stag=&apos;auxiliary_net&apos;):</div><div class="line">    bn1 = tf.layers.batch_normalization(inputs=input, name=&apos;bn1&apos; + stag, reuse=tf.AUTO_REUSE)</div><div class="line">    dnn1 = tf.layers.dense(bn1, 100, activation=None, name=&apos;f1&apos; + stag, reuse=tf.AUTO_REUSE)</div><div class="line">    dnn1 = tf.nn.sigmoid(dnn1)</div><div class="line">    dnn2 = tf.layers.dense(dnn1, 50, activation=None, name=&apos;f2&apos; + stag, reuse=tf.AUTO_REUSE)</div><div class="line">    dnn2 = tf.nn.sigmoid(dnn2)</div><div class="line">    dnn3 = tf.layers.dense(dnn2, 2, activation=None, name=&apos;f3&apos; + stag, reuse=tf.AUTO_REUSE)</div><div class="line">    y_hat = tf.nn.softmax(dnn3) + 0.00000001</div><div class="line">    return y_hat</div></pre></td></tr></table></figure>
<h4 id="AUGRU"><a href="#AUGRU" class="headerlink" title="AUGRU"></a>AUGRU</h4><p>我们首先需要计算attention的score，然后将其作为GRU的一部分输入：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">with tf.name_scope(&apos;Attention_layer_1&apos;):</div><div class="line">    att_outputs,alphas = din_fcn_attention(self.item_eb,rnn_outputs,ATTENTION_SIZE,self.mask,</div><div class="line">                                           softmax_stag=1,stag=&apos;1_1&apos;,mode=&apos;LIST&apos;,return_alphas=True)</div><div class="line"></div><div class="line">    tf.summary.histogram(&apos;alpha_outputs&apos;,alphas)</div></pre></td></tr></table></figure>
<p>接下来，就是AUGRU的结构，这里我们需要设计一个新的VecAttGRUCell结构，相比于GRUCell，修改的地方如下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-5f38e8cd9445f703.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>上图中左侧是GRU的源码，右侧是VecAttGRUCell的代码，我们主要修改了call函数中的代码，在GRU中，hidden state的计算为：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">new_h = u * state + (1 - u) * c</div></pre></td></tr></table></figure>
<p>AUGRU中，hidden state的计算为：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">u = (1.0 - att_score) * u</div><div class="line">new_h = u * state + (1 - u) * c</div></pre></td></tr></table></figure>
<p>代码中给出的hidden state计算可能与文中有些出入，不过核心的思想都是，对于attention score大的，保存的当前的c就多一些。</p>
<p>设计好了新的GRU Cell，我们就能计算兴趣的进化过程：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">with tf.name_scope(&apos;rnn_2&apos;):</div><div class="line">    rnn_outputs2,final_state2 = dynamic_rnn(VecAttGRUCell(HIDDEN_SIZE),inputs=rnn_outputs,</div><div class="line">                                            att_scores=tf.expand_dims(alphas,-1),</div><div class="line">                                            sequence_length = self.seq_len_ph,dtype=tf.float32,</div><div class="line">                                            scope=&quot;gru2&quot;</div><div class="line">                                            )</div><div class="line">    tf.summary.histogram(&quot;GRU2_Final_State&quot;,final_state2)</div></pre></td></tr></table></figure>
<p>得到兴趣进化的结果final_state2之后，需要与其他的embedding进行拼接，得到全联接层的输入：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">inp = tf.concat([self.uid_batch_embedded,self.item_eb,self.item_his_eb_sum,self.item_eb * self.item_his_eb_sum,final_state2],1)</div></pre></td></tr></table></figure>
<p>####全联接层得到最终输出<br>最后我们通过一个多层神经网络，得到最终的ctr预估值：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div></pre></td><td class="code"><pre><div class="line">def build_fcn_net(self,inp,use_dice=False):</div><div class="line">    bn1 = tf.layers.batch_normalization(inputs=inp,name=&apos;bn1&apos;)</div><div class="line">    dnn1 = tf.layers.dense(bn1,200,activation=None,name=&apos;f1&apos;)</div><div class="line"></div><div class="line">    if use_dice:</div><div class="line">        dnn1 = dice(dnn1,name=&apos;dice_1&apos;)</div><div class="line">    else:</div><div class="line">        dnn1 = prelu(dnn1,&apos;prelu1&apos;)</div><div class="line"></div><div class="line">    dnn2 = tf.layers.dense(dnn1,80,activation=None,name=&apos;f2&apos;)</div><div class="line">    if use_dice:</div><div class="line">        dnn2 = dice(dnn2,name=&apos;dice_2&apos;)</div><div class="line">    else:</div><div class="line">        dnn2 = prelu(dnn2,name=&apos;prelu2&apos;)</div><div class="line"></div><div class="line">    dnn3 = tf.layers.dense(dnn2,2,activation=None,name=&apos;f3&apos;)</div><div class="line">    self.y_hat = tf.nn.softmax(dnn3) + 0.00000001</div><div class="line"></div><div class="line">    with tf.name_scope(&apos;Metrics&apos;):</div><div class="line">        ctr_loss = -tf.reduce_mean(tf.log(self.y_hat) * self.target_ph)</div><div class="line">        self.loss = ctr_loss</div><div class="line">        if self.use_negsampling:</div><div class="line">            self.loss += self.aux_loss</div><div class="line">        tf.summary.scalar(&apos;loss&apos;,self.loss)</div><div class="line">        self.optimizer = tf.train.AdamOptimizer(learning_rate=self.lr).minimize(self.loss)</div><div class="line"></div><div class="line">        self.accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.round(self.y_hat),self.target_ph),tf.float32))</div><div class="line">        tf.summary.scalar(&apos;accuracy&apos;,self.accuracy)</div><div class="line"></div><div class="line">    self.merged = tf.summary.merge_all()</div></pre></td></tr></table></figure>
<p>这样，一个DIEN的模型就设计好了，其中的细节还是很多的，希望大家都能动手实现一下！</p>
<h2 id="参考文献-2"><a href="#参考文献-2" class="headerlink" title="参考文献"></a>参考文献</h2><p>1、<a href="https://blog.csdn.net/friyal/article/details/83115900" target="_blank" rel="external">https://blog.csdn.net/friyal/article/details/83115900</a><br>2、<a href="https://arxiv.org/pdf/1809.03672.pdf" target="_blank" rel="external">https://arxiv.org/pdf/1809.03672.pdf</a><br>3、<a href="https://github.com/mouna99/dien" target="_blank" rel="external">https://github.com/mouna99/dien</a></p>
<h1 id="推荐系统遇上深度学习-二十五-–当知识图谱遇上个性化推荐"><a href="#推荐系统遇上深度学习-二十五-–当知识图谱遇上个性化推荐" class="headerlink" title="推荐系统遇上深度学习(二十五)–当知识图谱遇上个性化推荐"></a>推荐系统遇上深度学习(二十五)–当知识图谱遇上个性化推荐</h1><p>之前在美团听过关于知识图谱和个性化推荐的一个讲座，接下来的几篇，我们将围绕讲座中提到的知识点，来介绍下知识图谱是如何同个性化推荐相结合的！本篇算是一个开篇吧，希望大家伙能够有一个基本的认识。</p>
<h2 id="1、推荐系统的任务和难点"><a href="#1、推荐系统的任务和难点" class="headerlink" title="1、推荐系统的任务和难点"></a>1、推荐系统的任务和难点</h2><p>推荐问题的本质是代替用户评估其从未看过、接触过或者使用过的物品。<br>推荐系统一般分为两类：<br><strong>评分预测</strong>：预测用户对物品的评价。比如在电影推荐中，系统需要预测用户对电影的评分，并以此为根据推送给用户可能喜欢的电影。这种场景下，我们经常使用的数据是用户对历史观看过的电影的评分数据，这些信息可以表达用户对电影的喜好程度，因此也叫做<strong>显式反馈（explicit feedback）</strong>。<br><strong>点击率预估</strong>：预测用户对于物品是否点击。比如在新闻推荐中，系统需要预测用户点击某新闻的概率来优化推荐方案。这种场景下常常使用的信息是用户的历史点击信息。这种信息只能表达用户的行为特征(点击／未点击），而不能反映用户的喜好程度，因此这种信息也叫做<strong>隐式反馈（implicit feedback）</strong>。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-cacd44ad941670c8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>传统的推荐系统使用显式／隐式信息作为输入来进行预测，存在两个主要的问题：<br><strong>稀疏性问题</strong>：实际场景中，用户和物品的交互信息往往是非常稀疏的。如电影推荐中，电影往往成千上万部，但是用户打过分的电影往往只有几十部。使用如此少的观测数据来预测大量的未知信息，会极大增加过拟合的风险。<br><strong>冷启动问题</strong>：对于新加入的用户或者物品，其没有对应的历史信息，因此难以准确的进行建模和推荐。</p>
<p>解决稀疏性和冷启动问题的一个常见思路是在推荐算法中额外引入一些辅助信息（side information）作为输入。辅助信息可以丰富对用户和物品的描述、增强推荐算法的挖掘能力，从而有效地弥补交互信息的稀疏或缺失。常见的辅助信息包括：<br><strong>社交网络</strong>：一个用户对某个物品感兴趣，他的朋友可能也会对该物品感兴趣<br><strong>用户/物品属性</strong>：拥有同种属性的用户可能会对同一类物品感兴趣<br><strong>图像/视频/音频/文本等多媒体信息</strong>：例如商品图片、电影预告片、音乐、新闻标题等<br><strong>上下文信息</strong>：用户-物品交互的时间、地点、当前会话信息等。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-38101a1635f99175.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>近年来，知识图谱(knowledge graph)作为一种新兴的辅助信息，逐渐引起了学者们的关注。它是如何帮助提升推荐性能的，我们来一探究竟。</p>
<h2 id="2、什么是知识图谱"><a href="#2、什么是知识图谱" class="headerlink" title="2、什么是知识图谱"></a>2、什么是知识图谱</h2><p>知识图谱于2012年5月17日由Google正式提出，其初衷是为了提高搜索引擎的能力，改善用户的搜索质量以及搜索体验。随着人工智能的技术发展和应用，知识图谱逐渐成为关键技术之一，现已被广泛应用于智能搜索、智能问答、个性化推荐、内容分发等领域。</p>
<p>知识图谱的官方定义如下：知识图谱是Google用于增强其搜索引擎功能的知识库。本质上, 知识图谱旨在描述真实世界中存在的各种实体或概念及其关系,其构成一张巨大的语义网络图，节点表示实体或概念，边则由属性或关系构成。（来自维基百科）。</p>
<p>知识图谱中包含的节点如下：<br><strong>实体</strong>: 指的是具有可区别性且独立存在的某种事物。如某一个人、某一个城市、某一种植物等、某一种商品等等。世界万物由具体事物组成，此指实体。如图1的“中国”、“美国”、“日本”等。，实体是知识图谱中的最基本元素，不同的实体间存在不同的关系。<br><strong>语义类（概念）</strong>：具有同种特性的实体构成的集合，如国家、民族、书籍、电脑等。 概念主要指集合、类别、对象类型、事物的种类，例如人物、地理等。<br>内容: 通常作为实体和语义类的名字、描述、解释等，可以由文本、图像、音视频等来表达。<br><strong>属性(值)</strong>: 从一个实体指向它的属性值。不同的属性类型对应于不同类型属性的边。属性值主要指对象指定属性的值。如图1所示的“面积”、“人口”、“首都”是几种不同的属性。属性值主要指对象指定属性的值，例如960万平方公里等。<br><strong>关系</strong>: 形式化为一个函数，它把 k k个点映射到一个布尔值。在知识图谱上，关系则是一个把k k个图节点(实体、语义类、属性值)映射到布尔值的函数。</p>
<p>三元组是知识图谱的一种通用表示方式，其基本形式主要包括<strong>(实体1-关系-实体2)</strong>和<strong>(实体-属性-属性值)</strong>等。如下面的例子，中国是一个实体，北京是一个实体，中国-首都-北京 是一个（实体-关系-实体）的三元组样例。北京是一个实体 ，人口是一种属性2069.3万是属性值。北京-人口-2069.3万构成一个（实体-属性-属性值）的三元组样例。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-e5bfa3817ddf8931.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<h2 id="3、知识图谱的优势"><a href="#3、知识图谱的优势" class="headerlink" title="3、知识图谱的优势"></a>3、知识图谱的优势</h2><p>知识图谱包含了实体之间丰富的语义关联，为推荐系统提供了潜在的辅助信息来源。将知识图谱引入推荐系统中，可以给推荐系统带来以下的特性：</p>
<p><strong>精确性</strong>：知识图谱为物品引入了更多的语义关系，可以深层次地发现用户兴趣。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-c97b3fdeac807399.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p><strong>多样性</strong>：通过知识图谱中不同的关系链接种类，有利于推荐结果的发散。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-19c261943d9cad98.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p><strong>可解释性</strong>：知识图谱可以连接用户的历史记录和推荐结果，从而提高用户对推荐结果的满意度和接受度，增强用户对推荐系统的信任。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-652bd379897ca712.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<h2 id="4、知识图谱与推荐系统相结合的方法"><a href="#4、知识图谱与推荐系统相结合的方法" class="headerlink" title="4、知识图谱与推荐系统相结合的方法"></a>4、知识图谱与推荐系统相结合的方法</h2><h4 id="4-1-基于特征的推荐方法"><a href="#4-1-基于特征的推荐方法" class="headerlink" title="4.1 基于特征的推荐方法"></a>4.1 基于特征的推荐方法</h4><p>基于特征的推荐方法，主要是从知识图谱中抽取一些用户和物品的属性作为特征，放入到传统模型中，如FM模型、LR模型等等。这并非是专门针对知识图谱设计，同时也无法引入关系特征。</p>
<h4 id="4-2-基于路径的推荐方法"><a href="#4-2-基于路径的推荐方法" class="headerlink" title="4.2 基于路径的推荐方法"></a>4.2 基于路径的推荐方法</h4><p>基于路径的推荐方法，以港科大KDD 2017的录用论文《Meta-Graph Based Recommendation Fusion over Heterogeneous Information Networks》为代表。我们在后面也将一起学习这篇文章。</p>
<p>该类方法将知识图谱视为一个异构信息网络（heterogeneous information network），然后构造物品之间的基于meta-path或meta-graph的特征。简单地说，meta-path是连接两个实体的一条特定的路径，比如“演员-&gt;电影-&gt;导演-&gt;电影-&gt;演员”这条meta-path可以连接两个演员，因此可以视为一种挖掘演员之间的潜在关系的方式。这类方法的优点是充分且直观地利用了知识图谱的网络结构，缺点是需要手动设计meta-path或meta-graph，这在实践中难以到达最优；同时，该类方法无法在实体不属于同一个领域的场景（例如新闻推荐）中应用，因为我们无法为这样的场景预定义meta-path或meta-graph。</p>
<h4 id="4-3-知识图谱特征学习Knowledge-Graph-Embedding"><a href="#4-3-知识图谱特征学习Knowledge-Graph-Embedding" class="headerlink" title="4.3 知识图谱特征学习Knowledge Graph Embedding"></a>4.3 知识图谱特征学习Knowledge Graph Embedding</h4><p>知识图谱特征学习（Knowledge Graph Embedding）为知识图谱中的每个实体和关系学习得到一个低维向量，同时保持图中原有的结构或语义信息。一般而言，知识图谱特征学习的模型分类两类：基于距离的翻译模型和基于语义的匹配模型。</p>
<h5 id="基于距离的翻译模型（distance-based-translational-models）"><a href="#基于距离的翻译模型（distance-based-translational-models）" class="headerlink" title="基于距离的翻译模型（distance-based translational models）"></a>基于距离的翻译模型（distance-based translational models）</h5><p>这类模型使用基于距离的评分函数评估三元组的概率，将尾节点视为头结点和关系翻译得到的结果。这类方法的代表有TransE、TransH、TransR等；</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-8a66bcd8ace84076.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>上面三个方法的基本思想都是一样的，我们以TransE为例来介绍一下这些方法的核心思想。在空间中，三元组的头节点h、关系r、尾节点t都有对应的向量，我们希望的是h + r = t，如果h + r的结果和t越接近，那么我们认为这些向量能够很好的表示知识图谱中的实体和关系。</p>
<h5 id="基于语义的匹配模型（semantic-based-matching-models）"><a href="#基于语义的匹配模型（semantic-based-matching-models）" class="headerlink" title="基于语义的匹配模型（semantic-based matching models）"></a>基于语义的匹配模型（semantic-based matching models）</h5><p>类模型使用基于相似度的评分函数评估三元组的概率，将实体和关系映射到隐语义空间中进行相似度度量。这类方法的代表有SME、NTN、MLP、NAM等。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-562609a9e3dac2d5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>上述方法的核心是构造一个二分类模型，将h、r和t输入到网络中，如果(h,r,t)在知识图谱中真实存在，则应该得到接近1的概率，如果不存在，应该得到接近0的概率。</p>
<h5 id="结合知识图谱特征学习的推荐系统"><a href="#结合知识图谱特征学习的推荐系统" class="headerlink" title="结合知识图谱特征学习的推荐系统"></a>结合知识图谱特征学习的推荐系统</h5><p>知识图谱特征学习与推荐系统相结合，往往有以下几种方式：依次训练、联合训练、交替训练。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-651f4bcf339244cf.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>依次训练的方法主要有：<strong>Deep Knowledge-aware Network(DKN)</strong><br>联合训练的方法主要有：<strong>Ripple Network</strong><br>交替训练主要采用multi-task的思路，主要方法有：<strong>Multi-task Learning for KG enhanced Recommendation (MKR)</strong></p>
<p>在接下来的系列文章中，我们将介绍上面的几种算法，敬请期待！</p>
<h2 id="参考文献-3"><a href="#参考文献-3" class="headerlink" title="参考文献"></a>参考文献</h2><p>1、<a href="https://baijiahao.baidu.com/s?id=1592653047313321258&amp;wfr=spider&amp;for=pc" target="_blank" rel="external">https://baijiahao.baidu.com/s?id=1592653047313321258&amp;wfr=spider&amp;for=pc</a><br>2、<a href="https://baijiahao.baidu.com/s?id=1602210213239784098&amp;wfr=spider&amp;for=pc" target="_blank" rel="external">https://baijiahao.baidu.com/s?id=1602210213239784098&amp;wfr=spider&amp;for=pc</a><br>3、<a href="https://baijiahao.baidu.com/s?id=1602648407587582255&amp;wfr=spider&amp;for=pc" target="_blank" rel="external">https://baijiahao.baidu.com/s?id=1602648407587582255&amp;wfr=spider&amp;for=pc</a></p>
<h1 id="推荐系统遇上深度学习-二十六-–知识图谱与推荐系统结合之DKN模型原理及实现"><a href="#推荐系统遇上深度学习-二十六-–知识图谱与推荐系统结合之DKN模型原理及实现" class="headerlink" title="推荐系统遇上深度学习(二十六)–知识图谱与推荐系统结合之DKN模型原理及实现"></a>推荐系统遇上深度学习(二十六)–知识图谱与推荐系统结合之DKN模型原理及实现</h1><p>在本系列的上一篇中，我们大致介绍了一下知识图谱在推荐系统中的一些应用，我们最后讲到知识图谱特征学习(Knowledge Graph Embedding)是最常见的与推荐系统结合的方式，知识图谱特征学习为知识图谱中的每个实体和关系学习到一个低维向量，同时保持图中原有的结构或语义信息，最常见的得到低维向量的方式主要有基于距离的翻译模型和基于语义的匹配模型。</p>
<p>知识图谱特征学习在推荐系统中的应用步骤大致有以下三种方式：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-2b95f73d788a2357.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>依次训练的方法主要有：<strong>Deep Knowledge-aware Network(DKN)</strong><br>联合训练的方法主要有：<strong>Ripple Network</strong><br>交替训练主要采用multi-task的思路，主要方法有：<strong>Multi-task Learning for KG enhanced Recommendation (MKR)</strong></p>
<p>本文先来介绍依次训练的方法<strong>Deep Knowledge-aware Network(DKN)</strong>。</p>
<p>论文下载地址为：<a href="https://arxiv.org/abs/1801.08284v1" target="_blank" rel="external">https://arxiv.org/abs/1801.08284v1</a></p>
<h2 id="1、DKN原理"><a href="#1、DKN原理" class="headerlink" title="1、DKN原理"></a>1、DKN原理</h2><h4 id="1-1-背景-1"><a href="#1-1-背景-1" class="headerlink" title="1.1 背景"></a>1.1 背景</h4><p>推荐系统最初是为了解决互联网信息过载的问题，给用户推荐其感兴趣的内容。在新闻推荐领域，有三个突出的问题需要解决：<br>1.新闻文章具有高度的时间敏感性，它们的相关性很快就会在短时间内失效。 过时的新闻经常被较新的新闻所取代。 导致传统的基于ID的协同过滤算法失效。<br>2.用户在阅读新闻的时候是带有明显的倾向性的，一般一个用户阅读过的文章会属于某些特定的主题，如何利用用户的阅读历史记录去预测其对于候选文章的兴趣是新闻推荐系统的关键 。<br>3.新闻类文章的语言都是高度浓缩的，包含了大量的知识实体与常识。用户极有可能选择阅读与曾经看过的文章具有紧密的知识层面的关联的文章。以往的模型只停留在衡量新闻的语义和词共现层面的关联上，很难考虑隐藏的知识层面的联系。</p>
<p>因此，Deep Knowledge-aware Network(DKN)模型中加入新闻之间知识层面的相似度量，来给用户更精确地推荐可能感兴趣的新闻。</p>
<h4 id="1-2-基础概念"><a href="#1-2-基础概念" class="headerlink" title="1.2 基础概念"></a>1.2 基础概念</h4><h5 id="1-2-1-知识图谱特征学习（Knowledge-Graph-Embedding）"><a href="#1-2-1-知识图谱特征学习（Knowledge-Graph-Embedding）" class="headerlink" title="1.2.1 知识图谱特征学习（Knowledge Graph Embedding）"></a>1.2.1 知识图谱特征学习（Knowledge Graph Embedding）</h5><p>知识图谱特征学习（Knowledge Graph Embedding）为知识图谱中的每个实体和关系学习得到一个低维向量，同时保持图中原有的结构或语义信息。一般而言，知识图谱特征学习的模型分类两类：基于距离的翻译模型和基于语义的匹配模型。</p>
<p>####基于距离的翻译模型（distance-based translational models）<br>这类模型使用基于距离的评分函数评估三元组的概率，将尾节点视为头结点和关系翻译得到的结果。这类方法的代表有TransE、TransH、TransR等；</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-8a66bcd8ace84076.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>上面三个方法的基本思想都是一样的，我们以TransE为例来介绍一下这些方法的核心思想。在空间中，三元组的头节点h、关系r、尾节点t都有对应的向量，我们希望的是h + r = t，如果h + r的结果和t越接近，那么我们认为这些向量能够很好的表示知识图谱中的实体和关系。</p>
<p><strong>基于语义的匹配模型（semantic-based matching models）</strong></p>
<p>类模型使用基于相似度的评分函数评估三元组的概率，将实体和关系映射到隐语义空间中进行相似度度量。这类方法的代表有SME、NTN、MLP、NAM等。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-562609a9e3dac2d5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>上述方法的核心是构造一个二分类模型，将h、r和t输入到网络中，如果(h,r,t)在知识图谱中真实存在，则应该得到接近1的概率，如果不存在，应该得到接近0的概率。</p>
<h5 id="1-2-2-基于CNN的句子特征提取"><a href="#1-2-2-基于CNN的句子特征提取" class="headerlink" title="1.2.2 基于CNN的句子特征提取"></a>1.2.2 基于CNN的句子特征提取</h5><p>DKN中提取句子特征的CNN源自于Kim CNN，用句子所包含词的词向量组成的二维矩阵，经过一层卷积操作之后再做一次max-over-time的pooling操作得到句子向量，如下图所示：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-8e7c2554cecde4fa.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<h4 id="1-3-问题定义"><a href="#1-3-问题定义" class="headerlink" title="1.3 问题定义"></a>1.3 问题定义</h4><p>给定义一个用户user<sub>i</sub>,他的点击历史记为{t<sub>1</sub>,t<sub>2</sub>,t<sub>3</sub>,….,t<sub>N</sub>}是该用户过去一段时间内层点击过的新闻的标题，N代表用户点击过新闻的总数。每个标题都是一个词序列t={w<sub>1</sub>,w<sub>2</sub>,w<sub>3</sub>,….,w<sub>n</sub>},标题中的单词有的对应知识图谱中的一个实体 。举例来说，标题《Trump praises Las Vegas medical team》其中Trump与知识图谱中的实体“Donald Trump”对应，Las和Vegas与实体Las Vegas对应。本文要解决的问题就是给定用户的点击历史，以及标题单词和知识图谱中实体的关联，我们要预测的是：一个用户i是否会点击一个特定的新闻t<sub>j</sub>。</p>
<h4 id="1-4-模型框架"><a href="#1-4-模型框架" class="headerlink" title="1.4 模型框架"></a>1.4 模型框架</h4><p>DKN模型的整体框架如下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-d88a94cd73303a53.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>可以看到，DKN的网络输入有两个：候选新闻集合，用户点击过的新闻标题序列。输入数据通过KCNN来提取特征，之上是一个attention层，计算候选新闻向量与用户点击历史向量之间的attention权重，在顶层拼接两部分向量之后，用DNN计算用户点击此新闻的概率。接下来，我们介绍一下DKN模型中的一些细节。</p>
<h5 id="1-4-1-知识提取（Knowledge-Distillation）"><a href="#1-4-1-知识提取（Knowledge-Distillation）" class="headerlink" title="1.4.1 知识提取（Knowledge Distillation）"></a>1.4.1 知识提取（Knowledge Distillation）</h5><p>知识提取过程有三方面，一是得到标题中每个单词的embedding，二是得到标题中每个单词对应的实体的embedding。三是得到每个单词的上下文embedding。每个单词对应的embedding可以通过word2vec预训练的模型得到。这里我们主要讲后两部分。</p>
<p><strong>实体embedding</strong><br>实体特征即标题中每个单词对应的实体的特征表示，通过下面四个步骤得到：</p>
<ol>
<li>识别出标题中的实体并利用实体链接技术消除歧义</li>
<li>根据已有知识图谱，得到与标题中涉及的实体链接在一个step之内的所有实体所形成的子图。</li>
<li>构建好知识子图以后，利用基于距离的翻译模型得到子图中每个实体embedding。</li>
<li>得到标题中每个单词对应的实体embedding。</li>
</ol>
<p>过程图示如下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-67a326541ea4d52d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p><strong>上下文embedding</strong></p>
<p>尽管目前现有的知识图谱特征学习方法得到的向量保存了绝大多数的结构信息，但还有一定的信息损失，为了更好地利用一个实体在原知识图谱的位置信息，文中还提到了利用一个实体的上下文来进一步的刻画每个实体，具体来说，即用每个实体相连的实体embedding的平均值来进一步刻画每个实体，计算公式如下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-45df2710a390835f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>图示如下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-5e1fd7b6de9ea72e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<h5 id="1-4-2-新闻特征提取KCNN-Knowledge-aware-CNN"><a href="#1-4-2-新闻特征提取KCNN-Knowledge-aware-CNN" class="headerlink" title="1.4.2 新闻特征提取KCNN(Knowledge-aware CNN)"></a>1.4.2 新闻特征提取KCNN(Knowledge-aware CNN)</h5><p>在知识抽取部分，我们得到了三部分的embedding，一种最简单的使用方式就是直接将其拼接：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-00b00111e6a03729.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>但这样做存在几方面的限制：</p>
<ol>
<li>连接策略打破了单词和相关实体之间的联系，并且不知道它们的对齐方式。</li>
<li>单词的embedding和对应实体的embedding是通过不同的方法学习的，这意味着它们不适合在单个向量空间中将它们一起进行卷积操作。</li>
<li>连接策略需要单词的embedding和实体的embedding具有相同的维度，这在实际设置中可能不是最优的，因为词和实体embedding的最佳维度可能彼此不同。</li>
</ol>
<p>因此本文使用的是multi-channel和word-entity-aligned KCNN。具体做法是先把实体的embedding和实体上下文embedding映射到一个空间里，映射的方式可以选择线性方式g(e) = Me，也可以选择非线性方式g(e) = tanh(Me + b)，这样我们就可以拼接三部分作为KCNN的输入：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-53a3694d426368d9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>KCNN的过程我们之前已经介绍过了，这里就不再赘述。</p>
<h5 id="1-4-3-基于注意力机制的用户兴趣预测"><a href="#1-4-3-基于注意力机制的用户兴趣预测" class="headerlink" title="1.4.3  基于注意力机制的用户兴趣预测"></a>1.4.3  基于注意力机制的用户兴趣预测</h5><p>获取到用户点击过的每篇新闻的向量表示以后，作者并没有简单地作加和来代表该用户，而是计算候选文档对于用户每篇点击文档的attention，再做加权求和，计算attention：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-7b4f38f238ddcf89.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<h4 id="1-5-实验结果"><a href="#1-5-实验结果" class="headerlink" title="1.5 实验结果"></a>1.5 实验结果</h4><p>本文的数据来自bing新闻的用户点击日志，包含用户id，新闻url，新闻标题，点击与否（0未点击，1点击）。搜集了2016年10月16日到2017年7月11号的数据作为训练集。2017年7月12号到8月11日的数据作为测试集合。使用的知识图谱数据是Microsoft Satori。以下是一些基本的统计数据以及分布：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-715c64b5a02dfec6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>本文将DKN与FM、KPCNN、DSSM、Wide&amp;Deep、DeepFM等模型进行对比试验，结果如下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-6da64978cf210a4d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>随后，本文根据DKN中是否使用上下文实体embedding、使用哪种实体embedding计算方法、是否对实体embedding进行变换、是否使用attention机制等进行了对比试验，结果如下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-370fd47239d10fd7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>实验表明，在使用DKN模型时，同时使用实体embedding和上下文embedding、使用TransD方法、使用非线性变换、使用attention机制可以获得更好的预测效果。</p>
<h2 id="2、DKN模型tensorflow实现"><a href="#2、DKN模型tensorflow实现" class="headerlink" title="2、DKN模型tensorflow实现"></a>2、DKN模型tensorflow实现</h2><p>接下来我们就来看一下DKN模型的tensorflow实现。本文的代码地址：<a href="https://github.com/princewen/tensorflow_practice/tree/master/recommendation/Basic-DKN-Demo" target="_blank" rel="external">https://github.com/princewen/tensorflow_practice/tree/master/recommendation/Basic-DKN-Demo</a></p>
<p>参考的代码地址为：<a href="https://github.com/hwwang55/DKN" target="_blank" rel="external">https://github.com/hwwang55/DKN</a></p>
<p>目录的结构如下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-fe7e7abdf2069d4c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>可以看到，除代码外，还有news和kg两个文件夹，按照如下的步骤运行代码，就可以得到我们的训练数据、测试数据、单词对应的embedding、实体对应的embedding、实体对应的上下文embedding：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">$ cd news</div><div class="line">$ python news_preprocess.py</div><div class="line">$ cd ../kg</div><div class="line">$ python prepare_data_for_transx.py</div><div class="line">$ cd Fast-TransX/transE/ (note: you can also choose other KGE methods)</div><div class="line">$ g++ transE.cpp -o transE -pthread -O3 -march=native</div><div class="line">$ ./transE</div><div class="line">$ cd ../..</div><div class="line">$ python kg_preprocess.py</div></pre></td></tr></table></figure>
<p>目录中共4个python文件，含义分别为：<br><strong>data_loader.py</strong>:加载数据的代码，主要是产生模型的输入数据<br><strong>dkn.py</strong>：定义DKN模型<br><strong>main.py</strong>：程序的入口<br><strong>trian.py</strong>: 训练DKN模型的代码</p>
<p>代码整体还是比较好理解的，这里我们主要介绍的是DKN模型相关的代码，这里大家需要注意的主要是各个变量转换的维度，当然，我在代码里都有对应的注释，大家可以跟着代码的节奏来体会DKN中变量维度的变换。</p>
<p><strong>定义输入</strong><br>模型的输入有五个部分：用户点击过的新闻的标题对应单词、用户点击过的实体、候选集新闻的单词、候选集新闻的实体、label。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">def _build_inputs(self,args):</div><div class="line">    with tf.name_scope(&apos;input&apos;):</div><div class="line">        self.clicked_words = tf.placeholder(dtype=tf.int32,shape=[None,args.max_click_history,args.max_title_length],name=&apos;clicked_words&apos;)</div><div class="line">        self.clicked_entities = tf.placeholder(dtype=tf.int32,shape=[None,args.max_click_history,args.max_title_length],name=&apos;clicked_entities&apos;)</div><div class="line">        self.news_words = tf.placeholder(dtype=tf.int32,shape=[None,args.max_title_length],name=&apos;news_words&apos;)</div><div class="line">        self.news_entities = tf.placeholder(dtype=tf.int32,shape=[None,args.max_title_length],name=&apos;news_entities&apos;)</div><div class="line">        self.labels = tf.placeholder(dtype=tf.float32,shape=[None],name=&apos;labels&apos;)</div></pre></td></tr></table></figure>
<p><strong>得到Embeddings</strong><br>得到所有单词、实体的embedding、实体的上下文embedding，注意这里实体的embedding和上下文embedding进行了一次非线性变换：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line">with tf.name_scope(&apos;embedding&apos;):</div><div class="line">    word_embs = np.load(&apos;news/word_embeddings_&apos; + str(args.word_dim) + &apos;.npy&apos;)</div><div class="line">    entity_embs = np.load(&apos;kg/entity_embeddings_&apos; + args.KGE + &apos;_&apos; + str(args.entity_dim) + &apos;.npy&apos;)</div><div class="line">    self.word_embeddings = tf.Variable(word_embs,dtype=np.float32,name=&apos;word&apos;)</div><div class="line">    self.entity_embeddings = tf.Variable(entity_embs,dtype=np.float32,name=&apos;entity&apos;)</div><div class="line">    self.params.append(self.word_embeddings)</div><div class="line">    self.params.append(self.entity_embeddings)</div><div class="line"></div><div class="line"></div><div class="line">    if args.use_context:</div><div class="line">        context_embs = np.load(</div><div class="line">            &apos;kg/context_embeddings_&apos; + args.KGE + &apos;_&apos; + str(args.entity_dim) + &apos;.npy&apos;)</div><div class="line">        self.context_embeddings = tf.Variable(context_embs, dtype=np.float32, name=&apos;context&apos;)</div><div class="line">        self.params.append(self.context_embeddings)</div><div class="line"></div><div class="line"></div><div class="line">    if args.transform:</div><div class="line">        self.entity_embeddings = tf.layers.dense(self.entity_embeddings,units = args.entity_dim,activation=tf.nn.tanh,name=&apos;transformed_entity&apos;,</div><div class="line">                                                 kernel_regularizer=tf.contrib.layers.l2_regularizer(args.l2_weight))</div><div class="line">        if args.use_context:</div><div class="line">            self.context_embeddings = tf.layers.dense(</div><div class="line">                self.context_embeddings, units=args.entity_dim, activation=tf.nn.tanh,</div><div class="line">                name=&apos;transformed_context&apos;, kernel_regularizer=tf.contrib.layers.l2_regularizer(args.l2_weight))</div></pre></td></tr></table></figure>
<p><strong>KCNN</strong><br>KCNN这里需要注意的是变量维度的变换，首先是输入数据的维度，对用户向量来说：(batch_size * max_click_history, max_title_length, full_dim），对新闻向量来说：(batch_size, max_title_length, full_dim)：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"># (batch_size * max_click_history, max_title_length, word_dim) for users</div><div class="line"># (batch_size, max_title_length, word_dim) for news</div><div class="line">embedded_words = tf.nn.embedding_lookup(self.word_embeddings,words)</div><div class="line">embedded_entities = tf.nn.embedding_lookup(self.entity_embeddings,entities)</div><div class="line"></div><div class="line"># (batch_size * max_click_history, max_title_length, full_dim) for users</div><div class="line"># (batch_size, max_title_length, full_dim) for news</div><div class="line">if args.use_context:</div><div class="line">    embedded_contexts = tf.nn.embedding_lookup(self.context_embeddings,entities)</div><div class="line">    concat_input = tf.concat([embedded_words,embedded_entities,embedded_contexts],axis=-1)</div><div class="line">    full_dim = args.word_dim + args.entity_dim * 2</div><div class="line">else:</div><div class="line">    concat_input = tf.concat([embedded_words,embedded_entities],axis=-1)</div><div class="line">    full_dim = args.word_dim + args.entity_dim</div></pre></td></tr></table></figure>
<p>接下来是卷积和池化操作：</p>
<p><strong>卷积</strong>：这里我们设定了不同大小的卷积核，卷积核的的大小为filter_size <em> full_dim，输入的信道有1个，卷积核的大小为n_filters：<br>因此对user向量来说，卷积后的大小变为：(batch_size </em> max_click_history, max_title_length - filter_size + 1, 1, n_filters)，<br>对新闻向量来说，大小变为：(batch_size, max_title_length - filter_size + 1, 1, n_filters)。</p>
<p><strong>池化</strong>：池化操作是max-over-time的，池化后维度为：<br>对用户向量来说：(batch_size * max_click_history, 1, 1, n_filters)，<br>对新闻向量来说：(batch_size, 1, 1, n_filters)：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line">for filter_size in args.filter_sizes:</div><div class="line">    filter_shape = [filter_size, full_dim, 1, args.n_filters]</div><div class="line">    w = tf.get_variable(name=&apos;w_&apos; + str(filter_size), shape=filter_shape, dtype=tf.float32)</div><div class="line">    b = tf.get_variable(name=&apos;b_&apos; + str(filter_size), shape=[args.n_filters], dtype=tf.float32)</div><div class="line">    if w not in self.params:</div><div class="line">        self.params.append(w)</div><div class="line"></div><div class="line">    # (batch_size * max_click_history, max_title_length - filter_size + 1, 1, n_filters_for_each_size) for users</div><div class="line">    # (batch_size, max_title_length - filter_size + 1, 1, n_filters_for_each_size) for news</div><div class="line">    conv = tf.nn.conv2d(concat_input, w, strides=[1, 1, 1, 1], padding=&apos;VALID&apos;, name=&apos;conv&apos;)</div><div class="line">    relu = tf.nn.relu(tf.nn.bias_add(conv, b), name=&apos;relu&apos;)</div><div class="line"></div><div class="line">    # (batch_size * max_click_history, 1, 1, n_filters_for_each_size) for users</div><div class="line">    # (batch_size, 1, 1, n_filters_for_each_size) for news</div><div class="line">    pool = tf.nn.max_pool(relu, ksize=[1, args.max_title_length - filter_size + 1, 1, 1],</div><div class="line">                          strides=[1, 1, 1, 1], padding=&apos;VALID&apos;, name=&apos;pool&apos;)</div><div class="line">    outputs.append(pool)</div><div class="line"></div><div class="line"># (batch_size * max_click_history, 1, 1, n_filters_for_each_size * n_filter_sizes) for users</div><div class="line"># (batch_size, 1, 1, n_filters_for_each_size * n_filter_sizes) for news</div><div class="line">output = tf.concat(outputs, axis=-1)</div><div class="line"></div><div class="line"># (batch_size * max_click_history, n_filters_for_each_size * n_filter_sizes) for users</div><div class="line"># (batch_size, n_filters_for_each_size * n_filter_sizes) for news</div><div class="line">output = tf.reshape(output, [-1, args.n_filters * len(args.filter_sizes)])</div><div class="line"></div><div class="line">return output</div></pre></td></tr></table></figure>
<p><strong>Attention机制</strong><br>接下来，我们要通过attention 机制得到user embeddings：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div></pre></td><td class="code"><pre><div class="line">with tf.variable_scope(&apos;kcnn&apos;, reuse=tf.AUTO_REUSE):  # reuse the variables of KCNN</div><div class="line">    # (batch_size * max_click_history, title_embedding_length)</div><div class="line">    # title_embedding_length = n_filters_for_each_size * n_filter_sizes</div><div class="line">    clicked_embeddings = self._kcnn(clicked_words, clicked_entities, args)</div><div class="line"></div><div class="line">    # (batch_size, title_embedding_length)</div><div class="line">    news_embeddings = self._kcnn(self.news_words, self.news_entities, args)</div><div class="line"></div><div class="line"># (batch_size, max_click_history, title_embedding_length)</div><div class="line">clicked_embeddings = tf.reshape(</div><div class="line">    clicked_embeddings, shape=[-1, args.max_click_history, args.n_filters * len(args.filter_sizes)])</div><div class="line"></div><div class="line"># (batch_size, 1, title_embedding_length)</div><div class="line">news_embeddings_expanded = tf.expand_dims(news_embeddings, 1)</div><div class="line"></div><div class="line"># (batch_size, max_click_history)</div><div class="line">attention_weights = tf.reduce_sum(clicked_embeddings * news_embeddings_expanded, axis=-1)</div><div class="line"></div><div class="line"># (batch_size, max_click_history)</div><div class="line">attention_weights = tf.nn.softmax(attention_weights, dim=-1)</div><div class="line"></div><div class="line"># (batch_size, max_click_history, 1)</div><div class="line">attention_weights_expanded = tf.expand_dims(attention_weights, axis=-1)</div><div class="line"></div><div class="line"># (batch_size, title_embedding_length)</div><div class="line">user_embeddings = tf.reduce_sum(clicked_embeddings * attention_weights_expanded, axis=1)</div><div class="line"></div><div class="line">return user_embeddings, news_embeddings</div></pre></td></tr></table></figure>
<p><strong>得到输出</strong><br>最终我们可以得到我们的输出，作为点击的概率值：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">self.scores_unnormalized = tf.reduce_sum(user_embeddings * news_embeddings,axis=1)</div><div class="line">self.scores = tf.sigmoid(self.scores_unnormalized)</div></pre></td></tr></table></figure>
<p>#参考文献<br>1、原文：<a href="https://arxiv.org/abs/1801.08284v1" target="_blank" rel="external">https://arxiv.org/abs/1801.08284v1</a><br>2、<a href="https://www.zuanbi8.com/talk/16467.html" target="_blank" rel="external">https://www.zuanbi8.com/talk/16467.html</a></p>
<h1 id="推荐系统遇上深度学习-二十七-–知识图谱与推荐系统结合之RippleNet模型原理及实现"><a href="#推荐系统遇上深度学习-二十七-–知识图谱与推荐系统结合之RippleNet模型原理及实现" class="headerlink" title="推荐系统遇上深度学习(二十七)–知识图谱与推荐系统结合之RippleNet模型原理及实现"></a>推荐系统遇上深度学习(二十七)–知识图谱与推荐系统结合之RippleNet模型原理及实现</h1><p>知识图谱特征学习在推荐系统中的应用步骤大致有以下三种方式：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-2b95f73d788a2357.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>依次训练的方法主要有：<strong>Deep Knowledge-aware Network(DKN)</strong><br>联合训练的方法主要有：<strong>Ripple Network</strong><br>交替训练主要采用multi-task的思路，主要方法有：<strong>Multi-task Learning for KG enhanced Recommendation (MKR)</strong></p>
<p>本文先来介绍联合训练的方法<strong>Ripple Network</strong>。</p>
<p>论文下载地址为：<a href="https://arxiv.org/abs/1803.03467" target="_blank" rel="external">https://arxiv.org/abs/1803.03467</a></p>
<h2 id="1、RippleNet原理"><a href="#1、RippleNet原理" class="headerlink" title="1、RippleNet原理"></a>1、RippleNet原理</h2><h4 id="1-1-RippleNet背景"><a href="#1-1-RippleNet背景" class="headerlink" title="1.1 RippleNet背景"></a>1.1 RippleNet背景</h4><p>在上一篇中我们介绍了Deep Knowledge-aware Network(DKN)，在DKN中，我们需要首先学习到entity的向量和relation的向量，但是学习到的向量，其目的是为了还原知识图谱中的三元组关系，而并非是为了我们的推荐任务而学习的。因此今天我们来介绍一下知识图谱和推荐系统进行联合训练的一种网络结构：RippleNet。</p>
<p>Ripple是波纹的意思，RippleNet就是模拟用户兴趣在知识图谱上的一个传播过程，如下图所示：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-9ce78dc2d25f2aaa.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>如上图，用户的兴趣以其历史记录为中心，在知识图谱上逐层向外扩散，而在扩散过程中不断的衰减，类似于水中的波纹，因此称为RippleNet。</p>
<h4 id="1-2-RippleNet网络结构"><a href="#1-2-RippleNet网络结构" class="headerlink" title="1.2 RippleNet网络结构"></a>1.2 RippleNet网络结构</h4><p>我们先来介绍两个相关的定义：<br><strong>Relevant Entity</strong>：在给定知识图谱的情况下，用户u的k-hop相关实体定义如下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-97a81a280c801e8d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>特别地，用户u的0-hop相关实体即用户的历史记录。</p>
<p><strong>Ripple Set</strong>：用户u的k-hop ripple set被定义为以k-1 Relevant Entity 为head的相关三元组：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-bff868eddc87d42a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>这里，为避免Ripple Set过大，一般都会设定一个最大的长度，进行截断。另一方面，构建的知识图谱都是有向图，只考虑点的出度。</p>
<p>接下来，我们来看看RippleNet的网络结构：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-ec734bf5b72d6bb8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>可以看到，最终的预测值是通过item embedding和user embedding得到的，item embedding通过embedding 层可以直接得到，关键是user embedding的获取。user embedding是通过图中的绿色矩形表示的向量相加得到的，接下来，我们以第一个绿色矩形表示的向量为例，来看一下具体是如何计算的。</p>
<p>第一个绿色矩形表示的向量，需要使用的是1-hop的ripple set，对于set中的每一个(h,r,t)，会计算一个与item-embedding的相关性，相关性计算公式如下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-09ae91d82c63a767.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>最后通过加权所有t对应的embedding，就得到了第一个绿色矩形表示的向量，表示用户兴趣经第一轮扩散后的结果：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-c788c44fe8913dea.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>接下来，我们重复上面的过程，假设一共H次，那么最终user embedding的结果为：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-f43b8ee3f402e19f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>而最终的预测值计算如下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-0970cea94b3fd1b3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<h4 id="1-3-RippleNet损失函数"><a href="#1-3-RippleNet损失函数" class="headerlink" title="1.3 RippleNet损失函数"></a>1.3 RippleNet损失函数</h4><p>在给定知识图谱G，用户的隐式反馈(即用户的历史记录)Y时，我们希望最大化后验概率：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-59c95891e06ffd2a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>后验概率展开如下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-8c15e292133d1c1d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>其中，我们认为参数的先验概率服从0均值的正态分布：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-59fa1515cbc2cbe3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>第二项的似然函数形式如下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-7f6fe098898f2d08.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>上面的式子搞得我有点懵，后面应该是一个具体的概率值而不是一个正态分布，G在θ条件下的分布也是一个0均值的正态分布，后面应该是取得I<sub>h,r,t</sub>-h<sup>T</sup>Rt的一个概率，由于我们希望我们得到的指数图谱特征表示能够更好的还原三元组关系，因此希望I<sub>h,r,t</sub>-h<sup>T</sup>Rt越接近0越好。</p>
<p>第三项没什么问题，即我们常用的二分类似然函数：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-0c27d9f50a9d88d4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>因此，我们可以得到RippleNet的损失函数形式如下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-7b568648318a2957.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<h2 id="2、RippleNet的Tensorflow实现"><a href="#2、RippleNet的Tensorflow实现" class="headerlink" title="2、RippleNet的Tensorflow实现"></a>2、RippleNet的Tensorflow实现</h2><p>本文的代码地址如下：<a href="https://github.com/princewen/tensorflow_practice/tree/master/recommendation/Basic-RippleNet-Demo" target="_blank" rel="external">https://github.com/princewen/tensorflow_practice/tree/master/recommendation/Basic-RippleNet-Demo</a></p>
<p>参考的代码地址为：<a href="https://github.com/hwwang55/RippleNet" target="_blank" rel="external">https://github.com/hwwang55/RippleNet</a></p>
<p>数据下载地址为：:<a href="https://pan.baidu.com/s/13vL-z5Wk3jQFfmVIPXDovw" target="_blank" rel="external">https://pan.baidu.com/s/13vL-z5Wk3jQFfmVIPXDovw</a>  密码:infx</p>
<p>在对数据进行预处理后，我们得到了两个文件：kg_final.txt和rating_final.txt</p>
<p>rating_final.txt数据形式如下，三列分别是user-id，item-id以及label（0是通过负采样得到的，正负样本比例为1:1）。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-3b9723bca8df010f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>kg_final.txt格式如下，三类分别代表h，r，t(这里entity和item用的是同一套id)：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-a8f7730e2f8b59c0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>好了，接下来我们重点介绍一下我们的RippleNet网络的构建。</p>
<p><strong>网络输入</strong><br>网络输入主要有item的id，label以及对应的用户的ripple set：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">def _build_inputs(self):</div><div class="line">    self.items = tf.placeholder(dtype=tf.int32, shape=[None], name=&quot;items&quot;)</div><div class="line">    self.labels = tf.placeholder(dtype=tf.float64, shape=[None], name=&quot;labels&quot;)</div><div class="line">    self.memories_h = []</div><div class="line">    self.memories_r = []</div><div class="line">    self.memories_t = []</div><div class="line"></div><div class="line">    for hop in range(self.n_hop):</div><div class="line">        self.memories_h.append(</div><div class="line">            tf.placeholder(dtype=tf.int32, shape=[None, self.n_memory], name=&quot;memories_h_&quot; + str(hop)))</div><div class="line">        self.memories_r.append(</div><div class="line">            tf.placeholder(dtype=tf.int32, shape=[None, self.n_memory], name=&quot;memories_r_&quot; + str(hop)))</div><div class="line">        self.memories_t.append(</div><div class="line">            tf.placeholder(dtype=tf.int32, shape=[None, self.n_memory], name=&quot;memories_t_&quot; + str(hop)))</div></pre></td></tr></table></figure>
<p><strong>embedding层构建</strong><br>这里需要的embedding主要有entity的embedding(与item 的embedding共用）和relation的embedding，假设embedding的长度为dim，那么注意到由于relation是要用来链接head和tail的，所以它的embedding的维度为dim * dim：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">def _build_embeddings(self):</div><div class="line">    self.entity_emb_matrix = tf.get_variable(name=&quot;entity_emb_matrix&quot;, dtype=tf.float64,</div><div class="line">                                             shape=[self.n_entity, self.dim],</div><div class="line">                                             initializer=tf.contrib.layers.xavier_initializer())</div><div class="line">    self.relation_emb_matrix = tf.get_variable(name=&quot;relation_emb_matrix&quot;, dtype=tf.float64,</div><div class="line">                                               shape=[self.n_relation, self.dim, self.dim],</div><div class="line">                                               initializer=tf.contrib.layers.xavier_initializer())</div></pre></td></tr></table></figure>
<p><strong>模型构建</strong><br>模型构建的代码如下，可以看到我们建立了一个transform_matrix的tensor，这个tensor就是用来更新计算过程中的item-embedding的，我们后面会详细介绍：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line">def _build_model(self):</div><div class="line">    # transformation matrix for updating item embeddings at the end of each hop</div><div class="line">    self.transform_matrix = tf.get_variable(name=&quot;transform_matrix&quot;, shape=[self.dim, self.dim], dtype=tf.float64,</div><div class="line">                                            initializer=tf.contrib.layers.xavier_initializer())</div><div class="line"></div><div class="line">    # [batch size, dim]</div><div class="line">    self.item_embeddings = tf.nn.embedding_lookup(self.entity_emb_matrix, self.items)</div><div class="line"></div><div class="line">    self.h_emb_list = []</div><div class="line">    self.r_emb_list = []</div><div class="line">    self.t_emb_list = []</div><div class="line">    for i in range(self.n_hop):</div><div class="line">        # [batch size, n_memory, dim]</div><div class="line">        self.h_emb_list.append(tf.nn.embedding_lookup(self.entity_emb_matrix, self.memories_h[i]))</div><div class="line"></div><div class="line">        # [batch size, n_memory, dim, dim]</div><div class="line">        self.r_emb_list.append(tf.nn.embedding_lookup(self.relation_emb_matrix, self.memories_r[i]))</div><div class="line"></div><div class="line">        # [batch size, n_memory, dim]</div><div class="line">        self.t_emb_list.append(tf.nn.embedding_lookup(self.entity_emb_matrix, self.memories_t[i]))</div><div class="line"></div><div class="line">    o_list = self._key_addressing()</div><div class="line"></div><div class="line">    self.scores = tf.squeeze(self.predict(self.item_embeddings, o_list))</div><div class="line">    self.scores_normalized = tf.sigmoid(self.scores)</div></pre></td></tr></table></figure>
<p>上面用到了两个函数，分别是_key_addressing()和predict()，接下来，我们来介绍这两个函数。</p>
<p>_key_addressing()是用来的到我们的olist的，即我们在RippleNet中的绿色矩形表示的向量：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line">def _key_addressing(self):</div><div class="line">    o_list = []</div><div class="line">    for hop in range(self.n_hop):</div><div class="line">        # [batch_size, n_memory, dim, 1]</div><div class="line">        h_expanded = tf.expand_dims(self.h_emb_list[hop], axis=3)</div><div class="line">        # [batch_size, n_memory, dim]</div><div class="line">        Rh = tf.squeeze(tf.matmul(self.r_emb_list[hop], h_expanded), axis=3)</div><div class="line">        # [batch_size, dim, 1]</div><div class="line">        v = tf.expand_dims(self.item_embeddings, axis=2)</div><div class="line">        # [batch_size, n_memory]</div><div class="line">        probs = tf.squeeze(tf.matmul(Rh, v), axis=2)</div><div class="line">        # [batch_size, n_memory]</div><div class="line">        probs_normalized = tf.nn.softmax(probs)</div><div class="line">        # [batch_size, n_memory, 1]</div><div class="line">        probs_expanded = tf.expand_dims(probs_normalized, axis=2)</div><div class="line">        # [batch_size, dim]</div><div class="line">        o = tf.reduce_sum(self.t_emb_list[hop] * probs_expanded, axis=1)</div><div class="line"></div><div class="line">        self.item_embeddings = self.update_item_embedding(self.item_embeddings, o)</div><div class="line">        o_list.append(o)</div><div class="line">    return o_list</div></pre></td></tr></table></figure>
<p>可以看到，在上面的代码中，我们计算的是ripple set中每一个(h,r,t)和item-embedding的相关性，再每一个hop计算完成后，有一个update_item_embedding的操作，在这里面，我们可以选择不同的替换策略：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">def update_item_embedding(self, item_embeddings, o):</div><div class="line">    if self.item_update_mode == &quot;replace&quot;:</div><div class="line">        item_embeddings = o</div><div class="line">    elif self.item_update_mode == &quot;plus&quot;:</div><div class="line">        item_embeddings = item_embeddings + o</div><div class="line">    elif self.item_update_mode == &quot;replace_transform&quot;:</div><div class="line">        item_embeddings = tf.matmul(o, self.transform_matrix)</div><div class="line">    elif self.item_update_mode == &quot;plus_transform&quot;:</div><div class="line">        item_embeddings = tf.matmul(item_embeddings + o, self.transform_matrix)</div><div class="line">    else:</div><div class="line">        raise Exception(&quot;Unknown item updating mode: &quot; + self.item_update_mode)</div><div class="line">    return item_embeddings</div></pre></td></tr></table></figure>
<p>在得到olist之后，我们可以只用olist里面最后一个向量，也可以选择相加所有的向量，来代表user-embedding，并最终计算得到预测值：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">def predict(self, item_embeddings, o_list):</div><div class="line">    y = o_list[-1]</div><div class="line">    if self.using_all_hops:</div><div class="line">        for i in range(self.n_hop - 1):</div><div class="line">            y += o_list[i]</div><div class="line"></div><div class="line">    # [batch_size]</div><div class="line">    scores = tf.reduce_sum(item_embeddings * y, axis=1)</div><div class="line">    return scores</div></pre></td></tr></table></figure>
<p><strong>计算损失</strong><br>我们前面提到了，模型的loss最终由三部分组成，在取对数后，三部分损失分别表示对数损失、知识图谱特征表示的损失，正则化损失：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line">def _build_loss(self):</div><div class="line">    self.base_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=self.labels, logits=self.scores))</div><div class="line"></div><div class="line">    self.kge_loss = 0</div><div class="line">    for hop in range(self.n_hop):</div><div class="line">        h_expanded = tf.expand_dims(self.h_emb_list[hop], axis=2)</div><div class="line">        t_expanded = tf.expand_dims(self.t_emb_list[hop], axis=3)</div><div class="line">        hRt = tf.squeeze(tf.matmul(tf.matmul(h_expanded, self.r_emb_list[hop]), t_expanded))</div><div class="line">        self.kge_loss += tf.reduce_mean(tf.sigmoid(hRt))</div><div class="line">    self.kge_loss = -self.kge_weight * self.kge_loss</div><div class="line"></div><div class="line">    self.l2_loss = 0</div><div class="line">    for hop in range(self.n_hop):</div><div class="line">        self.l2_loss += tf.reduce_mean(tf.reduce_sum(self.h_emb_list[hop] * self.h_emb_list[hop]))</div><div class="line">        self.l2_loss += tf.reduce_mean(tf.reduce_sum(self.t_emb_list[hop] * self.t_emb_list[hop]))</div><div class="line">        self.l2_loss += tf.reduce_mean(tf.reduce_sum(self.r_emb_list[hop] * self.r_emb_list[hop]))</div><div class="line">        if self.item_update_mode == &quot;replace nonlinear&quot; or self.item_update_mode == &quot;plus nonlinear&quot;:</div><div class="line">            self.l2_loss += tf.nn.l2_loss(self.transform_matrix)</div><div class="line">    self.l2_loss = self.l2_weight * self.l2_loss</div><div class="line"></div><div class="line">    self.loss = self.base_loss + self.kge_loss + self.l2_loss</div></pre></td></tr></table></figure>
<p>好了，代码的部分我们就介绍完了，如果大家感兴趣，可以下载相应的代码和数据，进行相应的编写和调试哟！</p>
<h2 id="参考文献：-1"><a href="#参考文献：-1" class="headerlink" title="参考文献："></a>参考文献：</h2><p>1、论文：<a href="https://arxiv.org/abs/1803.03467" target="_blank" rel="external">https://arxiv.org/abs/1803.03467</a></p>
<h1 id="推荐系统遇上深度学习-二十八-–知识图谱与推荐系统结合之MKR模型原理及实现"><a href="#推荐系统遇上深度学习-二十八-–知识图谱与推荐系统结合之MKR模型原理及实现" class="headerlink" title="推荐系统遇上深度学习(二十八)–知识图谱与推荐系统结合之MKR模型原理及实现"></a>推荐系统遇上深度学习(二十八)–知识图谱与推荐系统结合之MKR模型原理及实现</h1><p>知识图谱特征学习在推荐系统中的应用步骤大致有以下三种方式：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/4155986-583b62c99689bdde?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>依次训练的方法主要有：<strong>Deep Knowledge-aware Network(DKN)</strong><br>联合训练的方法主要有：<strong>Ripple Network</strong><br>交替训练主要采用multi-task的思路，主要方法有：<strong>Multi-task Learning for KG enhanced Recommendation (MKR)</strong></p>
<p>本文先来介绍交替训练的方法<strong>MKR</strong>。</p>
<p>网上没有找到相关的论文，只有在一篇帖子里有所介绍，github上可以找到源代码进行学习。</p>
<h2 id="1、MKR原理介绍"><a href="#1、MKR原理介绍" class="headerlink" title="1、MKR原理介绍"></a>1、MKR原理介绍</h2><p>由于推荐系统中的物品和知识图谱中的实体存在重合，因此可以采用多任务学习的框架，将推荐系统和知识图谱特征学习视为两个分离但是相关的任务，进行交替式的学习。</p>
<p>MKR的模型框架如下图，其中左侧是推荐系统任务，右侧是知识图谱特征学习任务。推荐部分的输入是用户和物品的特征表示，点击率的预估值作为输出。知识图谱特征学习部分使用的是三元组的头节点和关系作为输入，预测的尾节点作为输出：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-1bd15d306b436245.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>由于推荐系统中的物品和知识图谱中的实体存在重合，所以两个任务并非相互独立。所以作者在两个任务中设计了交叉特征共享单元（cross-feature-sharing units）作为两者的连接纽带。</p>
<p>交叉特征共享单元是一个可以让两个任务交换信息的模块。由于物品向量和实体向量实际上是对同一个对象的两种描述，他们之间的信息交叉共享可以让两者都获得来自对方的额外信息，从而弥补了自身的信息稀疏性的不足，其结构如下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-e6d7708efb922818.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>关于这个交叉单元具体实现，大家可以参照代码进行理解。</p>
<p>最后是损失函数部分，由于是交替训练的方式，所以在训练时首先固定推荐系统模块的参数，训练知识图谱特征学习模块的参数；然后固定知识图谱特征学习模块的参数，训练推荐系统模块的参数。</p>
<p>推荐系统模块是点击率预估模型，损失函数是对数损失加l2正则项；知识图谱特征学习模块希望预测得到的tail向量和真实的tail向量相近，因此首先计算二者的内积（内积可近似表示向量之间的余弦相似度），内积经过sigmoid之后取相反数，再加上l2正则项，即得到了知识图谱特征学习模块的损失。关于损失的计算，我们在代码里可以更清楚的看到。</p>
<h2 id="2、MKR模型tensorflow实现"><a href="#2、MKR模型tensorflow实现" class="headerlink" title="2、MKR模型tensorflow实现"></a>2、MKR模型tensorflow实现</h2><p>本文的代码地址为：<a href="https://github.com/princewen/tensorflow_practice/tree/master/recommendation/Basic-MKR-Demo" target="_blank" rel="external">https://github.com/princewen/tensorflow_practice/tree/master/recommendation/Basic-MKR-Demo</a><br>参考代码地址为：<a href="https://github.com/hwwang55/MKR" target="_blank" rel="external">https://github.com/hwwang55/MKR</a><br>数据下载地址为：<a href="https://pan.baidu.com/s/1uHkQXK_ozAgBWcMUMzOfZQ" target="_blank" rel="external">https://pan.baidu.com/s/1uHkQXK_ozAgBWcMUMzOfZQ</a>  密码:qw30</p>
<p>在对数据进行预处理后，我们得到了两个文件：kg_final.txt和rating_final.txt</p>
<p>rating_final.txt数据形式如下，三列分别是user-id，item-id以及label（0是通过负采样得到的，正负样本比例为1:1）。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-3b9723bca8df010f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>kg_final.txt格式如下，三类分别代表h，r，t(这里entity和item用的是同一套id)：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-a8f7730e2f8b59c0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>好了，接下来我们重点介绍一下我们的MKR框架的构建。</p>
<p><strong>模型输入</strong><br>模型输入有以下几部分：用户的id、物品的id、推荐系统部分的label、知识图谱三元组的head、relation、tail的对应id：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">def _build_inputs(self):</div><div class="line">    self.user_indices = tf.placeholder(tf.int32,[None],&apos;user_indices&apos;)</div><div class="line">    self.item_indices = tf.placeholder(tf.int32,[None],&apos;item_indices&apos;)</div><div class="line">    self.labels = tf.placeholder(tf.float32,[None],&apos;labels&apos;)</div><div class="line">    self.head_indices = tf.placeholder(tf.int32,[None],&apos;head_indices&apos;)</div><div class="line">    self.tail_indices = tf.placeholder(tf.int32,[None],&apos;tail_indices&apos;)</div><div class="line">    self.relation_indices = tf.placeholder(tf.int32,[None],&apos;relation_indices&apos;)</div></pre></td></tr></table></figure>
<p><strong>低层网络构建</strong><br>低层网络指下面的部分：<br><img src="https://upload-images.jianshu.io/upload_images/4155986-b777458a52c85f50.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>可以看到，user_id、item_id、head_id以及relation_id首先转换为对应的embedding，user_id和relation_id经由多层神经网络向上传播、而head_id和item_id经过交叉单元进行传播。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line">def _build_low_layers(self,args):</div><div class="line">    self.user_emb_matrix = tf.get_variable(&apos;user_emb_matrix&apos;, [self.n_user, args.dim])</div><div class="line">    self.item_emb_matrix = tf.get_variable(&apos;item_emb_matrix&apos;, [self.n_item, args.dim])</div><div class="line">    self.entity_emb_matrix = tf.get_variable(&apos;entity_emb_matrix&apos;, [self.n_entity, args.dim])</div><div class="line">    self.relation_emb_matrix = tf.get_variable(&apos;relation_emb_matrix&apos;, [self.n_relation, args.dim])</div><div class="line"></div><div class="line">    # [batch_size, dim]</div><div class="line">    self.user_embeddings = tf.nn.embedding_lookup(self.user_emb_matrix, self.user_indices)</div><div class="line">    self.item_embeddings = tf.nn.embedding_lookup(self.item_emb_matrix, self.item_indices)</div><div class="line">    self.head_embeddings = tf.nn.embedding_lookup(self.entity_emb_matrix, self.head_indices)</div><div class="line">    self.relation_embeddings = tf.nn.embedding_lookup(self.relation_emb_matrix, self.relation_indices)</div><div class="line">    self.tail_embeddings = tf.nn.embedding_lookup(self.entity_emb_matrix, self.tail_indices)</div><div class="line"></div><div class="line">    for _ in range(args.L):</div><div class="line">        user_mlp = Dense(input_dim=args.dim,output_dim=args.dim)</div><div class="line">        tail_mlp = Dense(input_dim=args.dim,output_dim = args.dim)</div><div class="line">        cc_unit = CrossCompressUnit(args.dim)</div><div class="line"></div><div class="line">        self.user_embeddings = user_mlp(self.user_embeddings)</div><div class="line">        self.item_embeddings,self.head_embeddings = cc_unit([self.item_embeddings,self.head_embeddings])</div><div class="line">        self.tail_embeddings = tail_mlp(self.tail_embeddings)</div><div class="line"></div><div class="line">        self.vars_rs.extend(user_mlp.vars)</div><div class="line">        self.vars_rs.extend(cc_unit.vars)</div><div class="line">        self.vars_kge.extend(tail_mlp.vars)</div><div class="line">        self.vars_kge.extend(cc_unit.vars)</div></pre></td></tr></table></figure>
<p>接下来，我们来看一下交叉单元的代码:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line">v,e = inputs</div><div class="line"></div><div class="line">v = tf.expand_dims(v,dim=2)</div><div class="line">e = tf.expand_dims(e,dim=1)</div><div class="line"></div><div class="line"></div><div class="line"># [batch_size, dim, dim]</div><div class="line">c_matrix = tf.matmul(v, e)</div><div class="line">c_matrix_transpose = tf.transpose(c_matrix, perm=[0, 2, 1])</div><div class="line"></div><div class="line"># [batch_size * dim, dim]</div><div class="line">c_matrix = tf.reshape(c_matrix, [-1, self.dim])</div><div class="line">c_matrix_transpose = tf.reshape(c_matrix_transpose, [-1, self.dim])</div><div class="line"></div><div class="line">v_output = tf.reshape(tf.matmul(c_matrix,self.weight_vv) + tf.matmul(c_matrix_transpose,self.weight_ev),[-1,self.dim]) + self.bias_v</div><div class="line"></div><div class="line">e_output = tf.reshape(tf.matmul(c_matrix, self.weight_ve) + tf.matmul(c_matrix_transpose, self.weight_ee),</div><div class="line">                      [-1, self.dim]) + self.bias_e</div><div class="line"></div><div class="line">return v_output,e_output</div></pre></td></tr></table></figure>
<p>item对应的embedding用v表示，head对应的embedding用e表示，二者初始情况下都是batch <em> dim大小的。过程如下：<br>1、v扩展成三维batch </em> dim <em> 1，e扩展成三维batch </em> 1 <em> dim，随后二者进行矩阵相乘v </em> e，我们知道三维矩阵相乘实际上是后两维进行运算，因此得到c_matrix的大小为 batch <em> dim </em> dim<br>2、对得到的c_matrix进行转置，得到c_matrix_transpose，大小为batch <em> dim </em> dim。这相当于将e扩展成三维batch <em> dim </em> 1，v扩展成三维batch <em> 1 </em> dim，随后二者进行矩阵相乘e <em> v。这是两种不同的特征交叉方式。<br>3、对c_matrix和c_matrix_transpose 进行reshape操作，变为（batch </em> dim ） * dim的二维矩阵<br>4、定义两组不同的参数和偏置，分别得到交叉后的v_output和e_output.</p>
<p><strong>高层网络构建</strong><br>高层网络指下面的部分：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-cb61c885483a0a1f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>对于推荐部分，可以采用内积直接得到CTR的预估值，也可以经过多层神经网络得到预估值；对于知识图谱部分，将head和relation对应的向量进行拼接，经过多层神经网络，得到一个tail对应向量的预估值，并与真实的tail向量计算内积。代码如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div></pre></td><td class="code"><pre><div class="line">def _build_high_layers(self,args):</div><div class="line">    #RS</div><div class="line">    use_inner_product = True</div><div class="line">    if use_inner_product:</div><div class="line">        self.scores = tf.reduce_sum(self.user_embeddings*self.item_embeddings,axis=1)</div><div class="line">    else:</div><div class="line">        self.user_item_concat = tf.concat([self.user_embeddings,self.item_embeddings],axis=1)</div><div class="line">        for _ in range(args.H - 1):</div><div class="line">            rs_mlp = Dense(input_dim = args.dim * 2 , output_dim = args.dim * 2)</div><div class="line">            self.user_item_concat = rs_mlp(self.user_item_concat)</div><div class="line">            self.vars_rs.extend(rs_mlp.vars)</div><div class="line"></div><div class="line">        rs_pred_mlp = Dense(input_dim=args.dim * 2,output_dim=1)</div><div class="line">        self.scores = tf.squeeze(rs_pred_mlp(self.user_item_concat))</div><div class="line">        self.vars_rs.extend(rs_pred_mlp)</div><div class="line"></div><div class="line">    self.scores_normalized = tf.nn.sigmoid(self.scores)</div><div class="line"></div><div class="line">    #KGE</div><div class="line">    self.head_relation_concat = tf.concat([self.head_embeddings,self.relation_embeddings],axis=1)</div><div class="line">    for _ in range(args.H - 1):</div><div class="line">        kge_mlp = Dense(input_dim=args.dim * 2,output_dim = args.dim * 2)</div><div class="line">        self.head_relation_concat = kge_mlp(self.head_relation_concat)</div><div class="line">        self.vars_kge.extend(kge_mlp.vars)</div><div class="line"></div><div class="line">    kge_pred_mlp = Dense(input_dim=args.dim * 2,output_dim = args.dim)</div><div class="line">    self.tail_pred = kge_pred_mlp(self.head_relation_concat)</div><div class="line">    self.vars_kge.extend(kge_pred_mlp.vars)</div><div class="line">    self.tail_pred = tf.nn.sigmoid(self.tail_pred)</div><div class="line"></div><div class="line">    self.scores_kge = tf.nn.sigmoid(tf.reduce_sum(self.tail_embeddings * self.tail_pred,axis=1))</div><div class="line">    #self.rmse = tf.reduce_mean(tf.sqrt(tf.reduce_sum(tf.square(self.tail_embeddings - self.tail_pred),axis=1) / args.dim))</div></pre></td></tr></table></figure>
<p><strong>定义损失</strong><br>推荐系统部分的损失是对数损失加l2正则项：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"># RS</div><div class="line">self.base_loss_rs = tf.reduce_mean(</div><div class="line">    tf.nn.sigmoid_cross_entropy_with_logits(labels=self.labels, logits=self.scores))</div><div class="line">self.l2_loss_rs = tf.nn.l2_loss(self.user_embeddings) + tf.nn.l2_loss(self.item_embeddings)</div><div class="line">for var in self.vars_rs:</div><div class="line">    self.l2_loss_rs += tf.nn.l2_loss(var)</div><div class="line">self.loss_rs = self.base_loss_rs + self.l2_loss_rs * args.l2_weight</div></pre></td></tr></table></figure>
<p>知识图谱特征学习模块用上一步计算的scores_kge的相反数再加上l2正则项：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"># KGE</div><div class="line">self.base_loss_kge = -self.scores_kge</div><div class="line">self.l2_loss_kge = tf.nn.l2_loss(self.head_embeddings) + tf.nn.l2_loss(self.tail_embeddings)</div><div class="line">for var in self.vars_kge:</div><div class="line">    self.l2_loss_kge += tf.nn.l2_loss(var)</div><div class="line">self.loss_kge = self.base_loss_kge + self.l2_loss_kge * args.l2_weight</div></pre></td></tr></table></figure>
<h2 id="参考文献-4"><a href="#参考文献-4" class="headerlink" title="参考文献"></a>参考文献</h2><p>1、<a href="http://baijiahao.baidu.com/s?id=1602210213239784098&amp;wfr=spider&amp;for=pc" target="_blank" rel="external">http://baijiahao.baidu.com/s?id=1602210213239784098&amp;wfr=spider&amp;for=pc</a></p>
<h1 id="推荐系统遇上深度学习-二十九-–协同记忆网络理论及实践"><a href="#推荐系统遇上深度学习-二十九-–协同记忆网络理论及实践" class="headerlink" title="推荐系统遇上深度学习(二十九)–协同记忆网络理论及实践"></a>推荐系统遇上深度学习(二十九)–协同记忆网络理论及实践</h1><p>协同过滤(collaborative filtering)是推荐系统中经典的一类方法。协同过滤中比较经典的解法有基于邻域方法、矩阵分解等，这些方法都有各自的优点和缺点，本文介绍的方法-<strong>协同记忆网络(Collaborative Memory Network，简称CMN)</strong>融合了不同协同过滤方法的优点。我们来一探究竟！</p>
<p>协同记忆网络CMN论文：<a href="https://arxiv.org/pdf/1804.10862.pdf" target="_blank" rel="external">https://arxiv.org/pdf/1804.10862.pdf</a></p>
<p>代码地址：<a href="https://github.com/tebesu/CollaborativeMemoryNetwork" target="_blank" rel="external">https://github.com/tebesu/CollaborativeMemoryNetwork</a></p>
<h2 id="1、协同过滤介绍"><a href="#1、协同过滤介绍" class="headerlink" title="1、协同过滤介绍"></a>1、协同过滤介绍</h2><p>在信息过载的时代，推荐系统显得十分重要。而在推荐系统中，协同过滤是一种非常受欢迎且有效的方法。协同过滤基于用户和物品的过去交互行为，同时认为相似的用户会消费相似的物品，从而进行推荐。</p>
<p>协同过滤是一类推荐方法，大体上可以分为三个分支：</p>
<ol>
<li><strong>基于邻域的方法</strong>。这也是我们常说的基于物品的协同过滤和基于用户的协同过滤方法。我们首先需要计算用户之间、物品之间的相似度，随后基于计算的相似度进行推荐。这种方法的一个主要缺陷就是只使用了局部的结构，而忽略了很多全局的信息，因为我们只使用K个相似用户或者相似物品进行相关的推荐。</li>
<li><strong>基于隐向量的方法</strong>。这一分支中最具代表性的是矩阵分解及后面的各种改进方法。通常的做法是将每一个用户和物品表示称一个n维的向量，通过用户矩阵和物品矩阵的相乘，希望能够尽可能还原评分矩阵。这种做法虽然考虑了全局的信息，但是忽略了一些比较强的局部联系。</li>
<li><strong>基于混合模型的方法</strong>。由于上述两种方法都有各自的缺陷，因此混合方法开始出现。最具代表性的是因子分解机和SVD++方法。</li>
</ol>
<p>也就是说，在使用协同过滤这些方法时，我们通常需要关注两点：<br>1、需要考虑全局的信息，充分利用整个评分矩阵。<br>2、需要考虑局部的信息，考虑用户或者物品之间的相似性。相似性高的用户或者物品给予更高的权重。</p>
<p>本文将要介绍的协同记忆网络，便是充分利用了上述两方面的信息。协同过滤我们已经介绍了，那么什么是记忆网络呢？我们接下来进行介绍。</p>
<h2 id="2、记忆网络Memory-Network简介"><a href="#2、记忆网络Memory-Network简介" class="headerlink" title="2、记忆网络Memory Network简介"></a>2、记忆网络Memory Network简介</h2><p>Memory Network是深度学习的一个小分支，从2014年被提出到现在也逐渐发展出了几个成熟的模型。我们这里只介绍其中两个比较基础的模型。一个是 Basic Memory Network，另一个是End to End Memory Network。</p>
<p>我们首先要搞清楚的是，为什么要有记忆网络？在翻译、问答等领域的任务中，我们通常使用的是Seq2Seq结构，由两个循环神经网络组成。循环神经网络(RNN,LSTM,GRU等)使用hidden states或者Attention机制作为他们的记忆功能，但是这种方法产生的记忆太小了，无法精确记录一段话中所表达的全部内容，也就是在将输入编码成dense vectors的时候丢失了很多信息。因此，在模型中加入一系列的记忆单元，增强模型的记忆能力，便有了Memory Network。</p>
<h4 id="2-1-Basic-Memory-Network"><a href="#2-1-Basic-Memory-Network" class="headerlink" title="2.1 Basic Memory Network"></a>2.1 Basic Memory Network</h4><p>基本的Memory Network由Facebook在2014年的“Memory Networks”一文中提出。该模型主要由一个记忆数组m和I，G，O，R四个模块。结构图如下所示：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-3acecc2c536c9a31.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>简单来说，就是输入的文本经过Input模块编码成向量，然后将其作为Generalization模块的输入，该模块根据输入的向量对memory进行读写操作，即对记忆进行更新。然后Output模块会根据Question（也会进过Input模块进行编码）对memory的内容进行权重处理，将记忆按照与Question的相关程度进行组合得到输出向量，最终Response模块根据输出向量编码生成一个自然语言的答案出来。各模块作用如下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-c3d7071e85b98fc7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>有关记忆网络的详细原理，参考文章：<a href="https://zhuanlan.zhihu.com/p/29590286或者原论文：https://arxiv.org/pdf/1410.3916.pdf。" target="_blank" rel="external">https://zhuanlan.zhihu.com/p/29590286或者原论文：https://arxiv.org/pdf/1410.3916.pdf。</a></p>
<h4 id="2-2-End-to-End-Memory-Network"><a href="#2-2-End-to-End-Memory-Network" class="headerlink" title="2.2 End to End Memory Network"></a>2.2 End to End Memory Network</h4><p>End to End Memory Network是Memory Network的一个改进版本，可以进行端对端的学习。原文中介绍的网络模型应用于QA任务。单层网络的结构如下图所示：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-c18d3771e13ce146.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>模型主要的参数包括A,B,C,W四个矩阵，其中A,B,C三个矩阵就是embedding矩阵，主要是将输入文本和Question编码成词向量，W是最终的输出矩阵。从上图可以看出，对于输入的句子s分别会使用A和C进行编码得到Input和Output的记忆模块，Input用来跟Question编码得到的向量相乘得到每句话跟q的相关性，Output则与该相关性进行加权求和得到输出向量。然后再加上q并传入最终的输出层。</p>
<p>进一步，我们可以使用多层的结构：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-252a7a1ff40e55d4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>有关End2End Memory Network的详细原理，可以参考文章：<a href="https://zhuanlan.zhihu.com/p/29679742和原论文：http://10.3.200.202/cache/11/03/papers.nips.cc/82b8c2ad3e5cde7cad659be2d37c251e/5846-end-to-end-memory-networks.pdf。" target="_blank" rel="external">https://zhuanlan.zhihu.com/p/29679742和原论文：http://10.3.200.202/cache/11/03/papers.nips.cc/82b8c2ad3e5cde7cad659be2d37c251e/5846-end-to-end-memory-networks.pdf。</a></p>
<p>当然，也可以动手实现一个网络结构，参考代码：<a href="https://github.com/princewen/tensorflow_practice/tree/master/nlp/Basic-EEMN-Demo" target="_blank" rel="external">https://github.com/princewen/tensorflow_practice/tree/master/nlp/Basic-EEMN-Demo</a></p>
<h2 id="3、协同记忆网络原理"><a href="#3、协同记忆网络原理" class="headerlink" title="3、协同记忆网络原理"></a>3、协同记忆网络原理</h2><p>我们的协同记忆网络CMN其实借鉴了End2End Memory Network的思路，我们先来看一下完整的网络结构，随后一步步进行介绍：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-d5b37b7bd4974a8e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>这里明确一点，我们的任务是预测用户u对于物品i的评分。</p>
<h4 id="3-1-User-Embedding"><a href="#3-1-User-Embedding" class="headerlink" title="3.1 User Embedding"></a>3.1 User Embedding</h4><p>首先，我们会有<strong>两组用户的Memory(其实就是Embedding)，分别是M和C，M用于计算用户之间关于某件物品i的相关性，C用于最终的输出向量。我们还有一组物品的Memory(其实就是Embedding)，我们称作E。</strong></p>
<p>对于预测用户u对于物品i的评分。我们首先会得到历史上所有跟物品i有反馈的用户集合，我们称作N(i)。接下来，我们要计算目标用户u和N(i)中每个用户的相关性，基于下面的公式：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-56ada6caabfb4da4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>其中，m<sub>u</sub>,m<sub>v</sub>分别是用户u和用户v在M中的相关记忆。e<sub>i</sub>代表物品i在E中的相关记忆。</p>
<h4 id="3-2-Neighborhood-Attention"><a href="#3-2-Neighborhood-Attention" class="headerlink" title="3.2 Neighborhood Attention"></a>3.2 Neighborhood Attention</h4><p>对于上一步计算出的相关性，我们需要通过一个softmax操作转换为和为1的权重向量：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-ee2d810640b21c66.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>同时，根据得到的权重向量，我们根据下面的式子得到输出向量：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-186018ac997bda9c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>其中，c<sub>v</sub>代表用户v在C中的相关记忆。</p>
<h4 id="3-3-Output-Module"><a href="#3-3-Output-Module" class="headerlink" title="3.3 Output Module"></a>3.3 Output Module</h4><p>最终的预测输出为：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-a8b805960f4ad836.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>可以看到，在上面的式子中，括号中的第一项是用户的记忆m<sub>u</sub>和物品的记忆e<sub>i</sub>进行的element-wise 相乘操作，这相当于矩阵分解的思想，即考虑了全局的信息。第二项相当于是按照基于邻域的思路得到的一个输出向量，即考虑了局部的相关用户的信息。最终经过激活函数φ和输出层得到最终的预测评分。因此，CMN不仅考虑了全局的结构信息，还考虑了局部的相关用户的信息。</p>
<h4 id="3-4-Multiple-Hops"><a href="#3-4-Multiple-Hops" class="headerlink" title="3.4 Multiple Hops"></a>3.4 Multiple Hops</h4><p>CMN模型可以扩展为多层。在每一层，我们的记忆是不变的，变化的主要是权重向量。每一层的权重向量计算公式如下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-7997eb0e11c4f847.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>这里冒出了一个z<sup>h</sup><sub>ui</sub>，初始的z为：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-e0caf8d85815c28b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>那么这就跟我们单层的CMN网络中的计算方式是一样的，因为内积的计算是符合乘法分配律的。</p>
<p>接下来每一层的z，计算公式如下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-a63feefb22ef262b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<h4 id="3-5-Parameter-Estimation"><a href="#3-5-Parameter-Estimation" class="headerlink" title="3.5 Parameter Estimation"></a>3.5 Parameter Estimation</h4><p>论文中CMN网络采取的是一个<strong>pair-wise的方式</strong>，训练时每次输入一个正样本得到一个评分，输入一个负样本得到一个评分，我们希望正样本的得分远大于负样本的得分。这种形式我们称为<strong>Bayesian Personalized Ranking (BPR) optimization criterion</strong>：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-64d00f3dd1a96efa.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>好了，模型的原理我们就介绍到这里了，接下来我们看一下代码实现吧。</p>
<h2 id="4、CMN模型的tensorflow实现"><a href="#4、CMN模型的tensorflow实现" class="headerlink" title="4、CMN模型的tensorflow实现"></a>4、CMN模型的tensorflow实现</h2><p>本文的代码参考地址为：<a href="https://github.com/tebesu/CollaborativeMemoryNetwork" target="_blank" rel="external">https://github.com/tebesu/CollaborativeMemoryNetwork</a></p>
<p>代码文件结构为：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-e20d29f3b24661a4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>该Demo中的部分神经网络结构由dm-sonnet进行了封装，可以通过 pip install dm-sonnet进行安装，默认进行安装的话，tensorflow的版本应该至少在1.8以上。</p>
<p>有关代码的实现细节，本文就不再介绍了，感兴趣的话，大家可以看一下这个Demo。</p>
<h2 id="参考文献-5"><a href="#参考文献-5" class="headerlink" title="参考文献"></a>参考文献</h2><p>1、《End-To-End Memory Networks》：<a href="http://10.3.200.202/cache/11/03/papers.nips.cc/82b8c2ad3e5cde7cad659be2d37c251e/5846-end-to-end-memory-networks.pdf" target="_blank" rel="external">http://10.3.200.202/cache/11/03/papers.nips.cc/82b8c2ad3e5cde7cad659be2d37c251e/5846-end-to-end-memory-networks.pdf</a> ）<br>2、《MEMORY NETWORKS》：<a href="https://arxiv.org/pdf/1410.3916.pdf" target="_blank" rel="external">https://arxiv.org/pdf/1410.3916.pdf</a><br>3、<a href="https://zhuanlan.zhihu.com/c_129532277" target="_blank" rel="external">https://zhuanlan.zhihu.com/c_129532277</a></p>
<h1 id="推荐系统遇上深度学习-三十-–深度矩阵分解模型理论及实践"><a href="#推荐系统遇上深度学习-三十-–深度矩阵分解模型理论及实践" class="headerlink" title="推荐系统遇上深度学习(三十)–深度矩阵分解模型理论及实践"></a>推荐系统遇上深度学习(三十)–深度矩阵分解模型理论及实践</h1><p>本篇为推荐系统遇上深度学习系列的第30篇文章，也是2019年以来的第一篇文章，2019年希望该系列能够到50篇！加油！</p>
<p>本文提出了一种基于神经网络结构的矩阵分解模型。该模型综合考虑了用户对物品的显式评分和非偏好隐式反馈，然后通过两组神经网络将用户和物品的特征提取到一个低维空间；并通过设计的新的损失函数进行反向学习。本文设计的新损失函数将显式反馈加入二元交叉熵损失函数中，称为归一化交叉熵损失。实验证明该模型在几个典型数据集上相对于其他经典模型表现更好。</p>
<p>论文题目为：《Deep Matrix Factorization Models for Recommender Systems》<br>论文地址为：<a href="https://www.ijcai.org/proceedings/2017/0447.pdf" target="_blank" rel="external">https://www.ijcai.org/proceedings/2017/0447.pdf</a></p>
<p><strong>本文提出的模型名称为Deep Matrix Factorization Models，下文我们简称DMF</strong></p>
<h2 id="1、DMF原理介绍"><a href="#1、DMF原理介绍" class="headerlink" title="1、DMF原理介绍"></a>1、DMF原理介绍</h2><h4 id="1-1-问题陈述"><a href="#1-1-问题陈述" class="headerlink" title="1.1 问题陈述"></a>1.1 问题陈述</h4><p>假设我们有M个用户以及N个物品，R是M*N的评分矩阵，R<sub>ij</sub>表示用户i对于物品j的评分。在实际中，我们有两种方式来构造用户-物品交互矩阵Y(实际中用于训练的矩阵)：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-869814388cc1d3de.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>大部分现有的方案中，使用(1)来构建交互矩阵Y，但本文使用(2)来构建交互矩阵。如果用户i对物品j有评分，那么Y<sub>ij</sub>=R<sub>ij</sub>，此时反馈被称为显式反馈。否则的话，Y<sub>ij</sub>=0，此时我们称这种反馈为<strong>非偏好隐式反馈(non-preference implicit feedback)</strong>(个人理解是评分为0不代表用户不喜欢，可能是用户没有接触过该物品)。</p>
<p>推荐系统的任务往往是对交互矩阵中未知的部分进行评分，随后对评分的结果进行排序，我们假定评分通过一个模型来得到：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-c845356f44ac81ff.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>如何定义一个F呢？举例来说，隐语义模型（LFM）简单地应用p<sub>i</sub>，q<sub>j</sub>的点积来预测:</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-16a803537394b95a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>那么，DMF是如何构建F的呢？在介绍之前，我们先介绍一下一些有用的符号：<br>Y：交互矩阵<br>Y<sup>+</sup>：Y中观测到的交互，即显式反馈<br>Y<sup>-</sup>：Y中的全部0元素<br>Y<sup>-</sup><sub>sampled</sub>：负样本，可以是Y<sup>-</sup>，也可以是Y<sup>-</sup>的部分采样<br>Y<sup>+</sup> ∪ Y<sup>-</sup><sub>sampled</sub>：所有训练样本<br>Y<sub>i*</sub>：交互矩阵中的第i行，表示用户i对所有物品的评分<br>Y<sub>*j</sub>：交互矩阵中的第j列，表示所有用户对物品j的评分</p>
<h4 id="2-2-DMF模型"><a href="#2-2-DMF模型" class="headerlink" title="2.2 DMF模型"></a>2.2 DMF模型</h4><p>DMF模型框架如下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-e8b0b2ee1cc9ed82.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>可以看到，为了预测用户i对物品j的评分，用户i的输入是交互矩阵中的第i行Y<sub>i*</sub>、物品j的输入是交互矩阵中的第j列Y<sub>*j</sub>。两部分的输入分别经过两个多层神经网络得到向量p<sub>i</sub>和q<sub>j</sub>，即用户i和物品j的隐向量表示。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-5765082045798292.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>随后，通过p<sub>i</sub>和q<sub>j</sub>的余弦距离得到预测评分：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-463bebec02124b10.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>论文中指出，这是第一次直接使用交互矩阵作为表示学习的输入。正如我们前面提到的，Y<sub>i*</sub>代表用户对所有物品的评分。它可以在某种程度上表明用户的全局偏好。Y<sub>*j</sub>代表了一个物品上所有用户评价。它可以在某种程度上表明一个物品的概要。论文中认为用户和项目的这些表示对于最终的低维表示非常有用。</p>
<h4 id="2-3-损失函数设计"><a href="#2-3-损失函数设计" class="headerlink" title="2.3 损失函数设计"></a>2.3 损失函数设计</h4><p>对于推荐系统来说，主要有两种目标函数，一是point-wise的，二是pair-wise的，本文使用point-wise的目标函数。目标函数中最重要的部分就是损失函数。</p>
<p>在许多现有的模型中，对于显式反馈，使用平方损失函数：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-20b1538463d23c9d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>但是平方损失函数对于隐式反馈来说并不是十分有效。隐式反馈通常被视作0-1二分类问题，通常使用交叉熵损失函数：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-99c9aef032b1a478.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>本文构造的交互矩阵，既有显示评分，也有隐式反馈。而平方损失关注显式评分，而交叉熵损失则关注隐式反馈。所以本文提出了归一化交叉熵损失函数，将显式的评分合并到交叉熵损失中。该损失函数的计算公式如下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-91a2d1691cb61f6e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>max(R)是所有评分中的最大值。还有一个注意的点是，预测值不能是负数，所以我们要对负数进行处理，使其变为一个非常小的数：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-9172bcbf97a0afbe.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>综上，DMF的过程如下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-519f2de7d6fb2306.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<h2 id="2、DMF代码实现"><a href="#2、DMF代码实现" class="headerlink" title="2、DMF代码实现"></a>2、DMF代码实现</h2><p>代码地址为：<a href="https://github.com/princewen/tensorflow_practice/tree/master/recommendation/Basic-DMF-Model" target="_blank" rel="external">https://github.com/princewen/tensorflow_practice/tree/master/recommendation/Basic-DMF-Model</a></p>
<p>数据为movieslen-1m的数据：<a href="https://grouplens.org/datasets/movielens/" target="_blank" rel="external">https://grouplens.org/datasets/movielens/</a></p>
<p>代码实现非常简单，这里我们简要介绍一下模型的构建过程：</p>
<p><strong>定义placeholder</strong><br>placeholde主要有用户id、物品id、评分以及dropout的值：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">def add_placeholders(self):</div><div class="line">    self.user = tf.placeholder(tf.int32)</div><div class="line">    self.item = tf.placeholder(tf.int32)</div><div class="line">    self.rate = tf.placeholder(tf.float32)</div><div class="line">    self.drop = tf.placeholder(tf.float32)</div></pre></td></tr></table></figure>
<p><strong>构造embedding</strong><br>由于我们直接使用交互矩阵中的一行或者一列作为输入，因此，用户的embedding是交互矩阵，而物品的embedding是交互矩阵的转置。下面代码中self.dataSet.getEmbedding()便是交互矩阵：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">def add_embedding_matrix(self):</div><div class="line">    self.user_item_embedding = tf.convert_to_tensor(self.dataSet.getEmbedding())</div><div class="line">    self.item_user_embedding = tf.transpose(self.user_item_embedding)</div></pre></td></tr></table></figure>
<p>接下来，便可以通过embedding_lookup函数来获取模型的输入：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">user_input = tf.nn.embedding_lookup(self.user_item_embedding, self.user)</div><div class="line">item_input = tf.nn.embedding_lookup(self.item_user_embedding, self.item)</div></pre></td></tr></table></figure>
<p><strong>两部分全连接网络</strong><br>对于得到的user_input和item_input，通过两部分全连接网络得到隐向量表示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">with tf.name_scope(&quot;User_Layer&quot;):</div><div class="line">    user_W1 = init_variable([self.shape[1], self.userLayer[0]], &quot;user_W1&quot;)</div><div class="line">    user_out = tf.matmul(user_input, user_W1)</div><div class="line">    for i in range(0, len(self.userLayer)-1):</div><div class="line">        W = init_variable([self.userLayer[i], self.userLayer[i+1]], &quot;user_W&quot;+str(i+2))</div><div class="line">        b = init_variable([self.userLayer[i+1]], &quot;user_b&quot;+str(i+2))</div><div class="line">        user_out = tf.nn.relu(tf.add(tf.matmul(user_out, W), b))</div><div class="line"></div><div class="line">with tf.name_scope(&quot;Item_Layer&quot;):</div><div class="line">    item_W1 = init_variable([self.shape[0], self.itemLayer[0]], &quot;item_W1&quot;)</div><div class="line">    item_out = tf.matmul(item_input, item_W1)</div><div class="line">    for i in range(0, len(self.itemLayer)-1):</div><div class="line">        W = init_variable([self.itemLayer[i], self.itemLayer[i+1]], &quot;item_W&quot;+str(i+2))</div><div class="line">        b = init_variable([self.itemLayer[i+1]], &quot;item_b&quot;+str(i+2))</div><div class="line">        item_out = tf.nn.relu(tf.add(tf.matmul(item_out, W), b))</div></pre></td></tr></table></figure>
<p><strong>得到预测评分</strong><br>预测评分通过计算用户和物品隐向量的余弦距离得到，还要将负数变为一个极小的正数：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">norm_user_output = tf.sqrt(tf.reduce_sum(tf.square(user_out), axis=1))</div><div class="line">norm_item_output = tf.sqrt(tf.reduce_sum(tf.square(item_out), axis=1))</div><div class="line">self.y_ = tf.reduce_sum(tf.multiply(user_out, item_out), axis=1, keep_dims=False) / (norm_item_output* norm_user_output)</div><div class="line">self.y_ = tf.maximum(1e-6, self.y_)</div></pre></td></tr></table></figure>
<p><strong>损失函数</strong><br>我们使用归一化的交叉熵损失函数：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">def add_loss(self):</div><div class="line">    regRate = self.rate / self.maxRate</div><div class="line">    losses = regRate * tf.log(self.y_) + (1 - regRate) * tf.log(1 - self.y_)</div><div class="line">    loss = -tf.reduce_sum(losses)</div><div class="line">    # regLoss = tf.add_n([tf.nn.l2_loss(v) for v in tf.trainable_variables()])</div><div class="line">    # self.loss = loss + self.reg * regLoss</div><div class="line">    self.loss = loss</div></pre></td></tr></table></figure>
<p>好了，代码就介绍到这里了，小伙伴们可以自己动手实现一下，非常的简单。再次祝大家学业顺利，开工大吉！</p>
<h1 id="推荐系统遇上深度学习-三十一-–使用自注意力机制进行物品推荐"><a href="#推荐系统遇上深度学习-三十一-–使用自注意力机制进行物品推荐" class="headerlink" title="推荐系统遇上深度学习(三十一)–使用自注意力机制进行物品推荐"></a>推荐系统遇上深度学习(三十一)–使用自注意力机制进行物品推荐</h1><p>论文名称：《Next Item Recommendation with Self-Attention》<br>论文地址：<a href="https://arxiv.org/abs/1808.06414?context=cs" target="_blank" rel="external">https://arxiv.org/abs/1808.06414?context=cs</a></p>
<p>在这篇文章中，我们将介绍一种基于self-attention 的序列推荐算法，该算法使用 self-attention 从用户短期的交互记录中学习用户近期的兴趣，同时该模型也使用度量学习的方式保留了用户的长久的兴趣。</p>
<p>整个网络是在<strong>度量学习(metric learning)</strong>的框架下进行训练，实验表明该方法可以在很大程度上改善序列化推荐的效果。接下来，我们就一探究竟。</p>
<h2 id="1、为什么要用自注意力机制？"><a href="#1、为什么要用自注意力机制？" class="headerlink" title="1、为什么要用自注意力机制？"></a>1、为什么要用自注意力机制？</h2><p>推荐系统中，很多情况下我们使用用户的历史交互数据进行推荐，比如点击数据、浏览数据、购买数据等等。使用这些交互数据进行推荐，我们可以把推荐问题当作一个序列问题，即通过历史交互中的物品序列来预测用户下一个可能发生交互的物品。</p>
<p>既然是序列问题，常用的解法主要有RNN和CNN。RNN中，相邻的物品之间的关系可以被捕获，而物品之间的长期关系被保存在隐层状态中；在CNN中，通过卷机核的窗口滑动来捕获物品之间的关系。但是，这两种方式都没有显式地建模物品之间的关系，而建模这种可能存在的关系是很有必要的。</p>
<p>这里写一下个人的理解，不一定是对的，求大佬轻喷。使用自注意力机制可能得到的结果类似于聚类，相似的物品之间相关性权重高，加权的结果使得它们在空间中的距离越来越近。举个例子，假设用户最近看过的10部电影中有5部科幻电影，5部爱情电影。科幻电影之间的相关性较高，极端一点，均为0.2，假设五部电影对应的向量为n1,n2,n3,n4,n5，那么n1 = 1/5(n1+n2+n3+n4+n5)，n2 = 1/5(n1+n2+n3+n4+n5)，..,n5 = 1/5(n1+n2+n3+n4+n5)，最终的结果就是n1=n2=n3=n4=n5。当然上面的只是极端情况，实际中很难实现，不过只是想通过这个例子说明一下个人的理解。</p>
<p>接下来，我们介绍一下模型的原理。</p>
<h2 id="2、模型原理"><a href="#2、模型原理" class="headerlink" title="2、模型原理"></a>2、模型原理</h2><h4 id="2-1-问题定义"><a href="#2-1-问题定义" class="headerlink" title="2.1 问题定义"></a>2.1 问题定义</h4><p>序列推荐问题的定义如下，假设我们有用户的集合<strong><em>U</em></strong>和物品的集合<strong><em>I</em></strong>，用户的总数为M，物品的总数为N，使用<strong>H<sup>u</sup></strong>表示用户的交互序列：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-9812d3757fc86399.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>我们的目标是在给定user交互序列的情况下，预测user下一个将要进行交互的item。</p>
<h4 id="2-2-使用自注意力机制建模短期兴趣"><a href="#2-2-使用自注意力机制建模短期兴趣" class="headerlink" title="2.2 使用自注意力机制建模短期兴趣"></a>2.2 使用自注意力机制建模短期兴趣</h4><p>用户近期的交互行为反映了用户的近期兴趣，这里使用自注意力机制来进行建模。如下图所示：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-58f38b24cd9de620.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>这里，我们假定使用用户最近的L条记录来计算短期兴趣。如果我们使用<strong>X</strong>表示整个物品集合的embedding，那么，用户u在t时刻的前L条交互记录所对应的embedding表示如下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-287849dd019eedd7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>X<sup>u</sup><sub>t</sub>也将作为transformer 中block的输入。接下来，就是自注意力机制的计算过程 :</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-76667dddc59dcc68.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>对于上面的过程不清楚的同学，可以看一下参考之前的博客：<a href="https://www.jianshu.com/p/b1030350aadb" target="_blank" rel="external">https://www.jianshu.com/p/b1030350aadb</a></p>
<p>这里，作者做了以下几点处理：<br>1、首先，作者还在计算softmax前先掩掉Q′K′<sup>T</sup>得到的矩阵对角线值，以避免对角线上物品自身的内积分配过大的权重；<br>2、其次，作者直接使用输入的X<sup>u</sup><sub>t</sub>与attention score相乘，而不是先计算一个V′，使用V′与attention score相乘；<br>3、最后，作者在embedding乘上attention权重后，直接将embedding在序列维度求平均，作为用户短期兴趣向量；</p>
<p>除上面的基础结构外，作者还提到了可以增加poistional embeddings，这主要是给输入加入时间信号。时间信号的计算如下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-41f583980fdb3bc7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>时间信号的大小跟Q′和K′的形状是一样的，二者可以直接对位相加。</p>
<h4 id="2-3-长期兴趣建模"><a href="#2-3-长期兴趣建模" class="headerlink" title="2.3 长期兴趣建模"></a>2.3 长期兴趣建模</h4><p>上面的self-attention模块只使用了近期的L个交互，我们还希望建模用户的长期兴趣，不同于之前的矩阵分解，认为用户和物品属于两个不同的空间，这里我们把用户和物品放入同一个空间中。我们可以认为这是一个兴趣空间，用户的长期兴趣可以表示成空间中的一个向量，而一个物品也可以想象成是一种兴趣，同样映射到这个兴趣空间中的一个向量。那么，如果一个用户对一个物品的评分高的话，说明两个兴趣是相近的，那么它们对应的向量在空间中距离应该较近。这个距离用平方距离计算：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-91ceeabb5ea82986.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>上式中，<strong>U</strong>是用户的兴趣向量，<strong>V</strong>是物品的兴趣向量，这与之前定义的<strong>X</strong>是物品的两种表示。</p>
<h4 id="2-4-模型训练"><a href="#2-4-模型训练" class="headerlink" title="2.4 模型训练"></a>2.4 模型训练</h4><p>综合短期兴趣和长期兴趣，模型的整体架构如下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-dccd3f4c564bbfc8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>综合两部分，我们便可以得到用户对于某个物品的推荐分，这里推荐分越低，代表用户和物品越相近，用户越可能与该物品进行交互：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-85c79c0aa6170a8c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>而模型采用pair-wise的训练方法进行训练，即输入一个正例和一个负例，希望负例的得分至少比正例高γ，否则就发生损失。所以损失函数是如下的加入L2正则项的合页损失函数：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-0dc44e0c9339c622.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>好啦，模型部分就介绍到这里啦，总体来说，模型的思想还是比较简单的，综合了用户的短期兴趣和长期兴趣。比较创新的一点就是使用了自注意力机制来表示物品之间的相关性吧。</p>
<p>由这篇论文也可以看出，自注意力机制、Transformer不仅仅在NLP领域得到应用，推荐系统领域也开始尝试，所以学好这个模型是十分必要的呀！</p>
<h1 id="推荐系统遇上深度学习-三十二-–《推荐系统实践》思维导图"><a href="#推荐系统遇上深度学习-三十二-–《推荐系统实践》思维导图" class="headerlink" title="推荐系统遇上深度学习(三十二)–《推荐系统实践》思维导图"></a>推荐系统遇上深度学习(三十二)–《推荐系统实践》思维导图</h1><p><img src="https://upload-images.jianshu.io/upload_images/4155986-611ba7e51f45d9dd.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>本文是项亮《推荐系统实践》一书的思维导图，这本书介绍了推荐系统中最基本的方法、冷启动问题及解决方案、如何利用标签、上下文信息以及社交网络数据进行推荐等内容，对想要了解推荐系统的同学来讲，算是一个比较好的入门作品。</p>
<p>1、推荐系统基础</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-07cc0a61db6b564c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1000" alt=""></p>
<p>2、利用用户行为数据进行推荐<br><img src="https://upload-images.jianshu.io/upload_images/4155986-82e45877838853f5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1000" alt=""></p>
<p>3、冷启动问题<br><img src="https://upload-images.jianshu.io/upload_images/4155986-15ea4557b870b6d2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1000" alt=""></p>
<p>4、利用标签数据进行推荐</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-05842d10c21e0bf5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1000" alt=""></p>
<p>5、利用上下文信息进行推荐</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-8a5a65ea7f51b1d5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1000" alt=""></p>
<p>6、利用社交网络数据进行推荐</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-1e4af32d70add8a1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1000" alt=""></p>
<p>7、评分预测问题</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-1d1c145fa1a4d91d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1000" alt=""></p>
<h1 id="推荐系统遇上深度学习-三十三-–Neural-Attentive-Item-Similarity-Model"><a href="#推荐系统遇上深度学习-三十三-–Neural-Attentive-Item-Similarity-Model" class="headerlink" title="推荐系统遇上深度学习(三十三)–Neural-Attentive-Item-Similarity-Model"></a>推荐系统遇上深度学习(三十三)–Neural-Attentive-Item-Similarity-Model</h1><p>论文名称：《NAIS: Neural Attentive Item Similarity Model for Recommendation》<br>论文地址：<a href="https://arxiv.org/pdf/1809.07053.pdf" target="_blank" rel="external">https://arxiv.org/pdf/1809.07053.pdf</a></p>
<p>基于物品的协同过滤ItemCF是推荐领域常用的方法，其关键是评估item之间的相似性。本文将要介绍Neural Attentive Item Similarity Model(简称NASI)来解决ItemCF问题。该模型将注意力机制和神经网络相结合，提升了模型的预测准确性。接下来，我们将从基本的ItemCF问题入手，一步步得出NASI模型。</p>
<h2 id="1、ItemCF问题简介"><a href="#1、ItemCF问题简介" class="headerlink" title="1、ItemCF问题简介"></a>1、ItemCF问题简介</h2><h4 id="1-1-标准ItemCF问题"><a href="#1-1-标准ItemCF问题" class="headerlink" title="1.1 标准ItemCF问题"></a>1.1 标准ItemCF问题</h4><p>为了预测用户u对于物品i的评分，ItemCF的最基本思想是计算物品i与用户u之前交互过的所有物品的相似性，预测评分计算公式如下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-aecfa7422423ddbd.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>其中，R<sub>u</sub><sup>+</sup>是用户所有交互过的物品，r<sub>uj</sub>是用户u对物品j的反馈，s<sub>ij</sub>是物品i和物品j的相似性。其中，r<sub>uj</sub>可以是显式的评分，如0-5评分，也可以是隐式的反馈，如点击为1，未点击为0。</p>
<p>物品之间的相似性，直观的方法是根据用户-物品交互矩阵，将物品i所在的列作为其向量表示，进一步使用余弦相似度等度量方式计算物品之间的相似性。但是这种方法缺乏针对推荐的优化，类似于一种静态方法，物品的向量不是通过优化得到的。因此性能并不是那么优秀。所以接下来我们将介绍Learning-based Methods，这些方法可以自适应地从数据中学习item相似度，从而提高itemCF的准确性。</p>
<h4 id="1-2-Learning-based-Methods-for-Item-based-CF"><a href="#1-2-Learning-based-Methods-for-Item-based-CF" class="headerlink" title="1.2 Learning-based Methods for Item-based CF"></a>1.2 Learning-based Methods for Item-based CF</h4><p>Learning-based Methods通过优化一个目标函数，来学习item之间的相似性，如SLIM(short for sparse Linear Method)方法中，目标函数设定为：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-c424f27784e71b0e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>上面的目标函数中，S代表物品的相似度矩阵。而预测评分的计算仍然基于1.1中的方法。假设物品个数为I，那么模型需要优化的参数有I * I个。上面的式子同时使用L1正则和L2正则，防止了过拟合，增加了模型的稀疏性。但也存在一定的缺点，当物品集数量很大时，参数太多难以优化，同时，模型只能学习同时被打过分的物品之间的两两的相似性。</p>
<p>为了解决这个问题，我们又有了FISM(short for factored item similarity model)方法，其用低维度嵌入向量表示每一个物品。对于每一个物品，都有两个嵌入向量p和q，当物品是预测的物品时，使用p，当物品是交互历史中的物品时，使用q，此时用户评分计算方式如下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-447ab0826310a5c1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>FISM只考虑隐式反馈。对于上面的预测模型，可以通过优化推荐的标准损失（对数损失或者平方损失）来学习物品的嵌入向量表示p和q。</p>
<p>虽然FISM方法取得了不错的性能，但我们认为，当获得用户的表示时，它对用户的所有历史项目的平等处理会限制其表示能力。因此，我们将注意力机制加入其中，用于区分历史item的重要性，提出了NASI模型。</p>
<h2 id="2、NASI模型介绍"><a href="#2、NASI模型介绍" class="headerlink" title="2、NASI模型介绍"></a>2、NASI模型介绍</h2><p>这里，我们仍然只考虑隐式反馈，模型设计过程如下：</p>
<h4 id="2-1-第一版"><a href="#2-1-第一版" class="headerlink" title="2.1 第一版"></a>2.1 第一版</h4><p>在第一版的设计中，我们认为每个物品有一个固定的注意力权重a<sub>j</sub>，因此评分预测计算如下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-033b09deeaa95aa8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>很显然，这是不合理的，我们没有考虑目标物品对于历史物品的影响。于是我们有了第二版设计。</p>
<h4 id="2-2-第二版"><a href="#2-2-第二版" class="headerlink" title="2.2 第二版"></a>2.2 第二版</h4><p>在第二版的设计中，我们使用a<sub>ij</sub>来表示历史物品j和目标物品i的权重，评分计算如下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-75761ad19aa5275b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>这样显然也是有缺陷的，当训练集中物品i和物品j没有同时出现过时，a<sub>ij</sub>是无从学习的。于是我们有了第三版设计。</p>
<h4 id="2-3-第三版"><a href="#2-3-第三版" class="headerlink" title="2.3 第三版"></a>2.3 第三版</h4><p>第三版设计中，我们使用嵌入向量计算出权重，即：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-09f70538d959fd6b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>f通常用一个神经网络来表示，主流的计算方法有以下两种：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-53fa30b3e107732f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>通过f计算出的权重，我们还需要通过softmax进行归一化，因此，评分预测的计算如下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-bf56007acf08cdc2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>这么做看似是完美了，但是在实际的实验中，却没有取得理想的效果。这个问题主要来自softmax，在传统的注意力机制使用的场景中，如CV、NLP中，注意力机制的长度变化不是很大(这里的长度指图像中的区域个数，句子中单词的个数等等)，但是在推荐领域中，用户的历史交互长度可能变化很大。在MovieLens和Pinnterest数据中，用户历史长度的分布如下图所示：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-1e8ed7eda16500ba.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>可以看到，对于两个真实数据集，用户的历史长度变化很大，具体而言，用户历史长度的均值和方差分别为（166，37145）,（27，572）。在MovieLens数据集中，所有用户的平均长度为166，最大长度为2313。也就是说，最活跃用户的平均注意力权重是1/2313，比平均用户（即，1/166）少大约14倍。如此大的注意权重差异将导致优化模型的item嵌入是个问题。（可以简单的想，同样的物品i和物品j，在活跃用户和非活跃用户中得到的a<sub>ij</sub>差异会非常大）</p>
<p>为了解决用户历史长度不同的问题，我们便有了最终版的NAIS模型。</p>
<h4 id="2-4-最终版"><a href="#2-4-最终版" class="headerlink" title="2.4 最终版"></a>2.4 最终版</h4><p>在最终版的模型中，我们对活跃用户的注意力权重进行一定的惩罚，如下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-8e936ec49457a933.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>而模型的损失函数使用对数损失+L2正则：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-7f7630edc0692294.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>模型的框架图如下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-4cf3f6c03626825c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>好了，模型介绍就到这里了，关于模型中的一些细节，大家可以阅读原论文。</p>
<h2 id="3、NASI代码实现"><a href="#3、NASI代码实现" class="headerlink" title="3、NASI代码实现"></a>3、NASI代码实现</h2><p>作者给出了Python2版本的代码：<a href="https://github.com/AaronHeee/Neural-Attentive-Item-Similarity-Model" target="_blank" rel="external">https://github.com/AaronHeee/Neural-Attentive-Item-Similarity-Model</a></p>
<p>这个代码在Python3中是无法运行的，主要是Python3中range函数得到的不是list，需要使用list()函数进行转换，Python3版本的代码地址：<a href="https://github.com/princewen/tensorflow_practice/tree/master/recommendation/Basic-NAIS-Demo" target="_blank" rel="external">https://github.com/princewen/tensorflow_practice/tree/master/recommendation/Basic-NAIS-Demo</a></p>
<h1 id="推荐系统遇上深度学习-三十四-–YouTube深度学习推荐系统"><a href="#推荐系统遇上深度学习-三十四-–YouTube深度学习推荐系统" class="headerlink" title="推荐系统遇上深度学习(三十四)–YouTube深度学习推荐系统"></a>推荐系统遇上深度学习(三十四)–YouTube深度学习推荐系统</h1><p>看题目，相信大家都知道本文要介绍的便是经典的Youtube的深度学习推荐系统论文<strong>《Deep Neural Networks for YouTube Recommendations》</strong>，如果你之前已经读过该文章，那我们一起来回顾讨论一下；如果你没有读过这个文章，希望本文能够起到导读的作用，能够帮助你更好的理解文章！</p>
<h2 id="1、引言-6"><a href="#1、引言-6" class="headerlink" title="1、引言"></a>1、引言</h2><p>youtube是世界上最大的视频内容平台，在如此体量的平台中，推荐系统是至关重要的。但是，youtube的视频推荐面临三方面的挑战：<br>1）<strong>Scale</strong>：视频和用户数量巨大，很多现有的推荐算法能够在小的数据集上表现得很好，但是在这里效果不佳。需要构建高度专业化的分布式学习算法和高效的服务系统来处理youtube庞大的用户和视频数量。<br>2）<strong>Freshness</strong>：这体现在两方面，一方面视频更新频繁，另一方面用户行为更新频繁。<br>3）<strong>Noise</strong>：相较于庞大的视频库，用户的行为是十分稀疏的，同时，我们基本上能获得的都是用户的隐式反馈信号。构造一个强健的系统是十分困难的。</p>
<p>面临如此多的挑战，youtube是如何搭建自己的推荐系统的呢？我们一起来看看。</p>
<h2 id="2、Youtube推荐系统-整体架构"><a href="#2、Youtube推荐系统-整体架构" class="headerlink" title="2、Youtube推荐系统 整体架构"></a>2、Youtube推荐系统 整体架构</h2><p>Youtube推荐系统的整体架构如下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-cb5032986cde24f7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>由于网站视频数量太多，视频候选集太大，不宜用复杂网络直接进行推荐，这会造成响应时间的增加。因此，整个架构走粗排 + 精排两阶段的路子：</p>
<p><strong>Candidate Generation Model</strong>：在这一步，从成百上千万的视频中选择百量级的候选视频</p>
<p><strong>Ranking Model</strong>：这一步，完成对几百个候选视频的精排。</p>
<p>接下来，我们介绍这两阶段的实现细节。</p>
<h2 id="3、候选集生成Candidate-Generation"><a href="#3、候选集生成Candidate-Generation" class="headerlink" title="3、候选集生成Candidate Generation"></a>3、候选集生成Candidate Generation</h2><p>Candidate Generation阶段，会从巨大的视频库中挑选几百个用户可能感兴趣的候选集。模型的结构如下图所示：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-1104f822da782760.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>关于该架构，我们从以下几个方面进行讨论：</p>
<h4 id="3-1-输入特征"><a href="#3-1-输入特征" class="headerlink" title="3.1 输入特征"></a>3.1 输入特征</h4><p>可以看到，模型的输入包括用户的观看过的视频的embedding，用户搜索过的token的embedding，用户的地理信息embedding，用户的年龄和性别信息。</p>
<p>这里有一个很有意思并且值得我们深思的特征，被称为<strong>“Example Age”</strong>。我们知道，每一秒中，YouTube都有大量视频被上传，推荐这些最新视频对于YouTube来说是极其重要的。同时，通过观察历史数据发现，用户更倾向于推荐那些尽管相关度不高但是是最新（fresh）的视频。看论文的图片，我们可能认为该特征表示视频被上传之后距现在的时间。但文章其实没有定义这个特征是如何获取到的，应该是训练时间-Sample Log的产生时间。而在线上服务阶段，该特征被赋予0值甚至是一个比较小的负数。这样的做法类似于在广告排序中消除position bias。</p>
<p>假设这样一个视频十天前发布的，许多用户在当前观看了该视频，那么在当天会产生许多Sample Log，而在后面的九天里，观看记录不多，Sample Log也很少。如果我们没有加入Example Age这个特征的话，无论何时训练模型，这个视频对应的分类概率都是差不多的，但是如果我们加入这个特征，模型就会知道，如果这条记录是十天前产生的话，该视频会有很高的分类概率，如果是最近几天产生的话，分类概率应该低一些，这样可以更加逼近实际的数据。实验结果也证明了这一点，参见下图：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-0d60b64d050ee97b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<h4 id="3-2-样本和上下文选择"><a href="#3-2-样本和上下文选择" class="headerlink" title="3.2 样本和上下文选择"></a>3.2 样本和上下文选择</h4><p>在这里，正样本是用户所有完整观看过的视频，其余可以视作负样本。</p>
<p>训练样本是从Youtube所有的用户观看记录里产生的，而并非只是通过推荐系统产生的。同时，针对每一个用户的观看记录，都生成了固定数量的训练样本，这样，每个用户在损失函数中的地位都是相等的，防止一小部分超级活跃用户主导损失函数。</p>
<p>在对待用户的搜索历史或者观看历史时，可以看到Youtube并没有选择时序模型，而是完全摒弃了序列关系，采用求平均的方式对历史记录进行了处理。这是因为考虑时序关系，用户的推荐结果将过多受最近观看或搜索的一个视频的影响。文章中给出一个例子，如果用户刚搜索过“tayer swift”，你就把用户主页的推荐结果大部分变成tayer swift有关的视频，这其实是非常差的体验。为了综合考虑之前多次搜索和观看的信息，YouTube丢掉了时序信息，讲用户近期的历史纪录等同看待。但是上述仅是经验之谈，也许类似阿里深度学习演化网络中RNN + Attention的方法，能够取得更好的推荐效果。</p>
<p>最后，在处理测试集的时候，YouTube没有采用经典的随机留一法（random holdout），而是把用户最近的一次观看行为作为测试集，如下图。这主要是避免引入超越特征。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-2375782eb9cd88de.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<h4 id="3-3-离线训练"><a href="#3-3-离线训练" class="headerlink" title="3.3 离线训练"></a>3.3 离线训练</h4><p>从模型结构可以看出，在离线训练阶段，我们将其视为了一个分类问题。我们使用隐式反馈来进行学习，用户完整观看过一个视频，便视作一个正例。如果将视频库中的每一个视频当作一个类别，那么在时刻t，对于用户U和上下文C，用户会观看视频i的概率为：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-2fa775e4d7be048b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>其中，u是用户的embedding，这个embedding，是网络最后一个Relu激活函数的输出，v<sub>i</sub>是视频i的embedding。那么问题来了，输入时，每一个视频也有一个对应的embedding，这个embedding是不是计算softmax的embedding呢？这里文章也没有说清楚？也许一个视频对应一个embedding，也许一个视频对应两组不同的embedding。关于这个问题的理解，欢迎大家在评论区留言！</p>
<p>使用多分类问题的一个弊端是，我们有百万级别的classes，模型是非常难以训练的，因此在实际中，Youtube并使用负样本采样(negative sampling)的方法，将class的数量减小。还有一种可以替换的方法，成为hierarchical softmax，但经过尝试，这种方法并没有取得很好的效果。关于上面的两种方法，大家是不是想起了word2vec中训练词向量的两种方式？但是这里的负采样和word2vec中的负采样方法是不同的，这里采样之后还是一个多分类问题，而word2vec中的负采样方法是将问题转为了一个二分类问题。</p>
<p>下图是离线训练的结果，使用的评价指标是MAP(Mean Average Precision)，主要考察的两个点是输入特征以及网络层数对于实验效果的影响：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-fe59f87b5d7c4313.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<h4 id="3-4-在线服务"><a href="#3-4-在线服务" class="headerlink" title="3.4 在线服务"></a>3.4 在线服务</h4><p>对于在线服务来说，有严格的性能要求，必须在几十毫秒内返回结果。因此，youtube没有重新跑一遍模型，而是通过保存用户的embedding和视频的embedding，通过最近邻搜索的方法得到top N的结果。</p>
<p>从图中可以看到，最终的结果是approx topN的结果，所以并不是直接计算用户embedding和每个视频embedding的内积。如果这样做的话，N个视频的内积计算 + 排序，时间复杂度大概是NlogN，这样很难满足时间复杂度要求。如果使用局部敏感哈希(Locality-Sensitive Hashing, LSH)等近似最近邻快速查找技术，时间复杂度是可以大大降低的。</p>
<p>但文中只是提到说使用hash的方法来得到近似的topN，所以也许不是局部敏感哈希方法，不过如果想要了解一下该方法的原理，可以参考博客：<a href="https://www.cnblogs.com/wt869054461/p/8148940.html" target="_blank" rel="external">https://www.cnblogs.com/wt869054461/p/8148940.html</a></p>
<h2 id="4、排序Ranking"><a href="#4、排序Ranking" class="headerlink" title="4、排序Ranking"></a>4、排序Ranking</h2><p>排序过程是对生成的候选集做进一步细粒度的排序。模型的结构图如下所示：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-2e46cf2dccca4760.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<h4 id="4-1-输入特征"><a href="#4-1-输入特征" class="headerlink" title="4.1 输入特征"></a>4.1 输入特征</h4><p>在排序阶段，输入的特征主要有：</p>
<p><strong>impression video ID embedding</strong>: 当前要计算的video的embedding<br><strong>watched video IDs average embedding</strong>: 用户观看过的最后N个视频embedding的average pooling<br><strong>language embedding</strong>: 用户语言的embedding和当前视频语言的embedding<br><strong>time since last watch</strong>: 用户上次观看同频道时间距现在的时间间隔<br><strong>previous impressions</strong>: 该视频已经被曝光给该用户的次数</p>
<p>前面三组特征是比较好理解的，我们重点来看一下后面两组特征的作用。第4个特征是用户上次观看同频道时间距现在的时间间隔，这里有一点attention的意思，加入我们刚看了一场NBA比赛的集锦，我们很可能继续观看NBA频道的其他视频，那么这个特征就很好地捕捉到了这一行为。第5个特征previous impressions则一定程度上引入了exploration的思想，避免同一个视频持续对同一用户进行无效曝光。尽量增加用户没看过的新视频的曝光可能性。</p>
<h4 id="4-2-特征处理"><a href="#4-2-特征处理" class="headerlink" title="4.2 特征处理"></a>4.2 特征处理</h4><p>特征处理主要包含对于离散变量的处理和连续变量的处理。</p>
<p>对于离散变量，这里主要是视频ID，Youtube这里的做法是有两点：<br>1、只保留用户最常点击的N个视频的embedding，剩余的长尾视频的embedding被赋予全0值。可能的解释主要有两点，一是出现次数较少的视频的embedding没法被充分训练。二是也可以节省线上服务宝贵的内存资源。<br>2、对于相同域的特征可以共享embedding，比如用户点击过的视频ID，用户观看过的视频ID，用户收藏过的视频ID等等，这些公用一套embedding可以使其更充分的学习，同时减少模型的大小，加速模型的训练。</p>
<p>对于连续特征，主要进行归一化处理，神经网络对于输入的分布及特征的尺度是十分敏感。因此作者设计了一种积分函数将连续特征映射为一个服从[0,1]分布的变量。该积分函数为： </p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-cea02da3ed7c6547.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>个人理解就是将概率密度分布转换成了累计密度分布。为了引入特征的非线性，除了加入归一化后的特征外，还加入了该特征的平方和开方值。</p>
<h4 id="4-3-建模期望观看时间"><a href="#4-3-建模期望观看时间" class="headerlink" title="4.3 建模期望观看时间"></a>4.3 建模期望观看时间</h4><p>在训练阶段，Youtube没有把问题当作一个CTR预估问题，而是通过weighted logistic 建模了用户的期望观看时间。</p>
<p>在这种情况下，对于正样本，权重是观看时间，而对于负样本，权重是单位权重(可以认为是1)，那么，此时，观看时长的几率(odds，在原逻辑回归中，指正例发生的概率与负例发生概率的比值)为：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-ea543a843dddad7c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>上式中，T<sub>i</sub>指样本中第i条正样本的观看时长，N是所有的训练样本，k是正样本的个数。在k特别小的情况下，上式近似为E<a href="1+P">T</a>，P是点击率，E[T]是视频的期望观看时长，因为P非常小，那么乘积近似于E[T]。</p>
<p>同时，对于逻辑回归，我们知道几率的计算公式其实就是exp(wx + b)，同时几率可以近似于期望观看时长E[T]，那么我们在测试阶段，就可以直接输出exp(wx + b)，作为期望观看时长。</p>
<p>离线训练的效果如下图：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-448067ca34dd5e05.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<h2 id="5、总结-1"><a href="#5、总结-1" class="headerlink" title="5、总结"></a>5、总结</h2><p>好了，本文就到这里了，我们一起回顾了一下Youtube的视频推荐系统，它是一个两阶段的系统。在每一个阶段，都有很多值得我们思考和学习的细节。最后，引用推荐阅读3中的十大问题，帮助你检验是否真正理解了文章的内容：</p>
<blockquote>
<p>1、文中把推荐问题转换成多分类问题，在next watch的场景下，每一个备选video都会是一个分类，因此总共的分类有数百万之巨，这在使用softmax训练时无疑是低效的，这个问题Youtube是如何解决的？<br>2、在candidate generation model的serving过程中，Youtube为什么不直接采用训练时的model进行预测，而是采用了一种最近邻搜索的方法？<br>3、Youtube的用户对新视频有偏好，那么在模型构建的过程中如何引入这个feature？<br>4、在对训练集的预处理过程中，Youtube没有采用原始的用户日志，而是对每个用户提取等数量的训练样本，这是为什么？<br>5、Youtube为什么不采取类似RNN的Sequence model，而是完全摒弃了用户观看历史的时序特征，把用户最近的浏览历史等同看待，这不会损失有效信息吗？<br>6、在处理测试集的时候，Youtube为什么不采用经典的随机留一法（random holdout），而是一定要把用户最近的一次观看行为作为测试集？<br>7、在确定优化目标的时候，Youtube为什么不采用经典的CTR，或者播放率（Play Rate），而是采用了每次曝光预期播放时间（expected watch time per impression）作为优化目标？<br>8、在进行video embedding的时候，为什么要直接把大量长尾的video直接用0向量代替？<br>9、针对某些特征，比如#previous impressions，为什么要进行开方和平方处理后，当作三个特征输入模型？<br>10、为什么ranking model不采用经典的logistic regression当作输出层，而是采用了weighted logistic regression？</p>
</blockquote>
<h2 id="推荐阅读"><a href="#推荐阅读" class="headerlink" title="推荐阅读"></a>推荐阅读</h2><p>以下几篇文章推荐大家进行阅读，便于加深理解：<br>1、<a href="https://blog.csdn.net/xiongjiezk/article/details/73445835" target="_blank" rel="external">https://blog.csdn.net/xiongjiezk/article/details/73445835</a><br>2、<a href="https://zhuanlan.zhihu.com/p/52169807" target="_blank" rel="external">https://zhuanlan.zhihu.com/p/52169807</a><br>3、<a href="https://zhuanlan.zhihu.com/p/52504407" target="_blank" rel="external">https://zhuanlan.zhihu.com/p/52504407</a></p>
<h1 id="推荐系统遇上深度学习-三十五-–强化学习在京东推荐中的探索-二"><a href="#推荐系统遇上深度学习-三十五-–强化学习在京东推荐中的探索-二" class="headerlink" title="推荐系统遇上深度学习(三十五)–强化学习在京东推荐中的探索(二)"></a>推荐系统遇上深度学习(三十五)–强化学习在京东推荐中的探索(二)</h1><p>本文介绍的论文题目为《Recommendations with Negative Feedback via Pairwise Deep Reinforcement Learning》，这应该是强化学习在京东推荐中的第二篇文章了，上一篇《Deep Reinforcement Learning for List-wise Recommendations》我们在本系列的第十五篇中已经介绍过了，大家可以进行回顾：<a href="https://www.jianshu.com/p/b9113332e33e。" target="_blank" rel="external">https://www.jianshu.com/p/b9113332e33e。</a></p>
<p>本文论文的下载地址为：<a href="https://arxiv.org/pdf/1802.06501.pdf" target="_blank" rel="external">https://arxiv.org/pdf/1802.06501.pdf</a></p>
<h2 id="1、背景-6"><a href="#1、背景-6" class="headerlink" title="1、背景"></a>1、背景</h2><p>现有的大多数推荐系统，如协同过滤、基于内容的方法，推荐过程大都是静态的，忽略了用户偏好的动态变化性。同时，大多数推荐系统的目标是最大化即时收益，如CVR、CTR等，忽略了对长期收益的考虑。</p>
<p>强化学习方法可以应对上述两个问题，强化学习将推荐问题视为序列决策问题，同时其目标是最大化长期受益。那么，京东是如何将强化学习应用到推荐系统中的呢？</p>
<h2 id="2、问题陈述"><a href="#2、问题陈述" class="headerlink" title="2、问题陈述"></a>2、问题陈述</h2><p>对于一个强化学习问题来说，其主要包含五个主要因素：</p>
<p><strong>状态空间S</strong>：状态空间中的状态定义为用户之前的浏览历史，包括点击／购买过的和略过的，二者分开进行处理。同时，物品是按照先后顺序进行排序的。<br><strong>动作空间A</strong>：这里，我们假设一次只给用户推荐一个物品，那么推荐的物品即动作。<br><strong>即时奖励R</strong>：在给用户推荐一个物品后，用户可以选择忽略、点击甚至购买该物品，根据用户的行为将给出不同的奖励。<br><strong>状态转移概率P</strong>：状态的转移主要根据推荐的物品和用户的反馈来决定的。<br><strong>折扣因子r</strong>：对未来收益打一定的折扣。</p>
<p>下图展示了强化学习用于推荐的过程：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-713f7704b18d92d6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<h2 id="3、基本模型框架"><a href="#3、基本模型框架" class="headerlink" title="3、基本模型框架"></a>3、基本模型框架</h2><p>由于动作空间和状态空间巨大，很难直接使用Q-table来解决问题，因此这里使用Deep Q Network(DQN)来作为基准的模型。</p>
<p>在基本的框架里，当前状态s和状态之间的转移关系定义如下：<br><strong>当前状态s</strong>: s={i<sub>1</sub>,i<sub>2</sub>,…,i<sub>N</sub>}，用户之前点击或购买过的N个物品，同时按照时间先后进行排序<br><strong>s转移到s’</strong>：假设当前的推荐物品a，用户若点击或购买，则s’={i<sub>2</sub>,i<sub>3</sub>,…,i<sub>N</sub>,a}，若用户略过，则s’=s 。</p>
<p>有了上述定义，基本的模型框架如下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-a5879bdcd26fe39d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>我们每次将state以及待推荐的物品作为输入，转换为embedding后经过多层神经网络得到Q(s,a)。</p>
<p>当然，对于DQN来说，上面的网络被称为eval-net，我们通常还有一个target-net，用于计算Q的实际值，在已知即时奖励r的情况下，实际值计算如下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-798b142f714a912e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>这样，DQN的损失函数定义为平方损失，下式中y是Q的实际值：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-95876c8600ca8f48.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>对于基本模型来说，一个明显的缺点就是，当推荐的物品被用户忽略时，状态是不会发生变化的。因此对于用户忽略过的物品，也应该被考虑在状态中。那么具体该怎么做？在下一节中我们将进行讨论。</p>
<h2 id="4、考虑负反馈以及偏序关系的强化学习推荐框架"><a href="#4、考虑负反馈以及偏序关系的强化学习推荐框架" class="headerlink" title="4、考虑负反馈以及偏序关系的强化学习推荐框架"></a>4、考虑负反馈以及偏序关系的强化学习推荐框架</h2><p>在考虑负反馈的情况下，当前状态s和状态之间的转移关系定义如下：<br><strong>当前状态s</strong>: 当前状态s包含两部分s=(s<sub>+</sub>,s<sub>-</sub>)，其中s<sub>+</sub>={i<sub>1</sub>,i<sub>2</sub>,…,i<sub>N</sub>}，表示用户之前点击或购买过的N个物品，s<sub>-</sub>={j<sub>1</sub>,j<sub>2</sub>,…,j<sub>N</sub>}，表示用户之前略过的N个物品。同时物品按照时间先后进行排序。<br><strong>s转移到s’</strong>：假设当前的推荐物品a，用户若点击或购买，则s’<sub>+</sub>={i<sub>2</sub>,i<sub>3</sub>,…,i<sub>N</sub>,a}，若用户略过，则s’<sub>-</sub>={j<sub>2</sub>,j<sub>3</sub>,…,j<sub>N</sub>,a} 。那么，s’ =  (s’<sub>+</sub>,s’<sub>-</sub>)。</p>
<p>在考虑负反馈的情况下，模型框架为：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-086d6041aa5826a3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>相较于基本的模型，该框架使用GRU来将s<sub>+</sub>,s<sub>-</sub>两个序列中的物品进行处理。</p>
<p>除考虑负反馈外，还考虑了物品之间的偏序关系，对于一个物品a，偏序对中的另一个物品我们称为a<sup>C</sup>，但只有满足三个条件，才可以称为a<sup>C</sup>。首先，a<sup>C</sup>必须与a是同一个类别的物品；其次，用户对于a<sup>C</sup>和a的反馈是不同的；最后，a<sup>C</sup>与a的推荐时间要相近。</p>
<p>若物品a找不到有偏序关系的物品a<sup>C</sup>，我们希望预估的Q值和实际的Q值相近，模型的损失函数为：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-98bc8e688d0cc59b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>若物品a能够找到有偏序关系的物品a<sup>C</sup>，此时，我们既希望预估的Q值和实际的Q值相近，同时又希望有偏序关系的两个物品的Q值差距越大越好，因此模型的损失函数变为：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-70fa355a4b782567.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>其中，y的计算如下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-27c78634cc082a87.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>至此，模型部分的介绍就结束了，当然，论文中还有关于模型训练和测试部分的内容，感兴趣的同学可以翻看一下论文！</p>
<h1 id="推荐系统遇上深度学习-三十六-–Learning-and-Transferring-IDs-Representation-in-E-commerce"><a href="#推荐系统遇上深度学习-三十六-–Learning-and-Transferring-IDs-Representation-in-E-commerce" class="headerlink" title="推荐系统遇上深度学习(三十六)–Learning-and-Transferring-IDs-Representation-in-E-commerce"></a>推荐系统遇上深度学习(三十六)–Learning-and-Transferring-IDs-Representation-in-E-commerce</h1><p>本文介绍的文章题目为《Learning and Transferring IDs Representation in E-commerce》，下载地址为：<a href="https://arxiv.org/abs/1712.08289" target="_blank" rel="external">https://arxiv.org/abs/1712.08289</a></p>
<p>本文介绍了一种ID类特征的表示方法。该方法基于item2vec方式，同时考虑了不同ID类特征之间的连接结构，在盒马鲜生app上取得了不错的应用效果，我们来一探究竟。</p>
<h2 id="1、背景-7"><a href="#1、背景-7" class="headerlink" title="1、背景"></a>1、背景</h2><p>在推荐系统特别是电商领域的推荐中，ID类特征是至关重要的的特征。传统的处理方式一般是one-hot编码。但是这种处理方式存在两个主要的弊端：<br>1）高维稀疏问题：对于高维稀疏问题，若有N个物品，那么用户交互过的物品的可能情况共2^N种情况，为了使我们的模型更加具有可信度，所需要的样本数量是随着N的增加呈指数级增加的。<br>2）它无法反映ID之间的关系：对于同质信息来说，比如不同的物品，假设是iphon5和iphone6，以及iphone5和华为，在转换成one-hot编码后，距离是一样的，但是实际上，iphon5和iphone6的距离应该更近。对于异质信息，如物品ID和商铺ID，它们的距离甚至无法衡量，但实际上，一家卖苹果手机的商铺和苹果手机之间，距离应该更近。</p>
<p>对于上述问题，出现了word2vec以及item2vec的解决方案，将ID类特征转换为一个低维的embedding向量，这种方式在电商领域的推荐中取得了不错的效果。</p>
<p>本文提出的方式，基于item2vec，同时还考虑了不同ID类特征之间的连接结构，通过这些连接，在ItemID序列中的信息可以传播到其它类型的ID特征，并且可以同时学习这些ID特征的表示，框架如下图所示：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-a339b2661aeb9eee.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>上面的学习方式在盒马app中有以下几方面的应用：<br>1）Measuring the similarity between items：建模物品之间的相似度<br>2）Transferring from seen items to unseen items：将已知物品的向量迁移到位置物品上<br>3）Transferring across different domains：将不同领域的向量进行迁移<br>4）Transferring across different tasks.：从不同的应用场景中进行迁移。</p>
<p>上面的几个应用我们在后文中会详细介绍。接下来，我们首先来介绍一下本文如何对ID类特征进行处理。</p>
<h2 id="2、学习ID的表征方式"><a href="#2、学习ID的表征方式" class="headerlink" title="2、学习ID的表征方式"></a>2、学习ID的表征方式</h2><h4 id="2-1-Skip-gram-on-User’s-Interactive-Sequences"><a href="#2-1-Skip-gram-on-User’s-Interactive-Sequences" class="headerlink" title="2.1 Skip-gram on User’s Interactive Sequences"></a>2.1 Skip-gram on User’s Interactive Sequences</h4><p>在电商领域，我们可以通过用户的隐式反馈，整理得到用户的一个交互序列。如果把每一个交互序列认为是一篇文档，那么我们可以通过Skip-Gram的方法来学习每一个item的向量。Skip-Gram的方法是最大化下面的对数概率：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-0d25ba23d89178b4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>其中，C是我们的上下文长度，假设长度是2，那么下图中梨的上下文就是前后的两个item：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-529b519cff603908.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>有关Skip-Gram模型的相关知识，可以参考：<a href="https://www.cnblogs.com/peghoty/p/3857839.html" target="_blank" rel="external">https://www.cnblogs.com/peghoty/p/3857839.html</a></p>
<p>在基本的Skip-Gram模型中，概率计算方式定义为如下的softmax方程：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-be455b49f2f46876.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>其中，每个物品都有两套向量表示，分别是目标向量表示和上下文向量表示。D表示item的总数量。</p>
<h4 id="2-2-Log-uniform-Negative-sampling"><a href="#2-2-Log-uniform-Negative-sampling" class="headerlink" title="2.2 Log-uniform Negative-sampling"></a>2.2 Log-uniform Negative-sampling</h4><p>当item的总数量十分巨大时，求解Skip-Gram的方法通常是负采样的方式，此时概率计算如下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-5d961ad4b3ce40fb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>这里使用的负采样方式是Log-uniform Negative-sampling。简单介绍一下其流程：首先将D个物品按照其出现的频率进行降序排序，那么排名越靠前的物品，其出现的频率越高。采样基于Zipfian分布，每个物品采样到的概率如下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-c1e02a22d8e6d0ae.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>由于分母都是一样的，分子依次为log(2/1),log(3/2)…log(D+1/D)，是顺次减小的，同时求和为1。那么排名越靠前即出现频率越高的商品，被采样到的概率是越大的。</p>
<p>那么，该分布的累积分布函数为：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-a0b3ed5bf2b85d32.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>这样，当随机产生一个(0,1]之间的随机数r时，可以通过下面的转换快速得到对应的index：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-c941e0a507ed5bfb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<h4 id="2-3-IDs-and-Their-Structural-Connections"><a href="#2-3-IDs-and-Their-Structural-Connections" class="headerlink" title="2.3 IDs and Their Structural Connections"></a>2.3 IDs and Their Structural Connections</h4><p>实际应用中，有许多组不同的ID，但是可以归结为两组：</p>
<p><strong>1）物品ID及其属性ID</strong>：物品item ID是电商领域的核心，每一个物品有对应的属性ID，比如产品product ID，店铺store ID，品牌brand ID，品类category ID等。举个简单的例子，xxx店卖的哈登篮球鞋是一个物品item ID，其对应的产品是哈登篮球鞋，对应一个product ID，另一家店卖的同款哈登篮球鞋是另一个item ID。xxx店对应的是一个store ID，addias是哈登篮球鞋对应的品牌brand ID，而篮球鞋是其对应的category ID。品类ID可能有分一级品类、二级品类和三级品类。</p>
<p><strong>2）用户ID</strong>：用户身份可以通过ID进行识别，这里的ID可以有多种形式，如cookie、uuid、用户昵称等等。</p>
<h4 id="2-4-Jointly-Embedding-Attribute-IDs"><a href="#2-4-Jointly-Embedding-Attribute-IDs" class="headerlink" title="2.4 Jointly Embedding Attribute IDs"></a>2.4 Jointly Embedding Attribute IDs</h4><p>如何将上面所说的属性ID加入到物品ID的表示上来呢？结构如下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-190969135dc088c0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>这里，定义item<sub>i</sub>的ID组IDs(item<sub>i</sub>)如下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-6c3b5b223ea151b5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>这里，id<sub>1</sub>(item<sub>i</sub>)代表物品本身，id<sub>2</sub>(item<sub>i</sub>)代表产品ID，id<sub>3</sub>(item<sub>i</sub>)代表店铺ID等等。那么，Skip-gram的概率计算变为如下的方式：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-94f1d148fca59fa0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>这里，每一种ID的向量长度可以是不同的，也就是说不通的ID类映射到不同的语义空间中。而权重的定义方式如下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-dc6eb648d3bffb9a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>比如，w<sub>i1</sub>=1,因为item<sub>i</sub>是唯一的，而如果item<sub>i</sub>对应的产品有10种item的话，那么w<sub>i2</sub>=1/10。</p>
<p>除了上面计算的item之间的共现概率外，我们还希望，属性ID和itemID之间也要满足一定的关系，简单理解就是希望itemID和其对应的属性ID关系越近越好，于是定义：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-d46efb6d87fee3e9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>其中M<sub>k</sub>将item ID对应的向量转换成跟每个属性ID对应向量长度一样的向量。</p>
<p>结合两部分的对数概率，加入正则项，则我们期望最大化的式子变为：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-3f62740bf4cb11cc.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<h4 id="2-5-Embedding-User-IDs"><a href="#2-5-Embedding-User-IDs" class="headerlink" title="2.5 Embedding User IDs"></a>2.5 Embedding User IDs</h4><p>用户ID的Embedding通常通过其交互过的item表示，比如通过一个RNN模型或者简单的取平均的方式，这里我们将用户最近交互过的T个物品对应向量的平均值，来代表用户的Embedding：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-bf7dc4d4324e7092.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>这里提到的是用平均值法代表用户的Embedding，后文还提到了一种加权法，主要根据用户的不同行为对T个物品进行加权，比如，购买过的物品要比只点击不购买的物品获得更高的权重。</p>
<h4 id="2-6-Model-Learning"><a href="#2-6-Model-Learning" class="headerlink" title="2.6 Model Learning"></a>2.6 Model Learning</h4><p>模型的训练具体参数如下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-b8523048a9109a37.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<h2 id="3、应用"><a href="#3、应用" class="headerlink" title="3、应用"></a>3、应用</h2><p>本节我们来介绍一下上述方法的四种应用。在应用中，上述的框架我们将其定义为一种ITEM2VEC的方法。下文中ITEM2VEC便指代本文提出的新方法。</p>
<p>##3.1 Measuring Items Similarity</p>
<p>在电商领域中，一种简单却有效的方式就是推荐给用户其喜欢的相似物品。通常使用cosine相似度来计算物品之间的相似度。那么应用上面的框架，基于得到的物品向量，便可以计算其相似度，进而推荐相似度最高的N个物品。</p>
<p>我们将其与协同过滤方法进行了对比，结果如下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-594ba5d2ae5e3e7b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>可以看到，点击率超过了协同过滤模型。</p>
<h4 id="3-2-Transferring-from-Seen-Items-to-Unseen-Items"><a href="#3-2-Transferring-from-Seen-Items-to-Unseen-Items" class="headerlink" title="3.2 Transferring from Seen Items to Unseen Items"></a>3.2 Transferring from Seen Items to Unseen Items</h4><p>对于新的物品，无法得到其向量表示，这导致了许多推荐系统无法对新物品进行处理。但本文提出的方法可以在一定程度上解决冷启动问题。在模型训练时，我们添加了约束，即希望itemID和其对应的属性ID关系越近越好，如下式：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-5cd8ce88f20d30ee.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>我们期望上面的式子越接近于1越好，因此：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-0817085a39559036.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>那么对于新的物品，其对应的属性ID我们往往是知道的，基于其属性ID对应的向量，我们便可以近似计算新物品的向量。上面最后一个地方可以好好理解一下，为什么可以表示成近似？</p>
<p>我的思考如下(不一定正确，望指正）：如果我们把括号里面的向量表示成另一个向量e的话，当两个向量长度固定的时候，什么时候内积取得最大值？是二者同向且共线的时候，那么e<sub>i1</sub>和e应该是线性关系，即使e是真实的e<sub>i1</sub>的n倍的话，在计算该物品与其他物品的相似度的时候，是不会产生影响的，因此e<sub>i1</sub>可近似用e来代替。</p>
<p>实验结果也表明，这种代替方式是十分有效的。下面的表格展示了实验结果：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-b9a88a521373f961.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<h4 id="3-3-Transferring-across-Different-Domains"><a href="#3-3-Transferring-across-Different-Domains" class="headerlink" title="3.3 Transferring across Different Domains"></a>3.3 Transferring across Different Domains</h4><p>第三个应用主要是针对用户冷启动，在盒马平台上，相对于淘宝平台用户数量还是少很多的。那么对于盒马平台上的新用户，我们如何进行推荐呢？过程如下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-c0133765c4872ee1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>这里，用Us表示淘宝的用户，Ut表示盒马的用户，Ui表示既是淘宝又是盒马的用户，那么进行推荐的过程如下：</p>
<p>1）计算淘宝用户Us之间的相似度，相似度的计算基于用户最近在淘宝交互过的T个商品的向量。可以是简单的平均，可以是加权平均。权重取决于人工的设定，比如购买是5，点击是1；<br>2）基于计算的用户相似度，对Ui中的用户进行k-均值聚类，这里聚成1000个类别；<br>3）对于每个类别，选择N个最受欢迎的盒马上的物品，作为候选集；<br>4）对于盒马上的一个新用户，如果它在淘宝上有交互记录，那么就取得他在淘宝上对应的用户向量，并计算该向量所属的类别；<br>5）基于得到的类别，将经过筛选和排序后的该类别的候选集中物品中推荐给用户。</p>
<p>我们对比了三种不同策略的PPM(Pay-Per-Impression)值，三种策略为：<br>1）推荐给新用户最热门的物品，该组为Base<br>2）基于简单平均的方式计算用户向量<br>3）基于加权平均的方式计算用户向量</p>
<p>实验结果表明，采用简单平均的方式，PPM提升71.4%，采用加权平均的方式，PPM提升141.8%。</p>
<h4 id="3-4-Transferring-across-Different-Tasks"><a href="#3-4-Transferring-across-Different-Tasks" class="headerlink" title="3.4 Transferring across Different Tasks"></a>3.4 Transferring across Different Tasks</h4><p>这里，我们主要对每个店铺第二天每个30分钟的配送需求进行预测，这里有三种不同的输入：<br>1）仅使用过去7天每三十分钟的店铺配送量作为输入<br>2）使用使用过去7天每三十分钟的店铺配送量作为输入 + 店铺ID的one-hot encoding<br>3）使用使用过去7天每三十分钟的店铺配送量作为输入 + 店铺ID对应的向量。</p>
<p>输入经过全联接神经网络得到配送需求的预测值，并通过RMAE指标来计算误差，结果如下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-fe4cf998fe8c58b0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<h2 id="4、总结-1"><a href="#4、总结-1" class="headerlink" title="4、总结"></a>4、总结</h2><p>本文介绍了一种处理ID类特征的方式，该方式基于Skip-Gram方法，并考虑了多种不同ID类特征之间的联系。本文介绍了该方法的详细过程，以及在盒马app上的具体应用，具有一定的参考价值。</p>
<h1 id="推荐系统遇上深度学习-三十七-–基于多任务学习的可解释性推荐系统"><a href="#推荐系统遇上深度学习-三十七-–基于多任务学习的可解释性推荐系统" class="headerlink" title="推荐系统遇上深度学习(三十七)–基于多任务学习的可解释性推荐系统"></a>推荐系统遇上深度学习(三十七)–基于多任务学习的可解释性推荐系统</h1><p>论文名称：《Why I like it: Multi-task Learning for Recommendation and Explanation》<br>论文地址：<a href="https://dl.acm.org/citation.cfm?id=3240365" target="_blank" rel="external">https://dl.acm.org/citation.cfm?id=3240365</a></p>
<p>本文提出了一种通过结合概率矩阵分解PMF和对抗式Seq2Seq模型的多任务学习框架，可以通过矩阵分解模型得到用户对物品的评分，通过Seq2Seq模型可以生成用户对于物品的评论，在提升推荐预测准确性的同时，能够在一定程度上解决推荐系统中难以提供可解释性推荐结果的难题。我们来一探究竟。</p>
<h2 id="1、整体框架"><a href="#1、整体框架" class="headerlink" title="1、整体框架"></a>1、整体框架</h2><p>该模型的整体框架如下图所示：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-6809862c7f3230da.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>可以看到，该模型架构可以分为两个部分：</p>
<p><strong>对抗式Seq2Seq模型（Adversarial Sequence-to-Sequence Learning）</strong>：这一部分学习生成用户对于物品的评论，并作为推荐结果的可解释性依据。</p>
<p><strong>上下文感知的PMF模型（Context-aware Matrix Factorization）</strong>：这一部分主要得到用户对于目标物品的评分。</p>
<p>接下来，我们分别介绍上面两部分的结构。</p>
<h2 id="2、对抗式Seq2Seq模型"><a href="#2、对抗式Seq2Seq模型" class="headerlink" title="2、对抗式Seq2Seq模型"></a>2、对抗式Seq2Seq模型</h2><p>关于该部分的模型，文章首先介绍了单独生成用户u的评论和单独生成物品j评论的过程，最后简单介绍了生成用户u对物品j个性化评论的过程。注意在上面的框架图中，对抗式Seq2Seq模型包含了单独生成用户评论和单独生成物品评论两个部分的结构。</p>
<p>这里我们以单独生成用户的评论为例，介绍模型的结构。单独生成物品的评论，是同样的原理。</p>
<p>首先定义用户文档d<sub>u,i</sub>代表用户i的所有历史评论的集合，而物品文档d<sub>v,j</sub>代表物品j的所有历史评论集合。</p>
<p>经典的Seq2Seq模型，Decoder阶段输入的是已知的正确数据，但是这种方式会导致<strong>exposure bias</strong>，即一步错，步步错。所以，文章首创一种对抗式 Seq2Seq 模型，与常见的 GAN 方式一样包括判别网络和生成网络。 </p>
<h4 id="2-1-Recurrent-Review-Generator"><a href="#2-1-Recurrent-Review-Generator" class="headerlink" title="2.1 Recurrent Review Generator"></a>2.1 Recurrent Review Generator</h4><p>生成器G的目的是尽可能生成像用户i所写的评论。包含两个主要的部分：用户文档encoder和用户评论生成decoder。</p>
<p><strong>用户文档encoder</strong></p>
<p>该部分的详细流程如下：<br>1、对于用户文档d<sub>u,i</sub>中的每一条评论，将其进行分词，得到（w<sub>1</sub>,w<sub>2</sub>,…,w<sub>T</sub>）单词序列。<br>2、单词序列通过embedding 表得到对应的k维embedding，这里的embedding通过首先通过word2vec预训练得到，随后经由反向传播的过程不断更新。<br>3、将单词序列对应的embedding输入到双向的GRU循环神经网络中，得到两部分的输出：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-f69d8ff3f8850289.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>两部分的输出分别代表正向输入和反向输入的最后时刻的输出，然后将两部分的输出进行拼接，得到每个评论的对应向量h<sub>T</sub>。<br>4、将每个评论的的对应向量h<sub>T</sub>进行求平均操作，得到用户的文本特征向量，用U<sup>~</sup><sub>i</sub>表示。</p>
<p><strong>用户评论生成decoder</strong><br>在得到用户的文本特征向量之后，decoder生成用户评论的过程可以用如下的条件概率表示：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-ea2db36ddc8e1cbf.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>这里的decoder的初始隐状态是全0的向量，而非encoder最后时刻的隐藏层状态。encoder和decoder通过用户文本特征向量相联系。decoder在t-1时刻得到选择的单词y<sub>i,t-1</sub>时，得到该单词对应的向量x<sub>i,t-1</sub>,并与用户文本特征向量进行拼接，作为decoder的t时刻的输入：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-7e2ee6d48c57188e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>decoder的初始输入是开始标记SOS，在得到结束标记EOS时终止。在训练初期，使用用户真正的评论进行参数预训练，随后通过蒙特卡洛搜索的方式生成评论。</p>
<h4 id="2-2-Convolutional-Review-Discriminator"><a href="#2-2-Convolutional-Review-Discriminator" class="headerlink" title="2.2 Convolutional Review Discriminator"></a>2.2 Convolutional Review Discriminator</h4><p>对于判别模型，其目的不仅仅是区分评论是否是人写的，还要区分生成的评论是否是用户i所写的，所以需要在结构中加入用户特征向量，该特征向量不是上文所说的用户文本特征向量，应该是对用户ID所对应的Embedding。该部分借鉴经典的textCNN结构进行判别，将评论词汇向量与用户特征向量进行 Concat 处理后作为输入，模型结构如下所示：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-8507f16f23d02b88.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<h4 id="2-3-Adversarial-Training-for-Review-Generation-with-REINFORCE"><a href="#2-3-Adversarial-Training-for-Review-Generation-with-REINFORCE" class="headerlink" title="2.3 Adversarial Training for Review Generation with REINFORCE"></a>2.3 Adversarial Training for Review Generation with REINFORCE</h4><p>对判别器来说，其目标是尽可能将人写的和生成器生成的区分开来，其目标是最大化下面的式子：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-d52774165fc5fdd4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>而对生成器来说，采用强化学习中策略梯度的方式进行训练，其梯度为：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-04dcf009d4fdca89.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>而最终生成网络的损失函数定义如下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-d4b33ee1e530b097.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>其中，U是经由矩阵分解模型得到的用户向量矩阵，如何得到我们将在后面介绍。这还不同于textCNN中输入的用户特征向量。所以我们缕一缕，现在每个用户有三组不同的向量：用户文本特征向量、经由矩阵分解得到的用户向量、textCNN中输入的用户向量。</p>
<p>在生成器加入后面的正则项的原因是为了使用户文本特征向量不仅仅经由用户的评论生成，同时还考虑用户在评分矩阵中所体现出的偏好。</p>
<h4 id="2-4-扩展"><a href="#2-4-扩展" class="headerlink" title="2.4 扩展"></a>2.4 扩展</h4><p>上面介绍了用户评论生成器，物品评论生成器和用户-物品评论生成器都是同样的过程，只是将输入做了一定的替换。这里我们就不再做过多的解释了。</p>
<h2 id="3、上下文感知的PMF模型"><a href="#3、上下文感知的PMF模型" class="headerlink" title="3、上下文感知的PMF模型"></a>3、上下文感知的PMF模型</h2><p>这里评分预测问题所选用的模型是概率矩阵分解PMF模型（Probabilistic Matrix Factorization）。</p>
<p>我们这里简单介绍一下该模型的原理。该模型基于如下两个假设：<br>使用如下两个假设：观测噪声（观测评分矩阵R和近似评分矩阵之差）为高斯分布，同时用户属性向量和物品属性向量矩阵均为多元高斯分布（即向量中的每一维服从不同的高斯分布），即：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-d0dce1de379501e0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-4d6ed4dedc6a320d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>上面第一个式子可以改写成：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-9d6c5b6ed8377976.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>我们的目标是基于目前的评分矩阵，找到最有可能的用户向量和物品向量，即最大化下面的后验概率：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-9c625e311d68b944.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>这里所有的方差都是超参数，所以可以认为是常量，所以可以对上面的式子进行化简（这里用户和物品向量是独立的）：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-f422cff606207f5b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>两边取对数得到：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-1344aa9c177c8295.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>将多元高斯分布的公式带入最终化简可得到：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-b2219078a6e4a357.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>上面第二行的数都是常数，所以我们只需要关心第一行的式子，最大化上面的式子，最终可化简为最小化下面的式子：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-5b64421bc32744f0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>有关详细的推导过程，可以参考文章：<a href="https://blog.csdn.net/df19900725/article/details/78222076?locationNum=4&amp;fps=1" target="_blank" rel="external">https://blog.csdn.net/df19900725/article/details/78222076?locationNum=4&amp;fps=1</a></p>
<p>本文对上面的过程进行了一定的改进，主要是用户属性向量和物品属性向量所服从的分布变为：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-06bd98a0d27d8550.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>而最终经由化简得到的损失函数为：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-a0f67232b47dfad5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>通过求导数为0，可以计算得到最终的结果为：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-eaa305403cfca82e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>感兴趣的同学可以自己推导一下上面的过程。</p>
<h2 id="4、模型训练流程"><a href="#4、模型训练流程" class="headerlink" title="4、模型训练流程"></a>4、模型训练流程</h2><p>一种常见的训练方式是将两部分的损失函数加起来得到一个全局的损失函数进行联合训练。但是，无论是生成模型还是评分预测模型，两部分都同时依赖用户向量U和用户文本特征向量U<sup>~</sup>，以及物品向量V和物品文本特征向量V<sup>~</sup>。如果想要生成好的评论，模型会期望所得到的用户向量U和物品向量V是准确的，如果想要得到较为精确的评分，模型会期望用户文本特征向量U<sup>~</sup>和物品文本特征向量V<sup>~</sup>是准确的，但是在训练初期，二者都不是特别精确，因此会导致两个任务之间相互误导。<br>所以实际中采用类似于EM方法的训练方式，即当训练其中一个任务时，固定另一个任务的参数，反复交叉训练，其流程如下图：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-b4f1e85b7b09c1bb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<h2 id="5、实验结论"><a href="#5、实验结论" class="headerlink" title="5、实验结论"></a>5、实验结论</h2><p>论文使用均方误差（MSE）作为对预测评分的评价指标。对 Yelp 2013、Yelp 2014 等 5 个国际通用数据集进行试验，结果如下所示。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-6836e07a9eaa612e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>MT 为本论文所提出的的多任务学习的推荐算法模型，由实验结果所示，该算法模型的 MSE 的结果最多个数据集的结果中都表现最优。</p>
<p>对于推荐系统的算法解释效果如何，最佳的评价方式就是线上与用户进行互动调研。但目前论文还没有这样做，论文采用了一种妥协的方式评价生成的评论质量如何。</p>
<p>利用 Perplexity的评价指标对比其他生成模型的生成效果以及 tf-idf 的相似性计算方式评价生成的评论与真实评论的近似性，结果如下所示，本论文模型生成的评论效果最佳。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-8e0500b3883cc35c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<h2 id="6、总结"><a href="#6、总结" class="headerlink" title="6、总结"></a>6、总结</h2><p>论文尝试用生成评论的方式解决推荐算法的解释合理性难题，并将预测评分率的效果达到了 state-of-the-art。评论生成的方式作为推荐解释的方式虽然存在争议，但不失为一条路径。</p>
<p>论文中涵盖了多种前沿领域的研究热点，包括对抗式网络、AutoEncoder、强化学习、多任务学习等等，是值得一看的优质应用论文。</p>
<h2 id="参考文献-6"><a href="#参考文献-6" class="headerlink" title="参考文献"></a>参考文献</h2><p>1、原文：<a href="https://dl.acm.org/citation.cfm?id=3240365" target="_blank" rel="external">https://dl.acm.org/citation.cfm?id=3240365</a><br>2、<a href="http://www.mingriqingbao.com/web/detail/forword/P/36267" target="_blank" rel="external">http://www.mingriqingbao.com/web/detail/forword/P/36267</a><br>3、<a href="https://blog.csdn.net/df19900725/article/details/78222076?locationNum=4&amp;fps=1" target="_blank" rel="external">https://blog.csdn.net/df19900725/article/details/78222076?locationNum=4&amp;fps=1</a></p>
<h1 id="推荐系统遇上深度学习-三十八-–CFGAN-一种基于GAN的协同过滤推荐框架"><a href="#推荐系统遇上深度学习-三十八-–CFGAN-一种基于GAN的协同过滤推荐框架" class="headerlink" title="推荐系统遇上深度学习(三十八)–CFGAN-一种基于GAN的协同过滤推荐框架"></a>推荐系统遇上深度学习(三十八)–CFGAN-一种基于GAN的协同过滤推荐框架</h1><p>本文论文题目：《CFGAN: A Generic Collaborative Filtering Framework based on Generative Adversarial Networks》<br>本文论文下载地址：<a href="https://dl.acm.org/citation.cfm?doid=3269206.3271743" target="_blank" rel="external">https://dl.acm.org/citation.cfm?doid=3269206.3271743</a></p>
<h2 id="1、背景-8"><a href="#1、背景-8" class="headerlink" title="1、背景"></a>1、背景</h2><p>使用GAN来进行推荐，之前已经有过IRGAN和GraphGAN的方法。</p>
<p>关于IRGAN，可以参考文章：<a href="https://www.jianshu.com/p/d151b52e57f9" target="_blank" rel="external">https://www.jianshu.com/p/d151b52e57f9</a></p>
<p>对于GraphGAN，论文本身针对于链接预测问题，可以扩展到推荐系统中，其最主要的贡献在于将图表示成宽度优先的树，并提出了graph softmax的方法，感兴趣的同学可以阅读下原文：<a href="https://arxiv.org/abs/1711.08267" target="_blank" rel="external">https://arxiv.org/abs/1711.08267</a></p>
<p>但是这两种方法都存在<strong>discrete item index generation</strong>的问题。我们首先来解释一下，什么是discrete item index generation。</p>
<p><strong>discrete item index generation</strong><br>对于IRGAN和GraphGAN来说，生成器G是基于概率，生成一个单独的项目ID或者ID列表，并通过强化学习中策略梯度的方式进行训练。在判别器的“指导”下，随着训练的进行，生成器G将生成与真实情况完全相同的项目ID。但是，这对判别器来说并不友好，同一个物品，它有可能既被标记为真实数据（real），又被标记为生成数据（fake），如下图的i<sub>3</sub>:</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-d9f3d317b347ff3b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>这样如果将其送进判别器进行判别，判别器将会产生困惑，使得判别器性能下降，之后在策略梯度迭代的过程中，判别器将向生成器提供错误的信号，自己的性能也开始降低。下面的实验结果展示了这一点：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-e81e27c2dc3965b7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-a9332aae47a6f6a7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>现象如图所示：在初始的几次迭代过程中，判别器与生成器的性能都逐渐提升，但随着实验的进行，判别器的性能突然降低，产生这样的原因便是同一物品被标记为不同类别的情况。在这种情况下，G和D之间的竞争过程也失去了原本的意义，推荐效果也存在局限。</p>
<h2 id="2、模型框架"><a href="#2、模型框架" class="headerlink" title="2、模型框架"></a>2、模型框架</h2><p>针对上面的问题，作者从原始的GAN出发。提出了一种<strong>vector-wise</strong>的训练方式。</p>
<p>我们都知道，最开始的GAN是用于图像生成的，G生成的是图像的向量，D直接判断这些向量是否是真实的图片。当G生成的是向量时，该向量可能非常与真实向量所接近，但与真实向量一模一样的概率却非常小，因此D被迷惑的概率也是非常小的。</p>
<p>在CFGAN中，G生成的是向量是什么呢？这里我们称其为<strong>购买向量purchase vector</strong>。这个也很好理解，假设我们有基于隐式反馈得到的用户-物品购买矩阵，那么从用户角度来说，购买向量便是矩阵中的一行，若用户与物品有过购买(其实不一定是购买，也可以认为是有过交互，不过本文统一认为为1的地方是用户对物品有过购买行为），该位置为1，如果没有购买，该位置是空（不是0），从物品角度来说，购买向量便是矩阵中的一列，若物品被用户购买过，该位置为1，如果没有购买过，该位置是空（不是0）。</p>
<p>因此，下面我们首先介绍从用户角度出发的CFGAN模型的设计，然后可以迁移到从物品角度出发的CFGAN框架。</p>
<h4 id="2-1-整体框架"><a href="#2-1-整体框架" class="headerlink" title="2.1 整体框架"></a>2.1 整体框架</h4><p>CFGAN的整体框架如下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-4f13de6d360f8fe3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p><strong>生成器G的设计</strong><br>生成器的输入包含两部分，一是用户向量c，另一个是噪声向量z。经过生成器的多层神经网络后，输出用户购买向量。向量中每一个值代表用户与物品交互的概率。非常巧妙的是，这里对输出的购买向量增加了一个mask。即图中下面的部分：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-b68ab37a463eef79.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>mask向量e<sub>u</sub>中，用户交互过的地方为1，没有交互过的地方为0，为什么要加入mask呢？首先，为0的地方并不代表用户不喜欢，其次，用户没有购买过的物品，在乘上0之后，其实是训练的时候丢掉了这些节点，因为乘上0之后，梯度永远为0，是不会反向传播回去的；同时，如果把真实购买向量对应位置设置为0的话，判别器在这些地方也是没有损失的。这与传统的矩阵分解方法是非常类似的，即用用户购买过的地方去训练，随后预测用户对未购买过的物品的评分。</p>
<p><strong>判别器D的设计</strong><br>判别器将用户真实的购买向量和G生成的购买向量进行混合，尽可能地将真实向量识别成real，将生成的向量识别成fake。这里同样需要输入用户向量c。</p>
<p><strong>模型训练</strong><br>生成器的损失函数为：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-fc3fb0db85dc2417.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>判别器的损失函数为：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-5716e83c95541b49.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<h4 id="2-2-框架改进"><a href="#2-2-框架改进" class="headerlink" title="2.2 框架改进"></a>2.2 框架改进</h4><p>ok，你是否觉得上述的模型讲的很有道理，近乎完美了呢？答案是否定的。咱想想看，如果生成器想要骗过判别器，非常简单就可以做到。因为没有交互过的物品经过mask之后会变成0，那么生成器只要生成全1的向量不就可以了嘛。所以说，只用用户购买过的物品，是不够的，还得通过负采样的技术，增加一定的负样本。即在每次训练迭代过程中，我们随机选择每个用户的未购买过物品的一部分，将其假设为负样本， 然后在训练生成器生成购买向量的时候，使其对应的负样本的值接近0。那么如何结合负采样技术，论文中提出了三种方式：<strong>zero- reconstruction regularization (named CFGAN−ZR)、partial-masking (named CFGAN−PM)、hybrid of the two (named CFGAN−ZP)</strong>。接下来，我们分别介绍三种方式：</p>
<p><strong>CFGAN−ZR</strong><br>ZR的方式，就是通过负采样得到一定比例的负样本，对于这些负样本，我们希望预测的分数越接近于0越好。此时G的优化目标变为：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-57a0b47a296ad5a9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>可以看到，对于ZR方式，前面部分的损失函数是没有变动的，即我们没有对mask进行修改。后面相当于增加了一个正则项，避免全1的结果出现。</p>
<p><strong>CFGAN−PM</strong></p>
<p>PM的方式，就是对mask进行修改，放开一些用户没有购买过的物品，这样，D在计算损失的时候会把这部分为交互过的部分加入，同时可以反向传播回G中，因此G不仅学习到在用户在购买过的物品上要得到接近1的输出，还会学习到在没有购买过的部分物品上要得到接近0的输出。此时G和D的损失函数变为：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-82d1962dfa35dd9b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p><strong>CFGAN−ZP</strong><br>ZP的方式便是上面两种方式的结合，G的损失函数变为：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-3ec51bbf2b9da6b7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>完整的CFGAN-ZP的过程如下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-899331747f53e7c7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<h4 id="2-3-迁移到物品角度"><a href="#2-3-迁移到物品角度" class="headerlink" title="2.3 迁移到物品角度"></a>2.3 迁移到物品角度</h4><p>迁移到物品角度，非常简单，G的生成目标变为生成物品的购买向量，D的目标变为判别物品的真实购买向量和G生成的物品购买向量。结合负采样的话，同样分为ZR、PM和ZP三种方法。</p>
<p>这样，我们其实一共得到了六种方法，从用户角度出发，我们有uCFGAN_ZR,uCFGAN_PM,uCFGAN_ZP,从物品角度出发，我们有iCFGAN_ZR,iCFGAN_PM,iCFGAN_ZP。后面我们将对比这六种方法的实验效果。</p>
<h2 id="3、模型实验"><a href="#3、模型实验" class="headerlink" title="3、模型实验"></a>3、模型实验</h2><p>模型实验主要关注于三个问题：<br>1）CFGAN模型应用于推荐任务，效果如何？<br>2）CFGAN模型使用时，最佳的参数设置为多少？<br>3）与目前state-of-the-art方法比较，CFGAN能够带来多大的提升？</p>
<p>实验选取的数据集包括以下四个：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-133cc7a4375e9e5f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>接下来，我们看下结果。</p>
<h4 id="3-1-模型的有效性"><a href="#3-1-模型的有效性" class="headerlink" title="3.1 模型的有效性"></a>3.1 模型的有效性</h4><p>该部分的实验效果如下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-5ff41198fcb1f88e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>对比我们之前看到的IRGAN和GraphGAN的效果，CFGAN相较于IRGAN准确率提升了72.6%，较GraphGAN提升了104.3%。</p>
<h4 id="3-2-超参数设置对于模型的影响"><a href="#3-2-超参数设置对于模型的影响" class="headerlink" title="3.2 超参数设置对于模型的影响"></a>3.2 超参数设置对于模型的影响</h4><p>这里主要的超参数包括ZR和PM方式中，负样本的比例，以及ZR方式中，正则项系数α。结果如下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-d29a07d0f0f2c18f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<h4 id="3-3-与目前主流推荐方法的比较"><a href="#3-3-与目前主流推荐方法的比较" class="headerlink" title="3.3 与目前主流推荐方法的比较"></a>3.3 与目前主流推荐方法的比较</h4><p>这里选取的推荐方法有ItemPop、BPR、FISM、CDAE、IRGAN和GraphGAN。而CFGAN选择iCFGAN−ZP方式。对比结果如下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-96f47cb94f59ffe8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>可以看到，CFGAN在各个数据集上，都有明显的效果提升。</p>
<h2 id="4、总结-2"><a href="#4、总结-2" class="headerlink" title="4、总结"></a>4、总结</h2><p>本文介绍了一种通过GAN来进行推荐任务的新思路，与之前结合强化学习的思路如IRGAN和GraphGAN不同，CFGAN更接近于GAN模型，G生成的是用户或者物品的购买向量，D的梯度可以反向传播回给G。该方法解决了IRGAN和GraphGAN中存在的discrete item index generation问题，取得了非常好的实验效果。值得大家阅读！</p>
<h1 id="推荐系统遇上深度学习-三十九-推荐系统中召回策略演进！"><a href="#推荐系统遇上深度学习-三十九-推荐系统中召回策略演进！" class="headerlink" title="推荐系统遇上深度学习(三十九)-推荐系统中召回策略演进！"></a>推荐系统遇上深度学习(三十九)-推荐系统中召回策略演进！</h1><p>推荐系统中的核心是从海量的商品库挑选合适商品最终展示给用户。由于商品库数量巨大，因此常见的推荐系统一般分为两个阶段，即召回阶段和排序阶段。召回阶段主要是从全量的商品库中得到用户可能感兴趣的一小部分候选集，排序阶段则是将召回阶段得到的候选集进行精准排序，推荐给用户。</p>
<p>本篇我们来总结一下推荐系统中几种常用的召回策略。主要有协同过滤、向量化召回和阿里最近在Aicon中提到的深度树匹配模型。</p>
<h2 id="1、协同过滤"><a href="#1、协同过滤" class="headerlink" title="1、协同过滤"></a>1、协同过滤</h2><p>协同过滤主要可以分为基于用户的协同过滤、 基于物品的协同过滤。当然还有基于模型的协同过滤，如矩阵分解等，本文不对这部分进行介绍。</p>
<h4 id="1-1-基于用户的协同过滤"><a href="#1-1-基于用户的协同过滤" class="headerlink" title="1.1 基于用户的协同过滤"></a>1.1 基于用户的协同过滤</h4><p>基于用户的协同过滤算法的基本思想是：当召回用户A的候选集时，可以先找到和他有相似兴趣的其他用户，然后把那些用户喜欢的、而用户A没有未交互的物品作为候选集。</p>
<p>因此，我们首先需要计算两个用户的兴趣相似度。给定用户u和用户v，令N(u)表示用户u曾经有过正反馈的物品集合，令N(v) 为用户v曾经有过正反馈的物品集合。那么我们可以通过以下两种方法计算用户的相似度：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/4155986-c07a19e650bbdd2f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="基于Jaccard公式"></p>
<p><img src="http://upload-images.jianshu.io/upload_images/4155986-c94fc5b6694567f4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="基于余弦相似度"></p>
<p>余弦相似度为什么是上面这种写法呢，因为这里，我们并不是用的用户对物品的评分，而是用的0-1表示，所以对两个集合做交集，相当于进行了点乘。如果我们的矩阵是用户对物品的评分，那么计算余弦相似度的时候可以利用用户的具体评分而不是0-1值。</p>
<p>如果简单的基于余弦相似度，显得过于粗糙，以图书为例，如果两个用户都曾经买过《新华字典》，这丝毫不能说明他们兴趣相似， 因为绝大多数中国人小时候都买过《新华字典》。但如果两个用户都买过《数据挖掘导论》，那可 以认为他们的兴趣比较相似，因为只有研究数据挖掘的人才会买这本书。换句话说，两个用户对冷门物品采取过同样的行为更能说明他们兴趣的相似度，因此，我们可以基于物品的流行度对热门物品进行一定的惩罚：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/4155986-6e0dd341b4fa3103.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>得到用户之间的兴趣相似度后，UserCF算法会给用户推荐和他兴趣最相似的K个用户喜欢的 物品。如下的公式度量了UserCF算法中用户u对物品i的感兴趣程度:</p>
<p><img src="http://upload-images.jianshu.io/upload_images/4155986-f396d4d0742cd12e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>其中，S(u, K)包含和用户u兴趣最接近的K个用户，N(i)是对物品i有过行为的用户集合，w<sub>uv</sub>是用户u和用户v的兴趣相似度，r<sub>vi</sub>代表用户v对物品i的兴趣.</p>
<h4 id="1-2-基于物品的协同过滤"><a href="#1-2-基于物品的协同过滤" class="headerlink" title="1.2 基于物品的协同过滤"></a>1.2 基于物品的协同过滤</h4><p>UserCF在一些网站(如Digg)中得到了应用，但该算法有一些缺点。首先， 随着网站的用户数目越来越大，计算用户兴趣相似度矩阵将越来越困难，其运算时间复杂度和空间复杂度的增长和用户数的增长近似于平方关系。其次，基于用户的协同过滤很难对推荐结果作出解释。因此，著名的电子商务公司亚马逊提出了另一个算法——基于物品的协同过滤算法。<br>基于物品的协同过滤算法(简称ItemCF)给用户推荐那些和他们之前喜欢的物品相似的物品。 比如，该算法会因为你购买过《数据挖掘导论》而给你推荐《机器学习》。不过，ItemCF算法并不利用物品的内容属性计算物品之间的相似度，它主要通过分析用户的行为记录计算物品之间的相似度。该算法认为，物品A和物品B具有很大的相似度是因为喜欢物品A的用户大都也喜欢物品 B。</p>
<p>基于物品的协同过滤算法主要分为两步。<br>(1) 计算物品之间的相似度。<br>(2) 根据物品的相似度和用户的历史行为给用户生成召回候选集。</p>
<p>ItemCF的第一步是计算物品之间的相似度，在网站中，我们经常看到这么一句话：Customers Who Bought This Item Also Bought，那么从这句话的定义出发，我们可以用下面的公式定义物品相似度：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/4155986-93585daff46fd7a9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>这里，分母|N(i)|是喜欢物品i的用户数，而分子 N(i)N(j) 是同时喜欢物品i和物品j的用户数。因此，上述公式可以理解为喜欢物品i的用户中有多少比例的用户也喜欢物品j。但是却存在一个问题。如果物品j很热门，很多人都喜欢，那么W<sub>ij</sub>就会很大，接近1。因此，该公式会造成任何物品都会和热门的物品有很大的相似度，这 对于致力于挖掘长尾信息的推荐系统来说显然不是一个好的特性。为了避免推荐出热门的物品，可以用下面的公式:</p>
<p><img src="http://upload-images.jianshu.io/upload_images/4155986-5904c3f7334d8559.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>这里由于还是0-1的原因，我们的余弦相似度可以写成上面的形式。但是，是不是每个用户的贡献都相同呢? 假设有这么一个用户，他是开书店的，并且买了当当网上80%的书准备用来自己卖。那么， 他的购物车里包含当当网80%的书。假设当当网有100万本书，也就是说他买了80万本。从前面 对ItemCF的讨论可以看到，这意味着因为存在这么一个用户，有80万本书两两之间就产生了相似度。这个用户虽然活跃，但是买这些书并非都是出于自身的兴趣，而且这些书覆 盖了当当网图书的很多领域，所以这个用户对于他所购买书的两两相似度的贡献应该远远小于一个只买了十几本自己喜欢的书的文学青年。因此，我们要对这样的用户进行一定的惩罚，John S. Breese在论文1中提出了一个称为IUF(Inverse User Frequence)，即用户活跃度对数的 倒数的参数，他也认为活跃用户对物品相似度的贡献应该小于不活跃的用户，他提出应该增加IUF参数来修正物品相似度的计算公式:</p>
<p><img src="http://upload-images.jianshu.io/upload_images/4155986-f86bd9060a1fa3c0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>在得到物品之间的相似度后，ItemCF通过如下公式计算用户u对一个物品j的兴趣:</p>
<p><img src="http://upload-images.jianshu.io/upload_images/4155986-e90a31e7bf7b44f0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>这里N(u)是用户喜欢的物品的集合，S(j,K)是和物品j最相似的K个物品的集合，w<sub>ji</sub>是物品j和i 的相似度，r<sub>ui</sub>是用户u对物品i的兴趣。</p>
<h4 id="1-3-UserCF和ItemCF的比较"><a href="#1-3-UserCF和ItemCF的比较" class="headerlink" title="1.3 UserCF和ItemCF的比较"></a>1.3 UserCF和ItemCF的比较</h4><p>先说结论：新闻网站一般使用UserCF，而图书、电商网站一般使用ItemCF！<br>首先回顾一下UserCF算法和ItemCF算法的推荐原理。UserCF给用户推荐那些和他有共同兴 趣爱好的用户喜欢的物品，而ItemCF给用户推荐那些和他之前喜欢的物品类似的物品。从这个算 法的原理可以看到，UserCF的推荐结果着重于反映和用户兴趣相似的小群体的热点，而ItemCF 的推荐结果着重于维系用户的历史兴趣。换句话说，UserCF的推荐更社会化，反映了用户所在的小型兴趣群体中物品的热门程度，而ItemCF的推荐更加个性化，反映了用户自己的兴趣传承。<br>在新闻网站中，用户的兴趣不是特别细化，绝大多数用户都喜欢看热门的新闻。个性化新闻推荐更加强调抓住 新闻热点，热门程度和时效性是个性化新闻推荐的重点，而个性化相对于这两点略显次要。因 此，UserCF可以给用户推荐和他有相似爱好的一群其他用户今天都在看的新闻，这样在抓住热 点和时效性的同时，保证了一定程度的个性化。同时，在新闻网站中，物品的更新速度远远快于新用户的加入速度，而且 对于新用户，完全可以给他推荐最热门的新闻，因此UserCF显然是利大于弊。</p>
<p>但是，在图书、电子商务和电影网站，比如亚马逊、豆瓣、Netflix中，ItemCF则能极大地发 挥优势。首先，在这些网站中，用户的兴趣是比较固定和持久的。一个技术人员可能都是在购买 技术方面的书，而且他们对书的热门程度并不是那么敏感，事实上越是资深的技术人员，他们看 的书就越可能不热门。此外，这些系统中的用户大都不太需要流行度来辅助他们判断一个物品的 好坏，而是可以通过自己熟悉领域的知识自己判断物品的质量。因此，这些网站中个性化推荐的 任务是帮助用户发现和他研究领域相关的物品。因此，ItemCF算法成为了这些网站的首选算法。 此外，这些网站的物品更新速度不会特别快，一天一次更新物品相似度矩阵对它们来说不会造成 太大的损失，是可以接受的。同时，从技术上考虑，UserCF需要维护一个用户相似度的矩阵，而ItemCF需要维护一个物品 相似度矩阵。从存储的角度说，如果用户很多，那么维护用户兴趣相似度矩阵需要很大的空间， 同理，如果物品很多，那么维护物品相似度矩阵代价较大。</p>
<p>下表是对二者的一个全面的比较：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/4155986-94420bdd6bb22f47.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<h4 id="1-4-协同过滤总结"><a href="#1-4-协同过滤总结" class="headerlink" title="1.4 协同过滤总结"></a>1.4 协同过滤总结</h4><p>协同过滤方法通过在用户历史行为里面找相似的商品和用户，保证了基础的相关性。与此同时，因为只找相似的商品或相似用户的商品，所以系统屏蔽了大规模的计算，使整个召回的过程能够高效地完成。</p>
<p>但是协同过滤方法存在一定的弊端：在召回的时候，并不能真正的面向全量商品库来做检索，如itemCF方法，系统只能在用户历史行为过的商品里面找到侯选的相似商品来做召回，使得整个推荐结果的多样性和发现性比较差。这样做的结果就是，用户经常抱怨：为什么总给我推荐相同的东西！</p>
<h2 id="2、向量化召回"><a href="#2、向量化召回" class="headerlink" title="2、向量化召回"></a>2、向量化召回</h2><p>向量化召回，主要通过模型来学习用户和物品的兴趣向量，并通过内积来计算用户和物品之间的相似性，从而得到最终的候选集。其中，比较经典的模型便是Youtube召回模型。在实际线上应用时，由于物品空间巨大，计算用户兴趣向量和所有物品兴趣向量的内积，耗时十分巨大，有时候会通过局部敏感Hash等方法来进行近似求解。</p>
<h4 id="2-1-Youtube召回模型"><a href="#2-1-Youtube召回模型" class="headerlink" title="2.1 Youtube召回模型"></a>2.1 Youtube召回模型</h4><p>Youtube召回模型的架构如下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-1104f822da782760.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>从模型结构可以看出，在离线训练阶段，将其视为一个分类问题。我们使用隐式反馈来进行学习，用户完整观看过一个视频，便视作一个正例。如果将视频库中的每一个视频当作一个类别，那么在时刻t，对于用户U和上下文C，用户会观看视频i的概率为：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-2fa775e4d7be048b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>其中，u是用户的embedding，这个embedding，是网络最后一个Relu激活函数的输出，v<sub>i</sub>是视频i的embedding。那么问题来了，输入时，每一个视频也有一个对应的embedding，这个embedding是不是计算softmax的embedding呢？个人认为是两组不同的embedding，输入层的embedding分别是用户空间和视频空间的向量，最终的输出层，二者通过一系列全联接层的线性变化，转换到了同一空间，所以对于用户和视频来说，输出层的embedding是同一空间，可以认为是兴趣空间，这样二者的内积可以代表相似性。</p>
<p>使用多分类问题的一个弊端是，我们有百万级别的classes，模型是非常难以训练的，因此在实际中，Youtube并使用负样本采样(negative sampling)的方法，将class的数量减小。</p>
<p>对于在线服务来说，有严格的性能要求，必须在几十毫秒内返回结果。因此，youtube没有重新跑一遍模型，而是通过保存用户兴趣embedding和视频兴趣embedding，通过最近邻搜索的方法得到top N的结果。该近似方法中的代表是局部敏感Hash方法。我们在下一节中进行介绍。</p>
<h4 id="2-2-局部敏感哈希-Locality-Sensitive-Hashing-LSH"><a href="#2-2-局部敏感哈希-Locality-Sensitive-Hashing-LSH" class="headerlink" title="2.2 局部敏感哈希(Locality-Sensitive Hashing, LSH)"></a>2.2 局部敏感哈希(Locality-Sensitive Hashing, LSH)</h4><p>这里我们简单介绍一下局部敏感哈希(Locality-Sensitive Hashing, LSH)的基本思想，更加详细的介绍可以参考参考文献3。</p>
<p>LSH的基本思想如下：我们首先对原始数据空间中的向量进行hash映射，得到一个hash table，我们希望，原始数据空间中的两个相邻向量通过相同的hash变换后，被映射到同一个桶的概率很大，而不相邻的向量被映射到同一个桶的概率很小。因此，在召回阶段，我们便可以将所有的物品兴趣向量映射到不同的桶内，然后将用户兴趣向量映射到桶内，此时，只需要将用户向量跟该桶内的物品向量求内积即可。这样计算量被大大减小。</p>
<p>关键的问题是，如何确定hash-function？在LSH中，合适的hash-function需要满足下面两个条件：<br>1）如果d(x,y) ≤ d1， 则h(x) = h(y)的概率至少为p1；<br>2）如果d(x,y) ≥ d2， 则h(x) = h(y)的概率至多为p2；<br>其中d(x,y)表示x和y之间的距离， h(x)和h(y)分别表示对x和y进行hash变换。</p>
<p>满足以上两个条件的hash function称为(d1,d2,p1,p2)-sensitive。而通过一个或多个(d1,d2,p1,p2)-sensitive的hash function对原始数据集合进行hashing生成一个或多个hash table的过程称为Locality-sensitive Hashing。</p>
<h4 id="2-3-向量化召回评价"><a href="#2-3-向量化召回评价" class="headerlink" title="2.3 向量化召回评价"></a>2.3 向量化召回评价</h4><p>向量化召回是目前推荐召回核心发展的一代技术，但是它对模型结构做了很大的限制，必须要求模型围绕着用户和向量的embedding展开，同时在顶层进行内积运算得到相似性。在深度学习领域其实模型结构层出不穷，百花齐放，但是这样一个特定的结构实际上对模型能力造成了很大的限制。</p>
<h2 id="3、深度树匹配"><a href="#3、深度树匹配" class="headerlink" title="3、深度树匹配"></a>3、深度树匹配</h2><p>上面两种方法，揭示了召回中两个比较关键的问题：<strong>全库搜索</strong>、<strong>先进模型</strong>。如果说向量化召回通过内积运算的方式打开了全库搜索的天花板，那么下一阶段应该是：能否设计一套全新的推荐算法框架，它允许容纳任意先进的模型而非限定内积形式，并且能够对全库进行更好的检索。<strong>深度树匹配</strong>，就是从这个视角出发做的技术探索。这里我们简单介绍一下<strong>深度树匹配（Tree-based Deep Match，TDM）</strong>技术，ppt和详细介绍参照文献5和6。</p>
<p>深度树匹配的核心是构造一棵兴趣树，其叶子结点是全量的物品，每一层代表一种细分的兴趣：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-cbaf6a8236cbba12.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>接下来，我们主要介绍三个方面的内容：<br>1）怎么基于树来实现高效的检索<br>2）怎么在树上面做兴趣建模<br>3）兴趣树是怎么构建的</p>
<h4 id="3-1-怎么基于树来实现高效的检索"><a href="#3-1-怎么基于树来实现高效的检索" class="headerlink" title="3.1 怎么基于树来实现高效的检索"></a>3.1 怎么基于树来实现高效的检索</h4><p>在这里，假设已经得到深度树的情况下，高效检索采用的是Beam-Search的方式：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-6bff4b1e8d3ec8ad.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<h4 id="3-2-怎么在树上面做兴趣建模"><a href="#3-2-怎么在树上面做兴趣建模" class="headerlink" title="3.2 怎么在树上面做兴趣建模"></a>3.2 怎么在树上面做兴趣建模</h4><p>在已经得到深度树的情况下，一个新来的用户，我们怎么知道他对哪个分支的兴趣更大呢？我们首先需要将树建立为一棵最大堆树。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-40f93e5bcec661f7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>在实践中，构造最大堆树可以举个简单的例子，假设用户对叶子层 ITEM6 这样一个节点是感兴趣的，那么可以认为它的兴趣是 1，同层其他的节点兴趣为 0，从而也就可以认为 ITEM6 的这个节点上述的路径的父节点兴趣都为 1，那么这一层就是 SN3 的兴趣为 1，其他的为 0，这层就是 LN2 的兴趣为 1，其他为 0。如下图所示：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-e63997cc3ff9eda5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>当建立起如上的树之后，我们就可以在每一层构建一定的正负样本，通过构建模型来学习用户对于每一层节点的兴趣偏好。注意的是，每层的偏好都要学习，也就是说每层都要构建一个模型。同时，模型只需要关心是否足够拟合样本就可以了，并没有要求模型一定要把用户特征和 item 特征转换为顶层向量内积的形式，这样就给了模型很大的自由度，只要去拟合好足够的样本，那么任意的模型都是 OK 的。下面是一个模型的示例：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-73995f68b88ba2d6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<h4 id="3-3-兴趣树是怎么构建的"><a href="#3-3-兴趣树是怎么构建的" class="headerlink" title="3.3 兴趣树是怎么构建的"></a>3.3 兴趣树是怎么构建的</h4><p>前面两个问题，都是在给定树结构的情况下来介绍的，那么怎么来构建一棵兴趣树呢？每层是怎么分叉的呢？</p>
<p>树的叶节点对应具体的 item，目标是构建一个好的树结构来优化我们的检索效果。通过前面的分析知道，在进行兴趣建模时，对于叶子层的样本我们通过用户行为反馈得到，而中间层的样本则通过树结构采样得到。所以树结构决定了中间层的样本。</p>
<p>在进行快速检索时，采用从顶向下的检索策略，利用的是对每一层节点兴趣建模进行快速剪枝。要保证最终的检索效果，就需要每一层的兴趣判别模型能力足够强。由于树结构负责我们中间层的样本生成，所以我们的思路是通过优化树结构影响样本生成进而提升模型能力。具体来说，通过树结构优化降低中间层的样本混淆度，让中间层样本尽可能可分。</p>
<p>所以，整个树结构的生成创建和优化的过程，实际上是围绕着怎么来生成更好的样本、帮助模型学习的视角进行的，而不是只是考虑相似、聚类这样的模式。那么这里的核心方案是什么呢？</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-f3de0f31b9062f0a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>方案总结来说，就是最小化用户行为序列中相近的item-pair在树上的距离。假设用户的行为序列为A-》B-》D-》C，那么我们希望(A,B),(B,D),(D,C)在树上的距离越近越好。两个叶子结点的距离通过其最近的公共祖先确定。</p>
<p>好了，到这里，对深度树匹配模型做一个简单的总结：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-3a8e8cb26c15c6ee.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<h2 id="4、总结-3"><a href="#4、总结-3" class="headerlink" title="4、总结"></a>4、总结</h2><p>本文介绍了推荐系统在召回阶段常用的模型，有协同过滤模型、向量化召回模型和深度树匹配算法。</p>
<p>协同过滤模型无法做到全局检索，而向量化模型对模型的结构进行了限制。深度树匹配模型解决了上述两个方面的限制，可以做到全局检索+使用先进模型。</p>
<h2 id="参考文献-7"><a href="#参考文献-7" class="headerlink" title="参考文献"></a>参考文献</h2><p>1、推荐系统理论(二) – 利用用户行为数据进行推荐(协同过滤)：<a href="https://www.jianshu.com/p/8d90824d52c5" target="_blank" rel="external">https://www.jianshu.com/p/8d90824d52c5</a><br>2、项亮：《推荐系统实践》<br>3、局部敏感hash：<a href="https://www.cnblogs.com/wt869054461/p/8148940.html" target="_blank" rel="external">https://www.cnblogs.com/wt869054461/p/8148940.html</a><br>4、推荐系统遇上深度学习(三十四)–YouTube深度学习推荐系统：<a href="https://www.jianshu.com/p/8fa4dcbd5588" target="_blank" rel="external">https://www.jianshu.com/p/8fa4dcbd5588</a><br>5、深度树匹配slide：<a href="https://myslide.cn/slides/10614" target="_blank" rel="external">https://myslide.cn/slides/10614</a><br>6、深度树匹配详解：<a href="http://www.6aiq.com/article/1554659383706" target="_blank" rel="external">http://www.6aiq.com/article/1554659383706</a></p>
<h1 id="推荐系统遇上深度学习-四十-SESSION-BASED-RECOMMENDATIONS-WITH-RECURRENT-NEURAL-NETWORKS"><a href="#推荐系统遇上深度学习-四十-SESSION-BASED-RECOMMENDATIONS-WITH-RECURRENT-NEURAL-NETWORKS" class="headerlink" title="推荐系统遇上深度学习(四十)-SESSION-BASED-RECOMMENDATIONS-WITH-RECURRENT-NEURAL-NETWORKS"></a>推荐系统遇上深度学习(四十)-SESSION-BASED-RECOMMENDATIONS-WITH-RECURRENT-NEURAL-NETWORKS</h1><p>好啦，是时候继续我们推荐系统的学习了，从本篇开始，我们来一起了解一下Session-Based Recommendation。今天，我们介绍的文章题目为《SESSION-BASED RECOMMENDATIONS WITH RECURRENT NEURAL NETWORKS》，通过循环神经网络来进行会话推荐。论文下载地址为：<a href="http://arxiv.org/abs/1511.06939。" target="_blank" rel="external">http://arxiv.org/abs/1511.06939。</a></p>
<p>另外，本文代码的地址为：<a href="https://github.com/princewen/tensorflow_practice/tree/master/recommendation/Basic-SessionBasedRNN-Demo" target="_blank" rel="external">https://github.com/princewen/tensorflow_practice/tree/master/recommendation/Basic-SessionBasedRNN-Demo</a></p>
<p>参考的Python2版本代码地址为：<a href="https://github.com/Songweiping/GRU4Rec_TensorFlow" target="_blank" rel="external">https://github.com/Songweiping/GRU4Rec_TensorFlow</a></p>
<p>先来理解一下Session-Based Recommendation的定义。它的中文翻译是基于会话的推荐，我们可以理解为从进入一个app直到退出这一过程中，根据你的行为变化所发生的推荐；也可以理解为根据你较短时间内的行为序列发生的推荐，这时session不一定是从进入app到离开，比如airbnb的论文中，只要前后两次的点击不超过30min，都算做同一个session。</p>
<h2 id="1、模型介绍"><a href="#1、模型介绍" class="headerlink" title="1、模型介绍"></a>1、模型介绍</h2><h4 id="1-1-背景介绍"><a href="#1-1-背景介绍" class="headerlink" title="1.1 背景介绍"></a>1.1 背景介绍</h4><p>在本文出现之前（2016年），基于会话的推荐方法，主要有基于物品的协同过滤和基于马尔可夫决策过程的方法。</p>
<p>基于物品的协同过滤，需要维护一张物品的相似度矩阵，当用户在一个session中点击了某一个物品时，基于相似度矩阵得到相似的物品推荐给用户。这种方法简单有效，并被广泛应用，但是这种方法只把用户上一次的点击考虑进去，而没有把前面多次的点击都考虑进去（论文里这么说，不过我认为可以按比例混合多次点击的推荐结果吧）。</p>
<p>基于马尔可夫决策过程的推荐方法，也就是强化学习方法，其主要学习的是状态转移概率，即点击了物品A之后，下一次点击的物品是B的概率，并基于这个状态转移概率进行推荐。这样的缺陷主要是随着物品的增加，建模所有的可能的点击序列是十分困难的（可能论文年代比较久远，现在的话我们应该可以使用DQN等方法了）。</p>
<h4 id="1-2-基于RNN的会话推荐"><a href="#1-2-基于RNN的会话推荐" class="headerlink" title="1.2 基于RNN的会话推荐"></a>1.2 基于RNN的会话推荐</h4><p>回到正题，文中提出使用基于RNN的方法来进行基于会话的推荐，其结构图如下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-fd681f76356228e3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>模型的结构很简单，对于一个Session中的点击序列x=[x<sub>1</sub>,x<sub>2</sub>,x<sub>3</sub>…x<sub>r-1</sub>,x<sub>r</sub>]，依次将x<sub>1</sub>、x<sub>2</sub>,…,x<sub>r-1</sub>输入到模型中，预测下一个被点击的是哪一个Item。</p>
<p>首先，序列中的每一个物品x<sub>t</sub>被转换为one-hot，随后转换成其对应的embedding，经过N层GRU单元后，经过一个全联接层得到下一次每个物品被点击的概率。</p>
<p>值得一提的是，在我参考的代码中，每个物品其实对应了两套embedding，一个是输入层对应的输入embedding，一个是最后计算物品点击概率的embedding，我们可以记为softmax embedding。输入物品x<sub>t</sub>的输入embedding在经过多层的GRU单元之后，得到了一个输出向量，我们记为w<sub>1</sub>，对于物品y<sub>t</sub>，其对应的softmax embedding记为w<sub>2</sub>,那么点击y<sub>t</sub>（未softmax之前）的概率，由w<sub>1</sub>和w<sub>2</sub>的点乘得到。这里有点attention的味道了，具体大家可以参考给出的代码。</p>
<h4 id="1-3-模型的小trick"><a href="#1-3-模型的小trick" class="headerlink" title="1.3 模型的小trick"></a>1.3 模型的小trick</h4><p>为了提高训练的效率，文章采用两种策略来加快简化训练代价，分别为：</p>
<p><strong>Session-parallel MINI-BATCHES</strong><br>为了更好的并行计算，论文采用了 mini-batch 的处理，即把不同的session拼接起来，其示意图如下所示：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-3e01769c6f823a7a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>可以看到，Session1的长度为4，Session2的长度为2，Session3的长度为6，Session4的长度为2，Session5的长度为3。假设Batch-Size为3，那么我们首先用前三个Session进行训练，不过当训练到第三个物品时，Session2已经结束了，那么我们便将Session4来接替上，不过这里要注意将GRU中的状态重新初化。</p>
<p><strong>Sampling On the output</strong><br>物品数量如果过多的话，模型输出的维度过多，计算量会十分庞大，因此在实践中一般采取负采样的方法。论文采用了取巧的方法来减少采样需要的计算量，即选取了同一个batch 中其他 sequence 下一个点击的 item作为负样本，用这些正负样本来训练整个神经网络。</p>
<p>还是如上图所示，当输入是i<sub>1,1</sub>时，其对应的正样本为i<sub>1,2</sub>，那么对应的负样本就是i<sub>2,2</sub>,i<sub>3,2</sub>。</p>
<h4 id="1-4-Rank-Loss"><a href="#1-4-Rank-Loss" class="headerlink" title="1.4 Rank Loss"></a>1.4 Rank Loss</h4><p>这里，论文提出了两种pair-wise的损失函数，分别为<strong>BPR</strong>和<strong>TOP1</strong>。</p>
<p><strong>BPR</strong><br>BPR损失，对比了正样本和每个负样本的点击概率值，其计算公式如下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-4bc69a6f3214534e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>其中，i代表的是正样本，j代表的是负样本，若正样本的点击概率大于负样本的点击概率，这样损失会比较小，若正样本的点击概率小于负样本，损失会比较大。</p>
<p><strong>TOP1</strong><br>第二个损失函数感觉和第一个损失函数差不多，只不过对负样本的点击概率增加了正则项，同时sigmoid之后没有再取log：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-2e21a03dd78ee30b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>##1.5 实验结果</p>
<p>文中将提出的模型与按热度推荐、基于会话的热度推荐、基于物品的协同过滤、BPR-MF等模型进行了对比数据，所选取的两个数据集分别为RecSys Challenge 2015的数据集和Youtube-like OTT video的数据集。评价指标选取了Recall@20和MRR@20，实验结果如下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-4e038d04152a2863.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>随后，在不同数据集上，使用不同损失函数训练模型，评价指标的对比结果为：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-0752f41f36717a38.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>可以看到，使用文中提出的两种Rank Loss相较于使用point-wise的交叉熵损失函数，模型的推荐效果有了较大的提升。</p>
<h4 id="1-6-关于损失函数的讨论"><a href="#1-6-关于损失函数的讨论" class="headerlink" title="1.6 关于损失函数的讨论"></a>1.6 关于损失函数的讨论</h4><p>好了，最后我们来讨论一下我阅读本文时最为疑虑的地方吧，为什么使用pair-wise的损失函数，要比point-wise的损失函数更好呢？这主要还是看场景吧。比如在电商领域、外卖点餐的时候，我们可能很多东西都喜欢，但是只会挑选一个最喜欢的物品进行点击或者购买。这种情况下并不是一个非黑即白的classification问题，只是说相对于某个物品，我们更喜欢另一个物品，这时候更多的是体现用户对于不同物品的一个相对偏好关系，此时使用pair-wise的损失函数效果可能会好一点。在广告领域，一般情况下用户只会展示一个广告，用户点击了就是点击了，没点击就是没点击，我们可以把它当作非黑即白的classification问题，使用point-wise的损失函数就可以了。</p>
<p>不过还是要提一点，相对于使用point-wise的损失函数，使用pair-wise的损失函数，我们需要采集更多的数据，如果在数据量不是十分充足的情况下，point-wise的损失函数也许是更合适的选择。</p>
<p>以上仅仅是我个人的观点，咱们可以一起讨论，进步！</p>
<h1 id="推荐系统遇上深度学习-四十一-Improved-Recurrent-Neural-Networks-for-Session-based-Recommendations"><a href="#推荐系统遇上深度学习-四十一-Improved-Recurrent-Neural-Networks-for-Session-based-Recommendations" class="headerlink" title="推荐系统遇上深度学习(四十一)-Improved-Recurrent-Neural-Networks-for-Session-based-Recommendations"></a>推荐系统遇上深度学习(四十一)-Improved-Recurrent-Neural-Networks-for-Session-based-Recommendations</h1><p>本文论文的题目是《Improved Recurrent Neural Networks for Session-based Recommendations》<br>论文下载地址为：<a href="https://arxiv.org/abs/1606.08117" target="_blank" rel="external">https://arxiv.org/abs/1606.08117</a></p>
<p>本文仍然使用RNN做基于会话的推荐，但在此基础上，提出了几种提升预测效果的方法，我们一起来学习一下吧。</p>
<h2 id="1、基础模型"><a href="#1、基础模型" class="headerlink" title="1、基础模型"></a>1、基础模型</h2><p>基本的RNN模型如下图所示：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-1142b2ab24694f99.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>而对于其中一个序列，其过程如下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-83d42241b7e8e93e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>对于一个输入序列<strong>x</strong>= [x<sub>1</sub>,x<sub>2</sub>,….,x<sub>r-1</sub>,x<sub>r</sub>]，模型输出y=M(x)，并使用交叉熵损失或者rank的损失函数（如上一篇中提到的BPR和TOP1损失函数）来进行模型的训练。</p>
<h2 id="2、模型改进"><a href="#2、模型改进" class="headerlink" title="2、模型改进"></a>2、模型改进</h2><p>本节介绍几种针对基础模型的改进。包括<strong>Data augmentation</strong>、<strong>Adapting to temporal changes</strong>、<strong>Use of privileged information</strong>、<strong>Output embeddings for faster predictions</strong>，咱们细细道来。</p>
<h4 id="2-1-Data-augmentation"><a href="#2-1-Data-augmentation" class="headerlink" title="2.1 Data augmentation"></a>2.1 Data augmentation</h4><p>第一种方式是数据增强，本文提出了两种增强的方式。</p>
<p>第一种方式，便是将一条长度为n的序列拆分成n-1条训练数据，假设一条长度为4的序列(l1,l2,l3,l4),将其拆分成3条数据，即((l1),l2),((l1,l2),l3),((l1,l2,l3),l4)。如下图所示：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-8be2f05b3cb91521.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>第二种方式，是将点击序列中的一些数据随机的丢掉，可以增强训练的鲁棒性，如下图所示：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-4d7d30d2d8f79ff0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<h4 id="2-2-Adapting-to-temporal-changes"><a href="#2-2-Adapting-to-temporal-changes" class="headerlink" title="2.2 Adapting to temporal changes"></a>2.2 Adapting to temporal changes</h4><p>用户的行为偏好是随着时间而变化的，近期的行为能够更好的代表当前用户的偏好。因此啊，我们可以定义一个近期的时间节点，比如近半年之内，只用这部分数据去训练模型。但是呢，这样会造成训练数据太少。</p>
<p>所以文中使用预训练的方法。即用所有的数据先预训练模型，然后只用近期的数据进行模型的进一步训练。</p>
<h4 id="2-3-Use-of-privileged-information"><a href="#2-3-Use-of-privileged-information" class="headerlink" title="2.3 Use of privileged information"></a>2.3 Use of privileged information</h4><p>这里是使用privileged information（不知是否可以翻译为超越信息）来训练模型。假设有序列[x<sub>1</sub>,x<sub>2</sub>,….,x<sub>r</sub>,x<sub>r+1</sub>,…,x<sub>n-1</sub>,x<sub>n</sub>]，当此条训练数据是使用[x<sub>1</sub>,x<sub>2</sub>,….,x<sub>r</sub>]预测x<sub>r+1</sub>，那么其对应的privileged information是[x<sub>n</sub>,x<sub>n-1</sub>,…,x<sub>r+2</sub>]。</p>
<p>思路是，用户点击某item后的点击序列中实际上能提供该item的信息，这些信息尽管在实际预测时使用不上，但在训练时我们可以加以利用。具体做法上，先使用privileged information训练一个模型，作为teacher模型，然后训练一个student模型，即我们实际想要学习的模型。</p>
<p>假设teacher模型是M*，模型输出是M*(x*)，student模型是M，模型输出为M(x)，预测的实际输出(即label对应的one-hot encoding)为V(x<sub>n</sub>)，那么此时的损失函数为：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-5a27b4d1c7fc9437.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<h4 id="2-4-Output-embeddings-for-faster-predictions"><a href="#2-4-Output-embeddings-for-faster-predictions" class="headerlink" title="2.4 Output embeddings for faster predictions"></a>2.4 Output embeddings for faster predictions</h4><p>模型在最后输出层的参数数目为H * N，H是GRU单元的size，N是item的数量。当我们的item数目过多的时候，这样不仅训练慢，同时在预测阶段的时间也会比较长。有两种常见的方法，即我们在word2vec中见过的，hierarchical softmax和负采样。</p>
<p>本文提出了一种新的做法，即输出层预测的不再是点击每个item的概率，而是直接输出item的embedding，并与label对应的item的embedding进行对比，计算cosine距离作为损失。</p>
<p>但是，这种方法需要item的embedding十分准确，本文提出的方法是使用模型训练出的item embedding作为label。而这里的模型可以是使用基准模型+前三种改进方式训练出的模型。</p>
<h2 id="3、实验效果及结论"><a href="#3、实验效果及结论" class="headerlink" title="3、实验效果及结论"></a>3、实验效果及结论</h2><p>好了，论文实验了上面几种改进方法的效果：</p>
<p><strong>M1</strong>：基准RNN模型 + 数据增强<br><strong>M2</strong>：基准RNN模型 + 数据增强 + 预训练<br><strong>M3</strong>：基准RNN模型 + privileged information<br><strong>M4</strong>：基准RNN模型 + Output embeddings，这里使用的item embedding是使用M1模型训练出的。</p>
<p>模型结果如下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-729d66039a4805b9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>左面的结果，GRU size是100，右边是GRU size是1000。虚线是基准模型的结果，横轴的负数代表使用的训练集。比如，我们将所有的训练集按照时间先后排序，-8即代表使用最近的1/256的数据集进行训练，-6代表使用最近的1/64的数据集进行训练，0就是使用所有的训练集进行训练。</p>
<p>可以看到，M2的效果最好，同时，使用最近1/64的数据可以得到最好的效果。</p>
<h1 id="推荐系统遇上深度学习-四十二-使用图神经网络做基于会话的推荐"><a href="#推荐系统遇上深度学习-四十二-使用图神经网络做基于会话的推荐" class="headerlink" title="推荐系统遇上深度学习(四十二)-使用图神经网络做基于会话的推荐"></a>推荐系统遇上深度学习(四十二)-使用图神经网络做基于会话的推荐</h1><p>前两篇，我们介绍了如何使用循环神经网络来做基于会话的推荐，本篇我们更进一步，来看一下如何使用近期比较火热的图网络来做基于会话的推荐。</p>
<p>本文介绍的论文题目为：《Session-based Recommendation with Graph Neural Networks》<br>论文下载地址为：<a href="https://arxiv.org/abs/1811.00855" target="_blank" rel="external">https://arxiv.org/abs/1811.00855</a><br>代码地址为：<a href="https://github.com/princewen/tensorflow_practice/tree/master/recommendation/Basic-SRGNN-Demo（代码和参考代码一致，写了点注释）" target="_blank" rel="external">https://github.com/princewen/tensorflow_practice/tree/master/recommendation/Basic-SRGNN-Demo（代码和参考代码一致，写了点注释）</a><br>参考代码地址为：<a href="https://github.com/CRIPAC-DIG/SR-GNN" target="_blank" rel="external">https://github.com/CRIPAC-DIG/SR-GNN</a></p>
<p>好了，开始正题吧，在模型介绍时，我们尽量配合代码，让过程显得更加易懂一些。</p>
<h2 id="1、背景介绍"><a href="#1、背景介绍" class="headerlink" title="1、背景介绍"></a>1、背景介绍</h2><p>现有基于会话的推荐，方法主要集中于循环神经网络和马尔可夫链，论文提出了现有方法的两个缺陷：<br>1）当一个会话中用户的行为数量十分有限时，这些方法难以获取准确的用户行为表示。如当使用RNN模型时，用户行为的表示即最后一个单元的输出，作者认为只有这样并非十分准确。<br>2）根据先前的工作发现，物品之间的转移模式在会话推荐中是十分重要的特征，但RNN和马尔可夫过程只对相邻的两个物品的<strong>单向转移关系</strong>进行建模，而忽略了会话中其他的物品。</p>
<p>针对上面的问题，作者提出使用图网络来做基于会话的推荐，其整个模型的框架如下图所示：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-eb1a7c6253b6df60.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>接下来，我们就来介绍一下这个流程吧。</p>
<h2 id="2、模型介绍"><a href="#2、模型介绍" class="headerlink" title="2、模型介绍"></a>2、模型介绍</h2><h4 id="2-1-符号定义"><a href="#2-1-符号定义" class="headerlink" title="2.1 符号定义"></a>2.1 符号定义</h4><p>V={v<sub>1</sub>,v<sub>2</sub>,…,v<sub>m</sub>} 代表所有的物品。s=[v<sub>s,1</sub>,v<sub>s,2</sub>,…,v<sub>s,n</sub>]代表一个session中按照时间先后排序的用户点击序列，我们的目标是预测用户下一个要点击的物品v<sub>s,n+1</sub></p>
<h4 id="2-2-子图构建"><a href="#2-2-子图构建" class="headerlink" title="2.2 子图构建"></a>2.2 子图构建</h4><p>我们为每一个Session构建一个子图，并获得它对应的出度和入度矩阵。</p>
<p>假设一个点击序列是v1-&gt;v2-&gt;v4-&gt;v3，那么它得到的子图如下图中红色部分所示：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-bacdfb6d19d221c9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>再假设一个点击序列是v1-&gt;v2-&gt;v3-&gt;v2-&gt;v4，那么它得到的子图如下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-fbfd30c2108abb43.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>同时，我们会为每一个子图构建一个出度和入度矩阵，并对出度和入度矩阵的每一行进行归一化，如我们序列v1-&gt;v2-&gt;v3-&gt;v2-&gt;v4对应的矩阵如下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-48c3057c6312326b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>上图中，左边的矩阵是出度矩阵，右边的矩阵是入度矩阵，如果同时考虑出度和入度，模型学习的就不是简单的单向转移关系了，而是更加丰富的双向关系。</p>
<h4 id="2-3-基于Graph学习物品嵌入向量"><a href="#2-3-基于Graph学习物品嵌入向量" class="headerlink" title="2.3 基于Graph学习物品嵌入向量"></a>2.3 基于Graph学习物品嵌入向量</h4><p>基于Graph学习物品的嵌入向量，作者借鉴的下面这篇文章《GATED GRAPH SEQUENCE NEURAL NETWORKS》（地址：<a href="https://arxiv.org/pdf/1511.05493v3.pdf）的做法，其实就是一个GRU单元，不过在输入时，模型做了一定的改进。" target="_blank" rel="external">https://arxiv.org/pdf/1511.05493v3.pdf）的做法，其实就是一个GRU单元，不过在输入时，模型做了一定的改进。</a></p>
<p>模型的输入计算公式如下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-9d29c955351f76a9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>我们还是使用刚才的序列v1-&gt;v2-&gt;v3-&gt;v2-&gt;v4来一点点分析输入的过程。<br>1）a<sup>t</sup><sub>s,i</sub>是t时刻，会话s中第i个点击对应的输入<br>2）A<sub>s,i:</sub>代表的是一个分块矩阵，它是1*2n的，n代表序列中不同物品的数量，而非序列的长度，此处是4，而非5。举例来说，假设我们当前的i=2，那么其对应的A<sub>s,2\:</sub>=[0,0,1/2,1/2 | 1/2,0,1/2,0]。我们可以把A<sub>s,i:</sub>拆解为[A<sub>s,i:,in</sub>,A<sub>s,i:,out</sub>]<br>3）v<sup>t-1</sup><sub>i</sub>可以理解为序列中第i个物品，在训练过程中对应的嵌入向量，这个向量随着模型的训练不断变化，可以理解为隐藏层的状态，是一个d维向量。<br>4）H是d*2d的权重向量，也可以看作是一个分块的矩阵，可以理解为H=[H<sub>in</sub>|H<sub>out</sub>]，每一块都是d*d的向量。</p>
<p>那么我们来看看计算过程：<br>1）[v<sup>t-1</sup><sub>1</sub>,…,v<sup>t-1</sup><sub>n</sub>] ，结果是d <em> n的矩阵，转置之后是n\</em>d的矩阵，计作v<sup>t-1</sup><br>2）A<sub>s,i:</sub>v<sup>t-1</sup>H相当于[A<sub>s,i:,in</sub>v<sup>t-1</sup>H<sub>in</sub> , A<sub>s,i:,out</sub>v<sup>t-1</sup>H<sub>out</sub>]，即拆开之后相乘再拼接，因此结果是一个1 * 2d的向量。</p>
<p>上面的过程，相当于分别对一个节点的出度和入度进行处理，再进行合并。该过程是我通过代码慢慢理解的，代码如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">fin_state = tf.reshape(fin_state, [self.batch_size, -1, self.out_size])</div><div class="line">fin_state_in = tf.reshape(tf.matmul(tf.reshape(fin_state, [-1, self.out_size]),</div><div class="line">                                    self.W_in) + self.b_in, [self.batch_size, -1, self.out_size])</div><div class="line">fin_state_out = tf.reshape(tf.matmul(tf.reshape(fin_state, [-1, self.out_size]),</div><div class="line">                                     self.W_out) + self.b_out, [self.batch_size, -1, self.out_size])</div><div class="line">av = tf.concat([tf.matmul(self.adj_in, fin_state_in),</div><div class="line">                tf.matmul(self.adj_out, fin_state_out)], axis=-1)</div></pre></td></tr></table></figure>
<p>上面的代码中，fin_state相当于保存的是我们的v<sup>t-1</sup>，self.W_in相当于我们的H<sub>in</sub>， self.W_out相当于我们的H<sub>out</sub>，self.adj_in相当于A<sub>s,i:,in</sub>，self.adj_out相当于A<sub>s,i:,out</sub>。</p>
<p>有一丢丢的复杂，上面是我个人的理解的计算过程，大家可以作为参考。</p>
<p>上面的输入，我们充分考虑了图的信息，接下来，就是GRU单元了，这里的GRU单元没有太多变化，公式如下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-95e1f621cd59198e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>由上面的公式，整个学习的过程就是每个物品的向量独自进行循环，但是在每次输入的时候，会充分考虑图中的信息，简单化一下示意图如下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-68aa16a80efe4a6e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>图中我省略了一些不必要的线，不过我想你能够理解。</p>
<h4 id="2-4-生成Session对应的嵌入向量"><a href="#2-4-生成Session对应的嵌入向量" class="headerlink" title="2.4 生成Session对应的嵌入向量"></a>2.4 生成Session对应的嵌入向量</h4><p>好了，经过T轮的图网络，我们得到了一个session中每个点击物品的向量，分别为[v<sub>1</sub>,v<sub>2</sub>,…,v<sub>n</sub>]，即下图中红色的部分我们已经获得了：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-1e5bdc665a576058.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>接下来，我们要讲解的是下图中红色的部分：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-c0ecd9295724634e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>我们认为，当前序列中最后一个物品是十分关键的，所以把这个信息单独拎出来，令s<sub>1</sub> = v<sub>n</sub>。但是，我们已不能舍弃其他的信息，所以，模型中使用了一个attention的策略，分别计算前面的物品和最后一个点击物品的相关性，并进行加权，得到s<sub>g</sub>：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-2b2eebcf4638e27a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>最后，将两部分进行横向拼接，并进行线性变换，得到s<sub>h</sub>:</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-86d9ddd018b5a765.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<h4 id="2-5-给出推荐结果及模型训练"><a href="#2-5-给出推荐结果及模型训练" class="headerlink" title="2.5 给出推荐结果及模型训练"></a>2.5 给出推荐结果及模型训练</h4><p>在最后的输出层，使用s<sub>h</sub>和每个物品的embedding进行内积计算：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-2aed912966c4e1c9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>并通过一个softmax得到最终每个物品的点击概率：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-98b9c1e6f38e99d4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>损失函数是交叉熵损失函数：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-c354c303bf1eea90.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<h2 id="3、一点小疑问"><a href="#3、一点小疑问" class="headerlink" title="3、一点小疑问"></a>3、一点小疑问</h2><p>在上面进行内积计算的过程中，所使用的v<sub>i</sub>，应该不是经过GNN中间输出的v<sub>i</sub>，而是每个物品的初始embedding，这个初始的embedding，即我们GNN的初始的输入v<sup>0</sup>，如下图：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-25f586db8a646805.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>这个embedding在训练过程中不断的被更新。</p>
<p>个人感觉论文这里符号有点混乱。</p>
<h2 id="4、总结-4"><a href="#4、总结-4" class="headerlink" title="4、总结"></a>4、总结</h2><p>本文使用图网络进行基于会话的推荐，效果还是不错的，而且图网络逐渐成为现在人工智能领域的一大研究热点。感兴趣的小伙伴们，咱们又有好多知识要学习啦，你行动起来了么？</p>
<h1 id="推荐系统遇上深度学习-四十三-考虑用户微观行为的电商推荐"><a href="#推荐系统遇上深度学习-四十三-考虑用户微观行为的电商推荐" class="headerlink" title="推荐系统遇上深度学习(四十三)-考虑用户微观行为的电商推荐"></a>推荐系统遇上深度学习(四十三)-考虑用户微观行为的电商推荐</h1><p>在过去我们介绍的推荐方法中，特别是电商领域的推荐，其考虑的只是用户的**宏观交互行为(macro interaction)，如用户购买了xx物品，点击了xx物品。今天看到一篇不错的文章，将用户的微观行为如浏览商品的时间、对商品详情和评论的阅读等、渠道等等微观行为(micro behaviors)考虑进来，并取得了不错的实验效果。咱们来一探究竟。</p>
<p>论文名称：《Micro Behaviors: A New Perspective in E-commerce Recommender Systems》<br>论文地址：<a href="http://184pc128.csie.ntnu.edu.tw/presentation/18-03-13/Micro%20Behaviors%20A%20New%20Perspective%20in%20Ecommerce%20Recommender%20Systems.pdf" target="_blank" rel="external">http://184pc128.csie.ntnu.edu.tw/presentation/18-03-13/Micro%20Behaviors%20A%20New%20Perspective%20in%20Ecommerce%20Recommender%20Systems.pdf</a></p>
<h2 id="1、问题定义"><a href="#1、问题定义" class="headerlink" title="1、问题定义"></a>1、问题定义</h2><p>我们首先来看一下，什么是宏观交互行为和微观行为：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-1d990cc7afecb6f4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>上面的图中，宏观交互行为就是我们的一个点击序列，如iphone7-&gt;iphone 6-&gt;iphone 7 case -&gt; samsungGalaxy。但是微观行为多种多样，比如我们搜索了iphone，看到了iphone7，点击进入商品详情页后又看了商品的描述和用户的评价，并将其加入购物车等等。同时，每种行为都有一定的停留时间。</p>
<p>因此，我们有如下的符号定义：</p>
<p>P={p<sub>1</sub>,p<sub>2</sub>,..,p<sub>N</sub>}代表N个不同的商品。<br>A={a<sub>1</sub>,a<sub>2</sub>,…,a<sub>M</sub>}代表M种不同的行为。<br>D={d<sub>1</sub>,d<sub>2</sub>,…,d<sub>K</sub>}表示将停留时间分为K档。</p>
<p>因此，用户的每一次行为可以表示为(p<sub>i</sub>,a<sub>j</sub>,d<sub>k</sub>)，即用户在在商品p<sub>i</sub>上有过a<sub>j</sub>行为，并花费了d<sub>k</sub>档的时间。</p>
<p>我们的推荐问题就变为了，基于用户的行为序列(p<sub>i</sub>,a<sub>j</sub>,d<sub>k</sub>)，来预测用户下一个可能感兴趣的物品。</p>
<h2 id="2、数据分析"><a href="#2、数据分析" class="headerlink" title="2、数据分析"></a>2、数据分析</h2><p>论文收集的信息包含以下四个方面：</p>
<p><strong>Click Source</strong>：用户进入商品页的渠道，如主页、搜索页、购物车页、促销页等等。不同的渠道表明了用户不同的偏好，如用户从主页进入到商品页，用户也许只是想随便看看，但如果用户从搜索页进入到商品页，那么在一定程度上说明用户是有明确需求的。</p>
<p>渠道包含下面的五种，从上倒下分别是主页、类别页、促销页、购物车、搜索结果列表：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-cdabc24297f030fc.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p><strong>Browsing Modules</strong>：这里是说在商品页，用户浏览的主要模块，比如商品详情介绍、商品评论、规格。</p>
<p>模块这里分了三种，如下图，分别为商品评论、商品规格属性、一直浏览到最底部(即所有的都浏览了)：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-00157d5a3311bad2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p><strong>Cart and Order</strong>:加购和下单行为。这里特别提到的一点是，产品属性不同，代表的复购可能性不同，如用户刚买了一些小吃，那么他极有可能在短时间内再买一次，但是如果刚买了一个电视机，那么他基本不会在短时间内再买一次。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-d5bf69c99382e95d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p><strong>停留时间</strong>：停留时间这里划分了5档，如下图：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-64177b617bbde8a2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>好了，在介绍了基础的数据之后，作者进行了一定的基础分析，并用excel图表进行了展示。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-f079b310dbda4506.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>这里的转化率计算如下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-258c5e7407106905.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>可以看到，从Click Source来看，通过促销页和购物车页进入商品页，随后完成下单转化的比例最高。从Browsing Modules来看，如果用户阅读了评论、规格活着滑倒了底部的话，其转化率也会高于只浏览商品详情（这里可以简单通过对比Click Source和Browsing Modules的转化率，Click Source的平均转化率相当于所有Browsing Modules + 只浏览商品详情的平均转化率），而从购物车页面直接下单，转化率最高。</p>
<p>再来看停留时间的分析：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-cbe14870791b79f7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>上图表明，在一定的范围内，转化率随着停留时间增加而增加，当停留时间超过了一定的范围，再提升停留时间，转化率反而开始下降。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-9c1a9491c0562557.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>上图表明，用户通过搜索结果页进入到商品详情页后，停留时间更长，如果通过类别页进入商品页，停留时间较短。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-5d1a11d5af5d296c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>上图表明，停留时间越长，用户阅读商品评论和规格的概率越大，但最终滑倒底部的概率还是相对偏低的。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-46fbf30ab74fec30.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>上图展示了商品页入口和浏览模块之间的关系，如果用户从搜索列表页进入到商品页的话，他有更高的概率阅读商品的评论和规格。</p>
<p>通过上面的分析，我们发现了两个主要的结论：<br>1）微观行为是相互关联的<br>2）不同的微观行为，对于转化的影响是不同的。</p>
<p>接下来，我们就通过模型来建模具体的微观行为。</p>
<h2 id="3、推荐模型"><a href="#3、推荐模型" class="headerlink" title="3、推荐模型"></a>3、推荐模型</h2><p>我们的模型如下图所示：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-2e280917a471e177.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>共分为5层，输入层，embedding层，RNN层，attention层，输出层。</p>
<p><strong>输入层</strong></p>
<p>输入层输入的是用户的行为序列，S<sub>u</sub>={x<sub>1</sub>,x<sub>2</sub>,…,x<sub>n</sub>}，序列中每一项式商品ID、行为ID、停留时长ID的三元组，如x<sub>t</sub>=(p<sub>v</sub>,a<sub>m</sub>,d<sub>k</sub>)。</p>
<p><strong>Embedding 层</strong></p>
<p>商品ID、行为ID、停留时长ID在Embedding层分别转换为对应的embedding，然后进行横向拼接。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-8ba617a55240682e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p><strong>RNN层</strong></p>
<p>随后的RNN层，我们可以选择LSTM或者GRU，实际中LSTM和GRU效果差不多，但GRU相对于LSTM更加简单，因此选择了GRU。</p>
<p><strong>attention层</strong></p>
<p>attention对行为序列中的每一个时刻的RNN层的输出进行加权，计算公式如下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-35fd0641b1ec0e30.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p><strong>输出层</strong></p>
<p>模型的输出是attention层加权后的向量，采用的损失函数是交叉熵损失：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-b5a2f1dbf6baf651.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<h1 id="推荐系统遇上深度学习-四十四-Airbnb实时搜索排序中的Embedding技巧"><a href="#推荐系统遇上深度学习-四十四-Airbnb实时搜索排序中的Embedding技巧" class="headerlink" title="推荐系统遇上深度学习(四十四)-Airbnb实时搜索排序中的Embedding技巧"></a>推荐系统遇上深度学习(四十四)-Airbnb实时搜索排序中的Embedding技巧</h1><p>本文介绍的论文题目是：《Real-time Personalization using Embeddings for Search Ranking at Airbnb》</p>
<p>本文论文的下载地址为：<a href="https://dl.acm.org/authorize.cfm?key=N665520" target="_blank" rel="external">https://dl.acm.org/authorize.cfm?key=N665520</a></p>
<p>本论文获得了2018 年 KDD ADS track 的最佳论文，主要介绍了机器学习的Embedding在 Airbnb 爱彼迎房源搜索排序和实时个性化推荐中的实践，其中很多小trick具有一定的借鉴意义，咱们一起来学习学习！</p>
<p>文章正式开始前先提一下，论文中出现比较多的一个词是Listing，这里我们翻译为房源。</p>
<h2 id="1、背景-9"><a href="#1、背景-9" class="headerlink" title="1、背景"></a>1、背景</h2><p>咱们先来了解一下Airbnb搜索推荐的一些相关背景哈。</p>
<p>我们先来看看Airbnb的搜索结果：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-a1facb273d9b07d0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>点开任意一个推荐结果：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-808b3a393b77042e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>拖到底部，还有相似房源的推荐：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-4d9f7be975212078.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>Airbnb中99%的房源预订就来自于搜索结果的点击和相似房源的推荐。</p>
<p>但是，并不是你想预订房源，就能预订到的，还要看房东是否同意。所以，整个过程如下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-9fb99f234039131c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>所以说，这是一个双边的推荐过程，既需要考虑用户会不会预订，也需要考虑房东会不会接受预订。</p>
<p>为了进行更加准确的排序，Airbnb的搜索团队建立了一个Real-time实时的个性化排序模型，既考虑用户的短时兴趣，也考虑用户的长期兴趣。短时兴趣指用户在一个session中表现出的兴趣，而长期兴趣指用户在其所有历史行为中表现出的兴趣。在模型中，其精髓就在于Embedding的过程，包括Listing Embedding、User Type Embedding 和 Listing Type Embedding。</p>
<h2 id="2、Listing-Embedding"><a href="#2、Listing-Embedding" class="headerlink" title="2、Listing Embedding"></a>2、Listing Embedding</h2><p>对房源进行Embedding，可以建模用户的短时兴趣，也可以进行相似房源的推荐。它通过用户在Session中的点击序列训练得到，这里的session定义要注意以下两点：</p>
<p>1）每次点击，用户需要至少在页面上停留30s，否则被视为误点击，不进行考虑。<br>2）用户前后两次点击时间间隔大于30min，作为切割session的依据</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-640e77ef6bba55ea.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>我们怎么来训练房源的Embedding呢？参考的是Word2Vec的做法，首先，获取一大批的Session，计做S，每一个单独的session s=(l<sub>1</sub>,l<sub>2</sub>,…,l<sub>M</sub>)包含M次房源点击。其次，借鉴Skip-Gram的做法，给定一个中心的房源，预测其前后m个点击的房源：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-274747a78535f71f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>在Skip-Gram中，我们的目标是使如下的函数取值最大：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-df5b87bf3d864181.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>而概率计算如下，是一个softmax过程：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-40bc49e52f9964d4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>这里，V是所有的房源的集合，v<sub>l</sub>是房源的输入向量，而v’<sub>l</sub>是房源的输出向量。同一个房源，有一个输入向量和一个输出向量，我们来简单回顾一下这两个概念，通过下图便可以理解：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-cf0786f9622a7ad8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>这里，我画了一个简单的图示，来表示输入向量和输出向量，比如，对于第2个房源来说，其输入向量就是[0.8,0.4,0.2]，对于第1个房源来说，其输出向量就是[0.3,0.7,0.1]，那么在softmax之前，第二个房源和第一个房源的相关性计算为0.8 <em> 0.3 + 0.4 </em> 0.7 + 02 * 0.1 = 0.54。也就是说，从输入层到embedding层上的权重，是我们要学习的输入向量，而embedding层到输出层上的权重，正好对应于每个房源的输出向量。</p>
<p>但是呢，如果每次都计算根所有房源的相关性的话，太大了，因为我们有数百万的房源，所以通常情况下，会采用负采样的方法，即随机选择一小部分作为负样本。此时，优化的目标函数变为：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-543f71e75a347194.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>上面的式子中，D<sub>p</sub>代表的是正样本的集合，D<sub>n</sub>代表的是负样本的集合。</p>
<p>这样还不算完事，我们还有许多改进效果的小trick呢！一起来看看！</p>
<p><strong>trick1:  Booked Listing as Global Context</strong><br>在所有的训练用的Session中，有一部分发生了预订行为，我们称之为booked sessions，有一些没有发生预订行为，我们称之为exploratory sessions。对于booked sessions，无论最终预订的房源是否在Skip-Gram的窗口内，都将其放入到目标函数中。这么做出于这样的考虑：无论当前窗口是否包含预订的房源，被预订的房源都是与当前的中心房源是有关联的。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-598d06bb47bbb81a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>此时的目标函数变为：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-6ff18221657d6323.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>而对于exploratory sessions，不做任何特殊的处理。</p>
<p><strong>trick2:  Adapting Training for Congregated Search.</strong></p>
<p>用户在出行的时候，都是有明确的目的地的(文中的用词是market,目的地只是market的一个特例)。这样，我们在选择负样本的时候，在随机选择的基础上，可以加一批同目的地的房源负样本，记做D<sub>m<sub>n</sub></sub>。此时的目标函数变为：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-7d89dbb5c7b02058.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p><strong>解决冷启动问题</strong></p>
<p>对于新加入的房源，训练数据中是没有它的记录的，也就是无法训练得到其Embedding。那么文中的做法是，从已有的得到embedding的房源中，选择3个同种类(种类会在下一节介绍)且距离最近(但是要在半径10miles以内)的3个房源，并用其embedding的平均值来作为新房源的embedding。 98%的新房源都可以通过这种方式来获得相应的embedding。</p>
<p><strong>效果检验</strong></p>
<p>最后，我们来看看获得的房源embedding的效果如何。使用8亿的session，给每个房源训练一个32维的embedding，Skip-Gram的窗口设置为5。</p>
<p>使用embedding对房源进行k均值聚类，加利福尼亚的房源聚类结果如下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-bfe8f016d5e720c3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>可以看到，基本上距离越近的房源，其embedding也相近。</p>
<p>通过余弦相似度计算不同种类或者不同价格的房源的相似度，可以发现如果房源的类型相同，价格相近的话，其余弦相似度也是最高的。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-f3c0a64197a11613.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>除了价格这些很容易定义的分类外，连建筑类型、建筑风格这种比较难以准确辨别的分类方式，我们得到的Embedding也能很好的进行区分：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-ba96009fed7c6627.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<h2 id="3、User-Type-amp-Listing-Type-Embedding"><a href="#3、User-Type-amp-Listing-Type-Embedding" class="headerlink" title="3、User Type &amp; Listing Type Embedding"></a>3、User Type &amp; Listing Type Embedding</h2><p>用户在一个session内表现出的兴趣我们可以称为短时兴趣，但在推荐时，用户的长期兴趣有时候也很重要。比如用户正在洛杉矶找房源，那么便可以根据其之前在纽约预订过的房源信息来进行推荐。</p>
<p>尽管我们可以通过上面得到的房源embedding，可以捕获到不同城市之间房源的相关性信息，但是更加通用的做法是通过不同用户在不同城市的预订行为，来学习不同城市房源的相似性。</p>
<p>好了，思路来了，我们还是用word2vec，把用户的历史预订行为当作一个session，同样使用skip-gram来训练不就好了么？此时会存在一些问题：</p>
<p>1）预订序列数据相比于点击序列数据，是非常少的<br>2）许多用户只有过一次预订行为，这种的session是不能拿来用的<br>3）为了学习一个比较有效的embedding，房源至少要出现5-10次，但是有许多房源无法达到这样的标准<br>4）如果序列中两次预订的时间间隔过长的话，用户的偏好是会发生改变的。</p>
<p>好了，前三条总结来说，就是数据少，但是房源多啊。这样，我们不去学习每个具体房源的embedding，我们把房源进行归类，学习每一类房源的embedding。而为了解决第四个问题，我们把用户type考虑进来。</p>
<p>这样，原来的一条预订序列，此时变为了：(u<sub>type1</sub>,l<sub>type1</sub>,u<sub>type2</sub>,l<sub>type2</sub>,…,u<sub>typeM</sub>,l<sub>typeM</sub>)。这样，尽管用户的偏好随时间改变了，这种改变就体现在了user type的不断变化上。</p>
<p>接下来的问题就是，怎么对房源和用户归类？归类规则如下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-a4424f7279c08b40.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>文中举了两个例子：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-7d8f6a4f3af747a7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-df0c2a6cc030d2ca.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>细心的你可能发现了，在用户类别里，中间是画了一条线的。如果一个用户没有过预订行为，其类别只用上面五行表示，如果有预订行为，则用所有的行表示。</p>
<p>上文说过了，此时的序列是(u<sub>type1</sub>,l<sub>type1</sub>,u<sub>type2</sub>,l<sub>type2</sub>,…,u<sub>typeM</sub>,l<sub>typeM</sub>)。我们就可以用skip-gram进行训练了：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-d50228540af11d69.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>当中心是一个user-type时，目标是如下的函数最大化：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-9d172d789e417247.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>当中心是一个listing-type时，目标是如下的函数最大化：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-edab767295d40152.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>这样做带来了什么效果呢？user-type和listing-type得到的embedding是属于同一空间的，可以直接来计算相似度！</p>
<p>接下来就有意思了，在Airbnb里面，房东是可以拒绝用户的预订申请的，我们还想把这部分信息放进去，这部分就是显式的负样本：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-803da73a7d52d9d0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>此时的目标函数变为：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-eae047f2f9e30f42.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>这样，我们就能通过训练得到User Type Embedding 和 Listing Type Embedding。</p>
<h2 id="4、总结-5"><a href="#4、总结-5" class="headerlink" title="4、总结"></a>4、总结</h2><p>本文主要介绍Airbnb实时搜索排序中的Embedding技巧。我们简单来回顾一下：</p>
<p><strong>Listing Embedding</strong><br>1、使用Skip-Gram方法训练embedding<br>2、使用负采样的方式<br>3、将session分为booked sessions和exploratory sessions，对于booked sessions，将最终booked的房源加入目标函数中<br>4、采样一批同地区的房源加入到负样本中</p>
<p><strong>Listing &amp; User Type Embedding</strong><br>1、将预订序列变为(((u<sub>type1</sub>,l<sub>type1</sub>,u<sub>type2</sub>,l<sub>type2</sub>,…,u<sub>typeM</sub>,l<sub>typeM</sub>))序列<br>2、将房东拒绝预订的房源，作为显式的负样本加入到模型训练</p>
<p>同时，在处理<strong>冷启动</strong>时：<br>1、新加入的房源，找距离最近的3个同类型的房源，用他们embedding的平均值作为新房源的embedding<br>2、没有过预订行为的用户，只使用表格中的前五行作为其类别</p>
<p>经典文章，多读多看多想！</p>
<h2 id="参考文献：-2"><a href="#参考文献：-2" class="headerlink" title="参考文献："></a>参考文献：</h2><p>1、<a href="https://zhuanlan.zhihu.com/p/55149901" target="_blank" rel="external">https://zhuanlan.zhihu.com/p/55149901</a><br>2、<a href="https://zhuanlan.zhihu.com/p/57313656" target="_blank" rel="external">https://zhuanlan.zhihu.com/p/57313656</a><br>3、<a href="https://www.zhihu.com/question/302288216" target="_blank" rel="external">https://www.zhihu.com/question/302288216</a></p>
<h1 id="推荐系统遇上深度学习-四十五-探秘阿里之深度会话兴趣网络DSIN"><a href="#推荐系统遇上深度学习-四十五-探秘阿里之深度会话兴趣网络DSIN" class="headerlink" title="推荐系统遇上深度学习(四十五)-探秘阿里之深度会话兴趣网络DSIN"></a>推荐系统遇上深度学习(四十五)-探秘阿里之深度会话兴趣网络DSIN</h1><p>阿里又双叒叕开源新算法了，这次的名称叫做Deep Session Interest Network，我们将其翻译为深度会话兴趣网络，一起来看看吧～～</p>
<p>论文题目：《Deep Session Interest Network for Click-Through Rate Prediction》<br>论文链接：<a href="https://arxiv.org/abs/1905.06482" target="_blank" rel="external">https://arxiv.org/abs/1905.06482</a><br>代码链接：<a href="https://github.com/shenweichen/DSIN" target="_blank" rel="external">https://github.com/shenweichen/DSIN</a></p>
<h2 id="1、背景-10"><a href="#1、背景-10" class="headerlink" title="1、背景"></a>1、背景</h2><p>从用户行为中呢，我们发现，在每个会话中的行为是相近的，而在不同会话之间差别是很大的，如下图的例子：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-287551c0c8c096d1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>这里会话的划分和airbnb一样，首先将用户的点击行为按照时间排序，判断每两个行为之间的时间间隔，前后的时间间隔大于30min，就进行切分。可以看上图，第一个session中，用户查看的都是跟裤子相关的物品，第二个session中，查看的是戒指相关的物品，第三个则是上衣相关。</p>
<p>基于此，阿里提出了深度会话兴趣网络Deep Session Interest Network，来建模用户这种跟会话密切相关的行为。接下来，我们就来介绍模型的结构。</p>
<h2 id="2、模型结构"><a href="#2、模型结构" class="headerlink" title="2、模型结构"></a>2、模型结构</h2><h4 id="2-1-Base-Model"><a href="#2-1-Base-Model" class="headerlink" title="2.1 Base Model"></a>2.1 Base Model</h4><p>Base Model就是一个全连接神经网络，其输入的特征的主要分为三部分，<strong>用户特征</strong>，<strong>待推荐物品特征</strong>，<strong>用户历史行为序列特征</strong>。用户特征如性别、城市、用户ID等等，待推荐物品特征包含商家ID、品牌ID等等，用户历史行为序列特征主要是用户最近点击的物品ID序列。</p>
<p>这些特征会通过Embedding层转换为对应的embedding，拼接后输入到多层全连接中，并使用logloss指导模型的训练。</p>
<h4 id="2-2-DSIN"><a href="#2-2-DSIN" class="headerlink" title="2.2 DSIN"></a>2.2 DSIN</h4><p>DSIN模型的总体框架如下图：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-d6a96b2997f0e937.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>DSIN在全连接层之前，分成了两部分，左边的那一部分，将用户特征和物品特征转换对应的向量表示，这部分主要是一个embedding层，就不再过多的描述。右边的那一部分主要是对用户行为序列进行处理，从下到上分为四层：<br>1）序列切分层session division layer<br>2）会话兴趣抽取层session interest extractor layer<br>3）会话间兴趣交互层session interest interacting layer<br>4）会话兴趣激活层session interest acti- vating layer</p>
<p>接下来，我们主要介绍这4层。</p>
<h5 id="2-2-1-Session-Division-Layer"><a href="#2-2-1-Session-Division-Layer" class="headerlink" title="2.2.1 Session Division Layer"></a>2.2.1 Session Division Layer</h5><p>这一层将用户的行文进行切分，首先将用户的点击行为按照时间排序，判断每两个行为之间的时间间隔，前后的时间间隔大于30min，就进行切分。</p>
<p>切分后，我们可以将用户的行为序列S转换成会话序列Q。第k个会话Q<sub>k</sub>=[b<sub>1</sub>;b<sub>2</sub>;…;b<sub>i</sub>;…;b<sub>T</sub>],其中，T是会话的长度，b<sub>i</sub>是会话中第i个行为，是一个d维的embedding向量。所以Q<sub>k</sub>是T <em> d的。而Q，则是K </em> T * d的</p>
<h5 id="2-2-2-Session-Interest-Extractor-Layer"><a href="#2-2-2-Session-Interest-Extractor-Layer" class="headerlink" title="2.2.2 Session Interest Extractor Layer"></a>2.2.2 Session Interest Extractor Layer</h5><p>这里对每个session，使用transformer对每个会话的行为进行处理。有关Transformer的内容，可以参考文章<a href="https://mp.weixin.qq.com/s/RLxWevVWHXgX-UcoxDS70w。" target="_blank" rel="external">https://mp.weixin.qq.com/s/RLxWevVWHXgX-UcoxDS70w。</a></p>
<p>在Transformer中，对输入的序列会进行Positional Encoding。Positional Encoding对序列中每个物品，以及每个物品对应的Embedding的每个位置，进行了处理，如下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-5dcf5d8138a16d64.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>但在我们这里不一样了，我们同时会输入多个会话序列，所以还需要对每个会话添加一个Positional Encoding。在DSIN中，这种对位置的处理，称为<strong>Bias Encoding</strong>，它分为三块：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-d4d9493a610caa4a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>BE是K <em> T </em> d的，和Q的形状一样。BE<sub>(k,t,c)</sub>是第k个session中，第t个物品的嵌入向量的第c个位置的偏置项，也就是说，每个会话、会话中的每个物品有偏置项外，每个物品对应的embedding的每个位置，都加入了偏置项。所以加入偏置项后，Q变为：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-6e0be6d89e78d0f9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>随后，是对每个会话中的序列通过Transformer进行处理：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-9f92d33224df1cac.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>这里的过程和Transformer的Encoding的block处理是一样的，不再赘述。感兴趣的同学可以看一下上文提到的文章。</p>
<p>这样，经过Transformer处理之后，每个Session是得到的结果仍然是T * d，随后，我们经过一个avg pooling操作，将每个session兴趣转换成一个d维向量。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-365f9142c596e56c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>这样，I<sub>k</sub>就代表第k个session对应的兴趣向量。</p>
<h5 id="2-2-3-Session-Interest-Interacting-Layer"><a href="#2-2-3-Session-Interest-Interacting-Layer" class="headerlink" title="2.2.3 Session Interest Interacting Layer"></a>2.2.3 Session Interest Interacting Layer</h5><p>用户的会话兴趣，是有序列关系在里面的，这种关系，我们通过一个双向LSTM(bi-LSTM)来处理：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-41b5fa1f466ca744.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>每个时刻的hidden state计算如下</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-301b7c144b74414a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>相加的两项分别是前向传播和反向传播对应的t时刻的hidden state。这里得到的隐藏层状态H<sub>t</sub>，我们可以认为是混合了上下文信息的会话兴趣。</p>
<h5 id="2-2-4-Session-Interest-Activating-Layer"><a href="#2-2-4-Session-Interest-Activating-Layer" class="headerlink" title="2.2.4 Session Interest Activating Layer"></a>2.2.4 Session Interest Activating Layer</h5><p>用户的会话兴趣与目标物品越相近，那么应该赋予更大的权重，这里使用注意力机制来刻画这种相关性：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-7d393efb565658a4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>这里X<sup>I</sup>是带推荐物品向量。</p>
<p>同样，混合了上下文信息的会话兴趣，也进行同样的处理：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-dd6ed1dedf886b90.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>后面的话，就是把四部分的向量：用户特征向量、待推荐物品向量、会话兴趣加权向量U<sup>I</sup>、带上下文信息的会话兴趣加权向量U<sup>H</sup>进行横向拼接，输入到全连接层中，得到输出。</p>
<h2 id="3、模型试验"><a href="#3、模型试验" class="headerlink" title="3、模型试验"></a>3、模型试验</h2><p>模型使用了两个数据集进行了实验，分别是阿里妈妈的广告数据集和阿里巴巴的电商推荐数据集。</p>
<p>对比模型有：YoutubeNet、Wide &amp; Deep、DIN 、DIN-RNN(这个和DIN很像，在原始的DIN中，用户的行为序列没有使用RNN进行处理，而DIN-RNN使用bi-LSTM对用户的历史行为序列进行处理)、DIEN。</p>
<p>评价指标是AUC。结果如下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-3e386c68800cbf70.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>对于DSIN，这里有分了三种情况，第一个是DSIN，不过将Bias Encoding变为Transformer里面的Positional Encoding，第二个是DSIN，使用bias encoding，但不添加session inter-est interacting layer and the corresponding activation unit。第三个就是前文介绍的DSIN框架。可以看到，最后一个在两个数据集上的AUC均为最大。</p>
<h2 id="4、总结讨论"><a href="#4、总结讨论" class="headerlink" title="4、总结讨论"></a>4、总结讨论</h2><p>这里，论文对结果进行了进一步讨论，主要有：</p>
<h4 id="4-1-Effect-of-Multiple-Sessions"><a href="#4-1-Effect-of-Multiple-Sessions" class="headerlink" title="4.1 Effect of Multiple Sessions"></a>4.1 Effect of Multiple Sessions</h4><p>从实验结果来看，DIN-RNN的效果差于DIN，而DSIN-BE的效果好于DSIN-BE-No-SIIL。两组的差别均是有没有使用序列建模。文章里提到，对于序列建模来说，如果用户的行为时十分跳跃的，同时是突然结束的，会使得用户的行为看上进去具有很大的噪声(这里也不知道翻译的对不对，直接上原文吧）：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-cc659c43b418fea8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>这样就使得DIN-RNN的效果反而不如DIN，但在DSIN中，我们对用户的行为序列按照会话进行了分组，由于以下两点原因，使得DSIN中使用序列建模效果反而更好：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-ca2f00e8ff352528.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<h4 id="4-2-Effect-of-Session-Interest-Interacting-Layer"><a href="#4-2-Effect-of-Session-Interest-Interacting-Layer" class="headerlink" title="4.2 Effect of Session Interest Interacting Layer"></a>4.2 Effect of Session Interest Interacting Layer</h4><p>DSIN-BE的效果好于DSIN-BE-No-SIIL，说明通过 Effect of Session Interest Interacting Layer得到混合上下文信息的用户兴趣，可以进一步提升模型的效果。</p>
<h4 id="4-3-Effect-of-Bias-Encoding"><a href="#4-3-Effect-of-Bias-Encoding" class="headerlink" title="4.3 Effect of Bias Encoding"></a>4.3 Effect of Bias Encoding</h4><p>DSIN-BE的效果好于DSIN-PE，说明对不同的session添加偏置项，效果还是十分不错的。</p>
<p>##4.4 Visualization of Self-attention and the Activation Unit</p>
<p>这里论文展示了一下 Self-attention and the Activation Unit的效果，还是开篇的那个例子：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-1464c70544e5228a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<h1 id="推荐系统遇上深度学习-四十六-阿里电商推荐中亿级商品的embedding策略"><a href="#推荐系统遇上深度学习-四十六-阿里电商推荐中亿级商品的embedding策略" class="headerlink" title="推荐系统遇上深度学习(四十六)-阿里电商推荐中亿级商品的embedding策略"></a>推荐系统遇上深度学习(四十六)-阿里电商推荐中亿级商品的embedding策略</h1><p>本文分享的论文题目是《Billion-scale Commodity Embedding for E-commerce Recommendation in Alibaba》</p>
<p>论文地址：<a href="https://arxiv.org/abs/1803.02349" target="_blank" rel="external">https://arxiv.org/abs/1803.02349</a></p>
<p>在淘宝的推荐中，主要面临着三个技术挑战，分别是可扩展性(scalability)、稀疏性(sparsity)、冷启动问题(cold start)。本文提出了一种图嵌入(graph embedding)的方法来解决上面的三个问题，一起来看下吧。</p>
<p>值得一提的是，在本系列的第三十六篇：<br><a href="https://www.jianshu.com/p/285978e29458，我们介绍了阿里另一篇来做item" target="_blank" rel="external">https://www.jianshu.com/p/285978e29458，我们介绍了阿里另一篇来做item</a> embedding的文章，大家不妨先回顾一下。最后我们会对比一下这两种方法的区别。</p>
<h2 id="1、背景-11"><a href="#1、背景-11" class="headerlink" title="1、背景"></a>1、背景</h2><p>在淘宝的推荐中，面临以下三个问题：</p>
<p><strong>可扩展性(scalability)</strong>：一些现有的推荐系统方法，在小规模数据集上效果很好，但是在想淘宝这样的拥有十亿用户和二十亿商品的数据集上，表现得并不好。</p>
<p><strong>稀疏性(sparsity)</strong>：用户仅与非常少的商品有过交互行为，这样的话很难精确训练一个推荐模型。</p>
<p><strong>冷启动(cold start)</strong>：在淘宝中，每个小时都有百万级别的新的商品上线，这些商品没有过用户行为，预测用户对这些商品的偏好是十分具有挑战性的。</p>
<p>为了解决上面的这些问题，淘宝也采用了业界常用的两阶段框架，第一阶段称为匹配阶段，也可以叫做召回阶段，从大规模的商品集中召回一个比较小的候选集。第二阶段是排序阶段，对召回的候选集进行精确排序。</p>
<p>在召回阶段，主要的方法是计算商品之间的相似性，从而根据用户的历史交互行为得到用户可能喜欢的相似商品。计算商品的相似性，可以采用协同过滤的方法，但是协同过滤仅仅考虑了商品在交互矩阵中的共现性；使用<strong>图嵌入(Base Graph Embedding (BGE))</strong>的方法，比如随机游走的方法，可以学习到比较好的商品之间的相似性，但是对于出现次数很少甚至没有用户交互过的商品，依然难以有效地学习。</p>
<p>因此，本文提出使用基于side information的图嵌入学习方法，称作<strong>Graph Embedding with Side information (GES)</strong>。这里的side information你可以理解为辅助信息，比如一个商品的品牌、店铺名、类别等等。使用side information来学习商品的embedding的话，同一个品牌或者类别的商品应当更相似。但是在淘宝中，有数以百计的side information，这些side information对于商品向量的贡献程度是不同的，比如一个购买了iphone的用户，倾向于查看mac或者ipad，更多的是因为他们都是苹果的牌子。考虑不同的side information对最终的item embedding的不同影响，这种方法称作<strong>Enhanced Graph Embedding with Side information (EGES)</strong>。</p>
<p>接下来，我们就来介绍三种方法，分别是<strong>Base Graph Embedding (BGE)、Graph Embedding with Side information (GES)和Enhanced Graph Embedding with Side information (EGES)</strong>。</p>
<h2 id="2、模型介绍-1"><a href="#2、模型介绍-1" class="headerlink" title="2、模型介绍"></a>2、模型介绍</h2><h4 id="2-1-Base-Graph-Embedding-BGE"><a href="#2-1-Base-Graph-Embedding-BGE" class="headerlink" title="2.1 Base Graph Embedding (BGE)"></a>2.1 Base Graph Embedding (BGE)</h4><p>Base Graph Embedding (BGE)的完整流程可以参考下图：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-22677981881da65e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>首先，从用户的行为中抽取出序列表示，这里有两个地方需要注意：<br>1）如果使用用户整个的行为历史序列，计算和空间存储资源耗费巨大<br>2）用户的兴趣在长时间内是会变化的，但是用户短时间内的兴趣是相同的<br>基于以上两点原因，需要对用户的历史行为序列进行切割／这里以一小时为间隔，若两个商品的交互时间超过1小时，就进行切分。如图中的U2，E和D的时间间隔大于1小时，所以将序列切割为BE和DEF。</p>
<p>接下来，将所有的到的序列表示称有向带权图，如图中的D-&gt;A出现了一次，那么就会有一条从D指向A的边，同时边的权重记为1。再强调一次，这里是用所有用户经上一步的到的序列汇总起来得到一个有向带权图，而非每个用户对应于一张图。</p>
<p>在实际应用中，需要对一些噪声信息进行过滤，主要有：</p>
<p>1）点击之后用户停留时间小于1s，这可能是用户的误点击，需要过滤。<br>2）太过活跃的用户进行过滤，比如三个月内购买了1000件以上的商品，点击了3500个以上的商品。<br>3）同一个ID，但是发生变化的商品需要过滤。</p>
<p>在得到有向带权图之后，基于随机游走的方法产生一批序列，商品转移概率基于边的权重M<sub>ij</sub>：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-65bdeb51d5384cd6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>得到的序列举例如下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-d8c61ccb285a16f5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>随后，我们便可以通过Skip-Gram的方法来学习每个商品的向量啦。使用负采样的方式，我们的优化目标是：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-983a1cdc5101d62e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>感觉论文里这个地方写错了啊，应该是maxmize。前面的v<sub>j</sub>是正样本，后面的v<sub>t</sub>是采样得到的负样本。</p>
<p>还有一点我觉得值得商榷的是，对于Skip-Gram来说，每个商品对应了两个embedding，如下图：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-3ec9550d64d059be.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>最终获得的是商品在InputMatrix中对应的embedding，当前商品通过InputMatrix得到其Hidden Representation，然后与其计算dot product的应该是outputMatrix中商品的对应的embedding，所以感觉这里的符号表示有点问题。</p>
<h4 id="2-2-Graph-Embedding-with-Side-information-GES"><a href="#2-2-Graph-Embedding-with-Side-information-GES" class="headerlink" title="2.2 Graph Embedding with Side information (GES)"></a>2.2 Graph Embedding with Side information (GES)</h4><p>上面的Base方法，可以较好的学习到item embedding，但是冷启动问题无法很好的解决。基于此，提出了Graph Embedding with Side information方法。为了与之前的item embedding区分开，在加入Side information之后，我们称得到的embedding为商品的aggregated embeddings。商品v的aggregated embeddings计作H<sub>v</sub>。</p>
<p>aggregated embeddings的计算公式如下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-9902b0619fc4f52c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>其中，W<sup>0</sup>代表item embedding，W<sup>1</sup>,W<sup>n</sup>代表每种Side information对应的embedding。</p>
<p>具体的流程我们在下一节再细讲，因为GES和EGES的原理都是相通的。</p>
<h4 id="2-3-Enhanced-Graph-Embedding-with-Side-information-EGES"><a href="#2-3-Enhanced-Graph-Embedding-with-Side-information-EGES" class="headerlink" title="2.3 Enhanced Graph Embedding with Side information (EGES)"></a>2.3 Enhanced Graph Embedding with Side information (EGES)</h4><p>正如前文的例子，比如一个购买了iphone的用户，倾向于查看mac或者ipad，更多的是因为他们都是苹果的牌子。因此不同的side information在最终的aggregated embeddings中所占的权重应该是不同的，所以此时的aggregated embeddings计算公式如下:</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-3e06015d7ccbc784.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<h4 id="2-3-GES和EGES的学习"><a href="#2-3-GES和EGES的学习" class="headerlink" title="2.3 GES和EGES的学习"></a>2.3 GES和EGES的学习</h4><p>GES和EGES的流程如图：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-4a225b5f18b69ea8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>此时损失函数可以表示为：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-9189773bf99b3679.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>这里的Z<sub>u</sub>应该是商品u在OutputMatrix中对应的embedding，通过反向传播进行学习：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-25b45e98b517b1d7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>这里有一个需要注意的地方，自身item embedding和每种side information的权重，对每个商品来说是不同的，并非采用相同的权重，权重通过反向传播算法进行学习，具体表示为：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-f5b9b52dde560500.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>item embedding 和 side-information对应的embedding同样通过反向传播学习：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-abd743969e78dce8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<h2 id="3、实验分析"><a href="#3、实验分析" class="headerlink" title="3、实验分析"></a>3、实验分析</h2><h4 id="3-1-实验结果"><a href="#3-1-实验结果" class="headerlink" title="3.1 实验结果"></a>3.1 实验结果</h4><p>这里主要进行了两部分的实验，离线实验和在线实验。</p>
<p>对于离线实验，对比了不同模型的AUC，咱们这里不是有正样本和负样本嘛，使用学习到的embedding 计算dot-product之后，将样本排序，计算AUC，结果如下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-58e335ca5cfc1399.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>在线实验，对比了不同模型下推荐结果的CTR：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-aa1743f85586964a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>可以看到，都是EGES方法效果最好。</p>
<h4 id="3-2-案例分析"><a href="#3-2-案例分析" class="headerlink" title="3.2 案例分析"></a>3.2 案例分析</h4><p><strong>可视化embedding结果</strong></p>
<p>对于学习到的embedding，通过PCA降维的方式将其展示出来：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-6d46fb80cdf99200.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>可以看到结果中，足球、羽毛球和网球相关的商品基本都聚集在了一起。</p>
<p><strong>解决冷启动问题</strong></p>
<p>对于新加入的商品，我们使用其side information对应的embedding的均值来代替它的embedding，这样做的效果如下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-b4d28dca8bb4ded6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>可以看到，通过这样的方式计算得到冷启动商品的embedding，其相似商品结果是比较好的。</p>
<p><strong>EGES中的权重</strong></p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-c89bb3d7fa3bf226.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>前面提到过，自身item embedding和每种side information的权重，对每个商品来说是不同的。这里我们展示了部分商品对应的权重。</p>
<h2 id="4、对比总结"><a href="#4、对比总结" class="headerlink" title="4、对比总结"></a>4、对比总结</h2><p>与本系列第三十六篇相比，主要有下面两个不同吧。</p>
<p>1、本文混合了所有用户的交互序列构建了有向带权图，进一步通过随机游走的方式生成新的序列；而第三十六篇中方法直接使用用户的交互序列。感觉这两种方式都是可行的。本文以淘宝推荐为基础，商品数量巨大，通过随机游走的方式可以生成更多的训练集；而第三十六篇中方法终，以盒马鲜生推荐为基础，商品数量并没有那么多。<br>2、两篇文章对不同的side information都进行了加权，本文的权重是通过模型训练得到的，而第三十六篇文章中权重是预先定义好的。</p>
<h1 id="推荐系统遇上深度学习-四十七-TEM-基于树模型构建可解释性推荐系统"><a href="#推荐系统遇上深度学习-四十七-TEM-基于树模型构建可解释性推荐系统" class="headerlink" title="推荐系统遇上深度学习(四十七)-TEM-基于树模型构建可解释性推荐系统"></a>推荐系统遇上深度学习(四十七)-TEM-基于树模型构建可解释性推荐系统</h1><p>本文论文的题目为：《TEM: Tree-enhanced Embedding Model for Explainable Recommendation》</p>
<p>论文下载地址为：<a href="https://www.comp.nus.edu.sg/~xiangnan/papers/www18-tem.pdf" target="_blank" rel="external">https://www.comp.nus.edu.sg/~xiangnan/papers/www18-tem.pdf</a></p>
<p>推荐系统的方法，无论是协同过滤还是一些embedding-based方法，在可解释性上都有一定的欠缺，而本文提出了一种基于GBDT的可解释的推荐模型，我们一起来看一下是怎么做的。</p>
<h2 id="1、背景-12"><a href="#1、背景-12" class="headerlink" title="1、背景"></a>1、背景</h2><p>现有的推荐方法，如协同过滤和embedding-based的方法，在可解释性上都有一定的欠缺。对于协同过滤模型来说，推荐的理由无非就是<strong>你的朋友也喜欢这个</strong>或者<strong>这个物品和你之前喜欢的某某物品相似</strong>；对于一些embedding-based的方法，最简单的如矩阵分解方法，这种方法只能得到一个简单的评分，但感觉无法解释评分的含义，再比如因子分解机FM、NFM、Wide &amp; Deep和DCN等等，这些方法可以充分利用辅助信息side-information，来捕捉一些特征之间的交叉关系，但这种关系一旦使用了神经网络，就感觉是一个黑盒，推荐结果也是相当难以解释的（其实我感觉FM还是具有一定的解释性的）。</p>
<p>因此，本文想要实现一种推荐方法，同时满足以下两个要求：<strong>结果准确性accurate</strong>和<strong>推荐结果可解释性explainable</strong>。</p>
<p>对于准确性，我们希望能够得到与embedding-based的方法相近的性能，对于可解释性，举个简单的例子，比如我们给用户推荐了一个玫瑰金的iphone7，给出的推荐理由类似于20-25岁，收入过万的女性都喜欢粉丝的iphone，而非简单的xxx购买了或者是与你浏览过的xxx物品相似。也就是说，能够准确的识别起关键作用的交叉特征。</p>
<p>那么如何实现上面的两个目标呢？文章提出了Tree-enhanced Embedding Method (TEM)模型。下面我们一起来看一下是如何做的。</p>
<h2 id="2、模型介绍-2"><a href="#2、模型介绍-2" class="headerlink" title="2、模型介绍"></a>2、模型介绍</h2><p>TEM模型的整体架构如下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-ed411c49aac4935e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>在TEM模型中，对于给定的用户u和物品i，打分结果由下面的公式确定：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-18f41be00404b127.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>其中，x=[x<sub>u</sub>,x<sub>i</sub>]分别是用户和物品的特征向量，主要包含一些辅助信息side-information。公式中最重要的是f(u,i,x)，我们一步步展开这一部分如何得到。</p>
<h4 id="2-1-构建交叉特征"><a href="#2-1-构建交叉特征" class="headerlink" title="2.1 构建交叉特征"></a>2.1 构建交叉特征</h4><p>为了让交叉特征具有良好的可解释性，工业界一种广泛的做法是使用如逻辑回归这样的模型。如果让特征两两的交叉，随着特征增多，计算复杂度呈指数级增长。如果使用人工去选择一些比较靠谱的交叉特征，这需要比较丰富的领域知识，需要对业务特别熟悉。所以像逻辑回归这样的方法同样面临着一些问题。</p>
<p>因此本文提出使用GBDT来进行特征的交叉，GBDT包含一系列的决策树Q={Q<sub>1</sub>,Q<sub>2</sub>,…,Q<sub>S</sub>}，对于每一颗决策树，都会得到一个one-hot的向量，最终拼接起来就得到一个multi-hot向量。如下图中，有两棵决策树，假设特征x经过两棵树之后分别落在了第一颗树的第三个叶子结点和第二颗树的第二个叶子结点，那么得到的向量如下为[0,0,1,0,0,0,1,0]:</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-b9d865ff498a5bd2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>这里我们记最终得到的multi-hot向量为q：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-67594b9dcbe23c72.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<h4 id="2-2-通过交叉特征进行预测"><a href="#2-2-通过交叉特征进行预测" class="headerlink" title="2.2 通过交叉特征进行预测"></a>2.2 通过交叉特征进行预测</h4><p>这里，你是不是想到了我们之间介绍过的FaceBook提出的GBDT+LR的推荐方法。作者这里指出了该模型的一点不足。尽管GBDT+LR的方法可以捕捉一些重要的特征，但是模型一但确定，交叉特征的权重也就确定了，这在一定程度上对模型的准确性是有一定限制的。</p>
<p>论文这里举了一个简单的例子，假设有两个正样本(u,i,x)和(u’,i’,x’)，如果x和x’相同的话，那么GBDT+LR方法得到的top的交叉特征是相同的，也就是说，向u推荐i和向u’推荐i’的理由也是相同的，但实际中可能并不是这样的。因此，对于不同的用户-物品对，不同的交叉特征的权重应该是不同的。计算不同的权重，我们自然而然想到了attention机制。没错，这里也使用attention机制。</p>
<p>首先对于GBDT得到的multi-hot特征q，其中为1的位置数量是确定的，即GBDT中决策树的个数，假设为t。这里对GBDT中所有的叶子结点赋予一个对应的embedding向量，那么我们一共可以得到t个embedding向量。假设得到的embedding向量集合为V={v<sub>l</sub>}(q<sub>l</sub>&lt;&gt;0)，而用户和物品对应的embedding分别为p<sub>u</sub>和q<sub>i</sub>。那么交叉特征的权重计算公式图示和公式如下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-c6b90a7091d0a556.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-b991fea11318d3ab.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>在得到权重之后，作者这里提出了两种方式，一种是avg-pooling，一种是max-pooling，公式分别如下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-742daff90058f54b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>采用不同的pooling方式，对于结果的解释是不同的，咱们在后面详细讨论。</p>
<p>因此最终的预测公式表示如下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-d6d74cc71b4c2ec9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<h4 id="2-3-模型学习"><a href="#2-3-模型学习" class="headerlink" title="2.3 模型学习"></a>2.3 模型学习</h4><p>这里使用logloss来指导模型学习：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-939cb93ba125a403.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>值得一提的是，TEM模型包含两个级联的模型，这两个模型使用的是同样的logloss。在实际中，我们同样先训练GBDT，在训练好GBDT之后，通过其得到每个样本的交叉特征，再通过batch梯度下降Adagrad来优化剩余部分的模型参数。而每个batch中有正样本和负采样得到的一批负样本。</p>
<h2 id="3、可解释性分析"><a href="#3、可解释性分析" class="headerlink" title="3、可解释性分析"></a>3、可解释性分析</h2><p>前面在得到attention的权重后，使用两种不同的方式对embedding进行处理。一种是avg-pooling，一种是max-pooling，我们分别记作TEM-avg和TEM-max。这两种方式对结果的解释是不同的。</p>
<p>对于TEM-avg，我们可以直接使用得到的权重w<sub>uil</sub>来得到重要性最高的几个交叉特征。对于TEM-max，得到的最终向量的每一维可能来自不同的特征，假设embedding是K维，那么max-pooling后的向量最多可能来自k个不同的embedding向量，如下图所示：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-3066e1c6c2089c33.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>上图中，每一行是乘上权重后的embedding，即w<sub>uil</sub> * v<sub>l</sub>，共有14个交叉特征，此时选择重要程度最高的特征，就不能单单看权重w<sub>uil</sub>了，还要看其对max-pooling后结果的贡献。</p>
<h2 id="4、实验分析"><a href="#4、实验分析" class="headerlink" title="4、实验分析"></a>4、实验分析</h2><p>这里作者提出了三个问题</p>
<p>1）TEM模型可以达到与目前state-of-the-art的推荐模型相同的预测效果么？<br>2）TEM模型得到的推荐结果能够方便地通过交叉特征和attention来进行解释么？<br>3）超参数的设定是如何影响TEM模型的呢？</p>
<h4 id="4-1-性能分析"><a href="#4-1-性能分析" class="headerlink" title="4.1 性能分析"></a>4.1 性能分析</h4><p>作者在两个数据集上进行了对比试验，结果如下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-e6f7d2eb5005850f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>可以看到TEM-max的效果最好，其次是TEM-avg。</p>
<h4 id="4-2-可解释性案例分析"><a href="#4-2-可解释性案例分析" class="headerlink" title="4.2 可解释性案例分析"></a>4.2 可解释性案例分析</h4><p>其实我在看论文的时候一直有个疑问，模型最后得到预测输出的时候，还接了一层可以说是全连接的神经网络，那么是否是可以直接用attention得到的权重w<sub>uil</sub>来对结果进行解释。比如使用TEM-avg的时候，第l个交叉特征对模型预测结果贡献的最终结果是w<sub>uil</sub> <em> r<sup>T</sup> </em> v<sub>l</sub>，而并非w<sub>uil</sub>，二者需要具有一定的一致性，我们才可以近似用w<sub>uil</sub>来代表特征的重要程度。作者通过case studay验证了这一点：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-07a55b603af4509b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>而对于TEM-max方式，其预测性能是最好的，但是作者这里没有给出相应的case study。感觉TEM-max可以通过交叉特征对max-pooling结果的贡献来确定其重要性，因为最后再乘上 r<sup>T</sup>，这个并不影响max-pooling结果的每一维来自于哪一个交叉特征。</p>
<p>因为 r<sup>T</sup> <em> max-pooling(w<sub>uil</sub> </em> v<sub>l</sub>) = sum(max-pooling(w<sub>uil</sub> * multiply（r<sup></sup> ，v<sub>l</sub>)）</p>
<p>可能我说的比较晦涩，通过下面可以理解下，假设我们把 r放在max-pooling之前，先和v<sub>l</sub>对位相乘，并不改变最终结果的每一维来自哪一个交叉特征对应的embedding：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-0c7733c2429b0a07.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>而两边最终结果也验证了我们公式的正确性。</p>
<h4 id="4-3-超参数学习"><a href="#4-3-超参数学习" class="headerlink" title="4.3 超参数学习"></a>4.3 超参数学习</h4><p>这里主要验证了下树的数量S和embedding的size对模型结果的影响，结果如下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-bae2a47de25babf3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-c793bfe88d300871.jpeg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<h1 id="推荐系统遇上深度学习-四十八-BST-将Transformer用于淘宝电商推荐"><a href="#推荐系统遇上深度学习-四十八-BST-将Transformer用于淘宝电商推荐" class="headerlink" title="推荐系统遇上深度学习(四十八)-BST-将Transformer用于淘宝电商推荐"></a>推荐系统遇上深度学习(四十八)-BST-将Transformer用于淘宝电商推荐</h1><p><img src="https://upload-images.jianshu.io/upload_images/4155986-c793bfe88d300871.jpeg?imageMogr2/auto-orient/strip%7CimageView2/2/w/430" alt=""></p>
<p>前几天还跟同事开玩笑说，DIN上面接一层Transformer，然后加一个Positional Encoding，然后再加Attention层，效果可能会好些。结果，今天就看到淘宝已经发出了相关的论文了，有一些地方还是没有想到的，咱们来一起看下。</p>
<p>本文的论文名称为：《Behavior Sequence Transformer for E-commerce Recommendation in Alibaba》</p>
<p>论文地址为：<a href="https://arxiv.org/pdf/1905.06874.pdf" target="_blank" rel="external">https://arxiv.org/pdf/1905.06874.pdf</a></p>
<h2 id="1、背景-13"><a href="#1、背景-13" class="headerlink" title="1、背景"></a>1、背景</h2><p>淘宝的推荐系统总体上也分为匹配和精排阶段，匹配阶段，我们刚刚在第四十六篇文章中介绍了其针对十亿级商品的embedding方案，感兴趣的同学可以看下前面的文章。</p>
<p>在精排阶段，主要基于Wide &amp; Deep和深度兴趣网络DIN来构建精排模型，这两种模型都存在一定的问题。</p>
<p><strong>Wide &amp; Deep</strong>：使用商品的类别和品牌特征、商品的统计特征、用户画像特征，通过Wide 和Deep两个部分来预测。离散特征使用常用的embedding来降维。这一框架效果还算不错，但是实践中忽略了用户历史的行为序列特征。</p>
<p><strong>DIN</strong>：该模型使用注意力机制来捕获目标商品与用户先前行为序列中商品之间的相似性，但未考虑用户行为序列背后的序列性质。</p>
<p>因此，本文尝试将NLP领域中大放异彩的Transformer模型来做推荐任务，我们一起来看下模型的整体框架。</p>
<h2 id="2、模型框架-1"><a href="#2、模型框架-1" class="headerlink" title="2、模型框架"></a>2、模型框架</h2><p>本文提出的模型称为<strong>Behavior Sequence Transformer</strong>，简称BST，其整体的架构如下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-1418d5b13a902e80.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>接下来，我们来详细介绍下。</p>
<h4 id="2-1-Embedding-Layer"><a href="#2-1-Embedding-Layer" class="headerlink" title="2.1 Embedding Layer"></a>2.1 Embedding Layer</h4><p>所有的other feature分为四个部分，用户特征、商品特征、上下文特征、交叉特征，特征示例如下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-0583579fd0b29b1c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>特征通过Embedding Layer来转换为对应的embedding。这里的交叉特征，有部分是基于业务经验提取出的，并非完全通过网络学习得到。</p>
<p>此外，我们还获得了行为序列中每个商品以及目标 商品的嵌入特征。每一个商品通过两部分来表示，“序列 item 特征”（红色部分）和“位置特征”（深蓝色）。其中，“序列 item 特征”包括 item_id 和 category_id。</p>
<p>这里，我们重点讲一下位置特征。在《Attention is all you need》论文中，作者提出了一种Positional Encoding来捕获句子中的顺序信息。同样，顺序也存在于用户的行为序列中。因此，我们添加“位置”作为 bottom layer 中每个 item 的输入特征，然后将其投射为低维向量。第i个位置的位置特征计算方式为pos(vi)=t(vt)-t(vi)，其中，t(vt) 表示推荐的时间戳，t(vi) 表示用户点击商品vi时的时间戳。采用这种方式，其效果优于原论文中使用的 sin 和 cos 函数。</p>
<p>至于如何将一个时间戳转化为低维向量，论文里也没有明确说明，可能是进行离散化后转化为embedding吧，如果有更好的想法的小伙伴，欢迎在留言区留言。</p>
<h4 id="2-2-Transformer-Layer"><a href="#2-2-Transformer-Layer" class="headerlink" title="2.2 Transformer Layer"></a>2.2 Transformer Layer</h4><p>感觉Transformer层没有做太多的优化，基本和原论文是一样的：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-4968ee5841ad858d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>有关Transfromer的内容，我们就不再详细介绍了，大家可以参考下面的文章，讲的比较通俗易懂：</p>
<p><a href="https://mp.weixin.qq.com/s/RLxWevVWHXgX-UcoxDS70w" target="_blank" rel="external">https://mp.weixin.qq.com/s/RLxWevVWHXgX-UcoxDS70w</a></p>
<p>这里所做的改进主要在于，在block中的Feed Forward时，使用了leakyRelu，如下图所示：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-852fe1e81e01bdb5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>在上面的两步之后，我们得到的输出中已经有序列信息在里面了，首先我们把时间戳信息作为位置特征进行了输入，其次，transformer本身也类似于一个双向的循环神经网络（我个人感觉是这样的，masked-self attention相当于单向的rnn，不加mask相当于双向的rnn）。</p>
<h4 id="2-3-MLP-Layer"><a href="#2-3-MLP-Layer" class="headerlink" title="2.3 MLP Layer"></a>2.3 MLP Layer</h4><p>接下来，将所有的embedding进行拼接，输入到三层的神经网络中，并最终通过sigmoid函数转换为0-1之间的值，代表用户点击目标商品的概率。</p>
<p>模型通过logloss进行训练：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-896cf02b41fb81d7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<h2 id="3、实验结果及分析"><a href="#3、实验结果及分析" class="headerlink" title="3、实验结果及分析"></a>3、实验结果及分析</h2><p>BST的参数设置如下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-b274bb24444688ce.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<h4 id="3-1-对比试验"><a href="#3-1-对比试验" class="headerlink" title="3.1 对比试验"></a>3.1 对比试验</h4><p>这里，主要使用了三个对比模型，分别是Wide &amp; Deep、DIN和Wide &amp; Deep(+Seq)。最后一个模型中，将用户历史行为序列中item对应的embedding的平均值加入到模型中。</p>
<p>对比试验结果如下</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-84a7a47b97edd862.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<h4 id="3-2-Transformer中block层数实验"><a href="#3-2-Transformer中block层数实验" class="headerlink" title="3.2 Transformer中block层数实验"></a>3.2 Transformer中block层数实验</h4><p>Transformer的block是可以进行堆叠的，论文里实验了1层、2层和3层的效果，最终1层的效果最好：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-fb9c26b4c0317f1b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-c793bfe88d300871.jpeg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>关注小编的公众号，后台回复“进群”，一起来交流学习吧！</p>
<h1 id="推荐系统遇上深度学习-四十九-九篇阿里推荐相关论文汇总！"><a href="#推荐系统遇上深度学习-四十九-九篇阿里推荐相关论文汇总！" class="headerlink" title="推荐系统遇上深度学习(四十九)-九篇阿里推荐相关论文汇总！"></a>推荐系统遇上深度学习(四十九)-九篇阿里推荐相关论文汇总！</h1><p>温故而知新！前面的四十八篇文章中，单单是阿里的文章咱们就写了九篇了。今天就来简单回顾一下！</p>
<p>相关论文咱也不一一贴地址了，关注公众号“小小挖掘机”后台回复“阿里推荐”，打包下载所有的论文。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-c793bfe88d300871.jpeg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>业界常用的推荐系统主要分为两个阶段，召回阶段和精排阶段，当然有时候在最后还会接一些打散或者探索的规则，这点咱们就不考虑了。</p>
<p>前面九篇文章中，有三篇是召回阶段的文章，六篇是排序阶段的文章。在召回阶段，主要是基于item的embedding来做的，因此前两篇我们介绍一下大规模商品集下的embedding策略。第三篇将介绍一下深度树匹配召回模型。在排序阶段，主要是进行CTR或者CVR的预估，我们先来介绍一下MLR模型，然后介绍一下主要基于用户历史行为来做推荐的DIN、DIEN、DSIN和BST模型。最后介绍一下多任务模型ESMM。</p>
<p>本文主要简单介绍一下论文中提出的模型的内容，以及为什么要这么做。至于详细的内容，可以通过给出的文章链接以及论文进行进一步的学习。</p>
<h2 id="1、Billion-scale-Commodity-Embedding-for-E-commerce-Recommendation-in-Alibaba"><a href="#1、Billion-scale-Commodity-Embedding-for-E-commerce-Recommendation-in-Alibaba" class="headerlink" title="1、Billion-scale Commodity Embedding for E-commerce Recommendation in Alibaba"></a>1、Billion-scale Commodity Embedding for E-commerce Recommendation in Alibaba</h2><p>这篇文章中，使用图嵌入(Graph Embedding)的方法来学习十亿级商品的Embedding。基本的过程如下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-22677981881da65e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1000" alt=""></p>
<p>首先，将用户的历史行为切割为不同的Session，每个Session表示一个行为序列；随后，将这些行为序列表示为有向带权图，图中的权重代表所有行为序列中两个物品的行为转移次数；然后，基于随机游走的方式，从图中得到更多的行为序列；最后，通过Skip-Gram的方式，学习每个物品的Embedding。</p>
<p>为了更好的解决冷启动问题，在学习物品Embedding时，进一步加入辅助信息Side-Information。这样，一个物品可表示为[w<sup>0</sup>,w<sup>1</sup>,..,w<sup>n</sup>]，其中w<sup>0</sup>代表物品本身的Embedding，w<sup>1</sup>,..,w<sup>n</sup>代表n中side-information对应的Embedding。这样，聚合后的物品aggregated embeddings计算有两种方式，直接平均和加权平均：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-6bde568142884aaa.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-c95d69d362880ff2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>直接平均的方式称为Graph Embedding with Side information (GES)，加权平均的方式称为 Enhanced Graph Embedding with Side information (EGES)。二者的流程如下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-4a225b5f18b69ea8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/802" alt=""></p>
<p>在加入了side-information之后，新加入的物品的embedding可以通过其side-information对应的embedding的平均值来表示了。一般来说新的物品都有对应的side-information。同时embedding基本会按天进行更新。</p>
<p>关于该方法更多的介绍，参考文章：<a href="https://www.jianshu.com/p/229b686535f1" target="_blank" rel="external">https://www.jianshu.com/p/229b686535f1</a></p>
<h2 id="2、Learning-and-Transferring-IDs-Representation-in-E-commerce"><a href="#2、Learning-and-Transferring-IDs-Representation-in-E-commerce" class="headerlink" title="2、Learning and Transferring IDs Representation in E-commerce"></a>2、Learning and Transferring IDs Representation in E-commerce</h2><p>本文提到的方法，大体的过程跟上一篇文章是类似的。但是行为序列直接使用用户日志中的行为序列，没有通过随机游走的方式从构造的图中产生。</p>
<p>在得到行为序列之后，同样通过Skip-Gram来学习物品的Embedding。每一个物品对应多组Embedding：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-6c3b5b223ea151b5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1000" alt=""></p>
<p>这里，id<sub>1</sub>(item<sub>i</sub>)代表物品本身，id<sub>2</sub>(item<sub>i</sub>)代表产品ID，id<sub>3</sub>(item<sub>3</sub>)代表店铺ID等等。</p>
<p>此时计算物品之间共现概率的公式如下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-94f1d148fca59fa0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/412" alt="">。</p>
<p>与第一篇文章通过平均把物品本身的Embedding和Side-Information对应的Embedding进行聚合的方式不同。这里分别计算了两个物品自身的Embedding和Side-Information对应的Embedding之间的相似性，随后进行求和作为两个物品的共现概率。第一种方式中，每个Embedding的长度必须一致，但是在后一种方式中，每种embedding的长度则无须一致。</p>
<p>而权重也是定义好的，并非像第一篇文章一样通过模型学习得到。权重定义如下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-dc6eb648d3bffb9a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/406" alt=""></p>
<p>上面的D是整个商品集，举个例子来说明一下上面的流程，假设当前是计算品牌这个side-information的权重，商品i的品牌是阿迪达斯，遍历所有的商品集D，发现共有10个品牌是阿迪达斯的物品，那么此时的V<sub>ik</sub>=10,w<sub>ik</sub>=1/10 。而w<sub>i1</sub>=1是确定的，因为第i个物品在商品集中是唯一出现的。</p>
<p>除了上面计算的item之间的共现概率外，我们还希望，属性ID和itemID之间也要满足一定的关系，简单理解就是希望itemID和其对应的属性ID关系越近越好，于是定义：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-d46efb6d87fee3e9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/476" alt=""></p>
<p>由于每种Embedding长度不同，使用M<sub>k</sub>来进行线性变换为一样的长度。</p>
<p>于是，我们期望最大化的式子为：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-3f62740bf4cb11cc.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/629" alt=""></p>
<p>此时，对于新的物品，同样通过其side-information来表示其embedding：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-eb723ec62d6e4c06.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>关于该方法更多的介绍，参考文章：<a href="https://www.jianshu.com/p/285978e29458" target="_blank" rel="external">https://www.jianshu.com/p/285978e29458</a></p>
<h2 id="3、TDM：Learning-Tree-based-Deep-Model-for-Recommender-Systems"><a href="#3、TDM：Learning-Tree-based-Deep-Model-for-Recommender-Systems" class="headerlink" title="3、TDM：Learning Tree-based Deep Model for Recommender Systems"></a>3、TDM：Learning Tree-based Deep Model for Recommender Systems</h2><p>推荐系统中的召回策略主要有协同过滤、向量化召回。使用协同过滤进行召回的时候，并不能真正的面向全量商品库来做检索，如itemCF方法，系统只能在用户历史行为过的商品里面找到侯选的相似商品来做召回，使得整个推荐结果的多样性和发现性比较差。向量化召回是目前推荐召回核心发展的一代技术，但是它对模型结构做了很大的限制，必须要求模型围绕着用户和向量的embedding展开，同时在顶层进行内积运算得到相似性。</p>
<p>基于上面的缺点，阿里提出了深度树匹配的召回策略。深度树匹配的核心是构造一棵兴趣树，其叶子结点是全量的物品，每一层代表一种细分的兴趣：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-cbaf6a8236cbba12.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/414" alt=""></p>
<p>接下来，我们主要介绍三个方面的内容：<br>1）怎么基于树来实现高效的检索<br>2）怎么在树上面做兴趣建模<br>3）兴趣树是怎么构建的</p>
<h4 id="3-1-怎么基于树来实现高效的检索-1"><a href="#3-1-怎么基于树来实现高效的检索-1" class="headerlink" title="3.1 怎么基于树来实现高效的检索"></a>3.1 怎么基于树来实现高效的检索</h4><p>在这里，假设已经得到深度树的情况下，高效检索采用的是Beam-Search的方式：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-6bff4b1e8d3ec8ad.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1000" alt=""></p>
<h4 id="3-2-怎么在树上面做兴趣建模-1"><a href="#3-2-怎么在树上面做兴趣建模-1" class="headerlink" title="3.2 怎么在树上面做兴趣建模"></a>3.2 怎么在树上面做兴趣建模</h4><p>在已经得到深度树的情况下，一个新来的用户，我们怎么知道他对哪个分支的兴趣更大呢？我们首先需要将树建立为一棵最大堆树。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-40f93e5bcec661f7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1000" alt=""></p>
<p>在实践中，构造最大堆树可以举个简单的例子，假设用户对叶子层 ITEM6 这样一个节点是感兴趣的，那么可以认为它的兴趣是 1，同层其他的节点兴趣为 0，从而也就可以认为 ITEM6 的这个节点上述的路径的父节点兴趣都为 1，那么这一层就是 SN3 的兴趣为 1，其他的为 0，这层就是 LN2 的兴趣为 1，其他为 0。如下图所示：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-e63997cc3ff9eda5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/622" alt=""></p>
<p>当建立起如上的树之后，我们就可以在每一层构建一定的正负样本，通过构建模型来学习用户对于每一层节点的兴趣偏好。注意的是，每层的偏好都要学习，也就是说每层都要构建一个模型。同时，模型只需要关心是否足够拟合样本就可以了，并没有要求模型一定要把用户特征和 item 特征转换为顶层向量内积的形式，这样就给了模型很大的自由度，只要去拟合好足够的样本，那么任意的模型都是 OK 的。下面是一个模型的示例：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-73995f68b88ba2d6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1000" alt=""></p>
<h4 id="3-3-兴趣树是怎么构建的-1"><a href="#3-3-兴趣树是怎么构建的-1" class="headerlink" title="3.3 兴趣树是怎么构建的"></a>3.3 兴趣树是怎么构建的</h4><p>前面两个问题，都是在给定树结构的情况下来介绍的，那么怎么来构建一棵兴趣树呢？每层是怎么分叉的呢？</p>
<p>树的叶节点对应具体的 item，目标是构建一个好的树结构来优化我们的检索效果。通过前面的分析知道，在进行兴趣建模时，对于叶子层的样本我们通过用户行为反馈得到，而中间层的样本则通过树结构采样得到。所以树结构决定了中间层的样本。</p>
<p>在进行快速检索时，采用从顶向下的检索策略，利用的是对每一层节点兴趣建模进行快速剪枝。要保证最终的检索效果，就需要每一层的兴趣判别模型能力足够强。由于树结构负责我们中间层的样本生成，所以我们的思路是通过优化树结构影响样本生成进而提升模型能力。具体来说，通过树结构优化降低中间层的样本混淆度，让中间层样本尽可能可分。</p>
<p>所以，整个树结构的生成创建和优化的过程，实际上是围绕着怎么来生成更好的样本、帮助模型学习的视角进行的，而不是只是考虑相似、聚类这样的模式。那么这里的核心方案是什么呢？</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-f3de0f31b9062f0a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1000" alt="image"></p>
<p>方案总结来说，就是最小化用户行为序列中相近的item-pair在树上的距离。假设用户的行为序列为A-》B-》D-》C，那么我们希望(A,B),(B,D),(D,C)在树上的距离越近越好。两个叶子结点的距离通过其最近的公共祖先确定。</p>
<p>好了，到这里，对深度树匹配模型做一个简单的总结：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-3a8e8cb26c15c6ee.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1000" alt="image"></p>
<p>关于该方法更多的介绍，参考文章：<a href="https://www.jianshu.com/p/ef3caa5672c8" target="_blank" rel="external">https://www.jianshu.com/p/ef3caa5672c8</a></p>
<h2 id="4、Learning-Piece-wise-Linear-Models-from-Large-Scale-Data-for-Ad-Click-Prediction"><a href="#4、Learning-Piece-wise-Linear-Models-from-Large-Scale-Data-for-Ad-Click-Prediction" class="headerlink" title="4、Learning Piece-wise Linear Models from Large Scale Data for Ad Click Prediction"></a>4、Learning Piece-wise Linear Models from Large Scale Data for Ad Click Prediction</h2><p>这篇文章介绍的方法就是我们所熟知的MLR(mixed logistic regression)算法。MLR可以看做是对LR的一个自然推广，它采用分而治之的思路，用分片线性的模式来拟合高维空间的非线性分类面，其形式化表达如下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-d4572939999edaf3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>其中u是聚类参数，决定了空间的划分，w是分类参数，决定空间内的预测。这里面超参数分片数m可以较好地平衡模型的拟合与推广能力。当m=1时MLR就退化为普通的LR，m越大模型的拟合能力越强，但是模型参数规模随m线性增长，相应所需的训练样本也随之增长。因此实际应用中m需要根据实际情况进行选择。例如，在阿里的场景中，m一般选择为12。下图中MLR模型用4个分片可以完美地拟合出数据中的菱形分类面。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-d9da3d968a2fdc1c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>在实际中，MLR算法常用的形式如下，使用softmax作为分片函数：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-ab8a627ded650751.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>关于损失函数的设计，采用了logloss以及L1，L2正则，形式如下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-65a604bb693beebf.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>关于该方法更多的介绍，参考文章：<a href="https://www.jianshu.com/p/627fc0d755b2" target="_blank" rel="external">https://www.jianshu.com/p/627fc0d755b2</a></p>
<h2 id="5、Deep-Interest-Network-for-Click-Through-Rate-Prediction"><a href="#5、Deep-Interest-Network-for-Click-Through-Rate-Prediction" class="headerlink" title="5、Deep Interest Network for Click-Through Rate Prediction"></a>5、Deep Interest Network for Click-Through Rate Prediction</h2><p>阿里相关人员通过观察收集到的线上数据，发现了电商领域用户行为数据中两个很重要的特性，分别是<strong>Diversity</strong>和<strong>Local activation</strong>。Diversity指用户在浏览电商网站的过程中显示出的兴趣是十分多样性的；Local activation指由于用户兴趣的多样性，只有部分历史数据会影响到当次推荐的物品是否被点击，而不是所有的历史记录。</p>
<p>针对上面提到的用户行为中存在的两种特性，阿里将其运用于自身的推荐系统中，推出了深度兴趣网路DIN，其架构如下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-4250d6869c6481a3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>DIN模型结构相对还是简单的，用户历史行为序列中每一个物品都对应着物品ID和店铺ID，目标物品(广告)也有物品ID和店铺ID，物品ID和店铺ID都会转换为对应的Embedding。随后，行为序列中物品ID对应的Embedding会分别与目标物品ID的Embedding计算attention score，并根据attention score求加权平均；行为序列中物品对应店铺ID的Embedding会分别与目标物品对应店铺ID的Embedding计算attention score，并根据attention score求加权平均。最后，所有的向量拼接后，通过全连接神经网络得到最终的输出。</p>
<p>关于该方法更多的介绍，参考文章：<a href="https://www.jianshu.com/p/73b6f5d00f46" target="_blank" rel="external">https://www.jianshu.com/p/73b6f5d00f46</a></p>
<h2 id="6、Deep-Interest-Evolution-Network-for-Click-Through-Rate-Prediction"><a href="#6、Deep-Interest-Evolution-Network-for-Click-Through-Rate-Prediction" class="headerlink" title="6、Deep Interest Evolution Network for Click-Through Rate Prediction"></a>6、Deep Interest Evolution Network for Click-Through Rate Prediction</h2><p>DIN中存在两个缺点，首先用户的兴趣是不断进化的，而DIN抽取的用户兴趣之间是独立无关联的，没有捕获到兴趣的动态进化性，其次是通过用户的显式的行为来表达用户隐含的兴趣，这一准确性无法得到保证。</p>
<p>基于以上两点，阿里提出了深度兴趣演化网络DIEN来CTR预估的性能，其模型结构如下：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/4155986-df004503462d5103?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="DIEN"></p>
<p>DIEN中包含Embedding层、兴趣抽取层Interest Extractor Layer、兴趣进化层Interest Evolution Layer和全连接层。</p>
<p>兴趣抽取层Interest Extractor Layer的主要目标是从embedding数据中提取出interest。但一个用户在某一时间的interest不仅与当前的behavior有关，也与之前的behavior相关，所以作者们使用GRU单元来提取interest。在兴趣抽取层，为了判定兴趣是否表示的合理，文中别出心裁的增加了一个辅助loss，来提升兴趣表达的准确性：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-bbb5a385306857b6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>这里，作者设计了一个二分类模型来计算兴趣抽取的准确性，我们将用户下一时刻真实的行为e(t+1)作为正例，负采样得到的行为作为负例e(t+1)’，分别与抽取出的兴趣h(t)结合输入到设计的辅助网络中，得到预测结果，并通过logloss计算一个辅助的损失。</p>
<p>兴趣进化层Interest Evolution Layer的主要目标是刻画用户兴趣的进化过程。用户的兴趣在变化过程中遵循如下规律：1）<strong>interest drift</strong>：用户在某一段时间的interest会有一定的集中性。比如用户可能在一段时间内不断买书，在另一段时间内不断买衣服。2）<strong>interest individual</strong>：一种interest有自己的发展趋势，不同种类的interest之间很少相互影响，例如买书和买衣服的interest基本互不相关。为了利用这两个时序特征，这里通过attention机制以找到与target AD相关的interest。同时把attention score加入到GRU单元中。</p>
<p>关于该方法更多的介绍，参考文章：<a href="https://www.jianshu.com/p/6742d10b89a8" target="_blank" rel="external">https://www.jianshu.com/p/6742d10b89a8</a></p>
<h2 id="7、Deep-Session-Interest-Network-for-Click-Through-Rate-Prediction"><a href="#7、Deep-Session-Interest-Network-for-Click-Through-Rate-Prediction" class="headerlink" title="7、Deep Session Interest Network for Click-Through Rate Prediction"></a>7、Deep Session Interest Network for Click-Through Rate Prediction</h2><p>从用户行为中呢，我们发现，在每个会话中的行为是相近的，而在不同会话之间差别是很大的。为了刻画这种与行为密切相关的行为表现，阿里提出了深度会话兴趣网络Deep Session Interest Network。其模型结构如下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-d6a96b2997f0e937.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1000" alt=""></p>
<p>DSIN在全连接层之前，分成了两部分，左边的那一部分，将用户特征和物品特征转换对应的向量表示，这部分主要是一个embedding层。右边的那一部分主要是对用户行为序列进行处理，从下到上分为四层：<br><strong>1）序列切分层session division layer</strong><br><strong>2）会话兴趣抽取层session interest extractor layer</strong><br><strong>3）会话间兴趣交互层session interest interacting layer</strong><br><strong>4）会话兴趣激活层session interest acti- vating layer</strong></p>
<p>Session Division Layer这一层将用户的行文进行切分，首先将用户的点击行为按照时间排序，判断每两个行为之间的时间间隔，前后的时间间隔大于30min，就进行切分。切分后，我们可以将用户的行为序列S转换成会话序列Q。第k个会话Q<sub>k</sub>=[b<sub>1</sub>;b<sub>2</sub>;…;b<sub>i</sub>;…;b<sub>T</sub>],其中，T是会话的长度，b<sub>i</sub>是会话中第i个行为，是一个d维的embedding向量。所以Q<sub>k</sub>是T <em> d的。而Q，则是K </em> T * d的</p>
<p>在Session Interest Extractor Layer，使用transformer对每个会话的行为进行处理。同时在Transformer中，对输入的序列会进行Positional Encoding。经过Transformer处理之后，经过一个avg pooling操作，将每个session兴趣转换成一个d维向量。</p>
<p>在Session Interest Interacting Layer，通过一个双向LSTM(bi-LSTM)来获取用户的会话兴趣，得到的隐藏层状态H<sub>t</sub>，我们可以认为是混合了上下文信息的会话兴趣。</p>
<p>在Session Interest Activating Layer，通过注意力机制来刻画用户的会话兴趣与目标物品的相关性，attention分成了两个部分，分别是目标物品与原始会话兴趣的相关性（没有通过bi-LSTM前的会话向量）以及目标物品与混合了上下文信息的会话兴趣（通过bi-LSTM后得到的隐藏层向量）的相关性。</p>
<p>最后的话，就是把四部分的向量：用户特征向量、待推荐物品向量、会话兴趣加权向量U<sup>I</sup>、带上下文信息的会话兴趣加权向量U<sup>H</sup>进行横向拼接，输入到全连接层中，得到输出。</p>
<p>关于该方法更多的介绍，参考文章：<a href="https://www.jianshu.com/p/82ccb10f9ede" target="_blank" rel="external">https://www.jianshu.com/p/82ccb10f9ede</a></p>
<h2 id="8、Behavior-Sequence-Transformer-for-E-commerce-Recommendation-in-Alibaba"><a href="#8、Behavior-Sequence-Transformer-for-E-commerce-Recommendation-in-Alibaba" class="headerlink" title="8、Behavior Sequence Transformer for E-commerce Recommendation in Alibaba"></a>8、Behavior Sequence Transformer for E-commerce Recommendation in Alibaba</h2><p>为了能够将用户行为序列以及序列中商品与待推荐商品之间的相关性等因素加入到模型中，阿里尝试用NLP领域中大放异彩的Transformer模型来做推荐任务，提出了<strong>Behavior Sequence Transformer</strong>，简称BST模型，其整体的架构如下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-1418d5b13a902e80.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>模型分为Embedding Layer、Transformer Layer和MLP Layer。</p>
<p>为了将行为序列信息加入到模型中，在Embedding Layer中，加入了位置特征的Embedding，第i个位置的位置特征计算方式为pos(vi)=t(vt)-t(vi)，其中，t(vt) 表示推荐的时间戳，t(vi) 表示用户点击商品vi时的时间戳。至于如何将一个时间戳转化为低维向量，可能是进行离散化后再转化为embedding。</p>
<p>在Transformer Layer，并不是只有行为序列中商品来做multi-head attention，待推荐的商品也会加入其中，这样其实就包含了行为序列中商品与待推荐商品之间的相关性。</p>
<p>关于该方法更多的介绍，参考文章：<a href="https://www.jianshu.com/p/caa2d87cb78c" target="_blank" rel="external">https://www.jianshu.com/p/caa2d87cb78c</a></p>
<h2 id="9、Entire-Space-Multi-Task-Model-An-E-ective-Approach-for-Estimating-Post-Click-Conversion-Rate"><a href="#9、Entire-Space-Multi-Task-Model-An-E-ective-Approach-for-Estimating-Post-Click-Conversion-Rate" class="headerlink" title="9、Entire Space Multi-Task Model: An E ective Approach for Estimating Post-Click Conversion Rate"></a>9、Entire Space Multi-Task Model: An E ective Approach for Estimating Post-Click Conversion Rate</h2><p>本文介绍的是一个Multi-Task的方法，称为ESMM。其关注的焦点是CVR预估，CVR代表从点击到购买的转化。不过传统的CVR预估问题存在着两个主要的问题：<strong>样本选择偏差</strong>和<strong>稀疏数据</strong>。如下图：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-31ccd3226cc1426d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p><strong>样本选择偏差(sample selection bias,SSB)</strong>指训练模型仅通过有点击行为的样本空间即途中灰色椭圆部分来进行，但训练好的模型是在整个样本空间X去做推断的。<strong>数据稀疏(data sparsity,DS)</strong>指点击行为的样本空间相对于整个样本空间来说是很小的。</p>
<p>ESMM模型借鉴了多任务学习的思路，引入了两个辅助的学习任务，分别用来拟合pCTR和pCTCVR，从而同时消除了上文提到的两个挑战。ESMM模型能够充分利用用户行为的顺序性模式，其模型架构下图所示：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-e583e6dbf39b38d5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>可以看到，ESMM模型由两个子网络组成，左边的子网络用来拟合pCVR，右边的子网络用来拟合pCTR，同时，两个子网络的输出相乘之后可以得到pCTCVR。因此，该网络结构共有三个子任务，分别用于输出pCTR、pCVR和pCTCVR。</p>
<p>假设我们用x表示feature(即impression),y表示点击，z表示转化，那么根据pCTCVR = pCTR * pCVR，可以得到：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-148dbce6adc8dd64.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>将乘法转化为除法，我们可以得到pCVR的计算：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-716f554b649e1c43.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>由上面提到的等式可以看出，pCVR是可以通过pCTR和pCTCVR的预估推导出来的。因此，我们只需要关注pCTR和pCTCVR两个任务即可。为什么是这两个任务呢？其实就是为了消除样本选择偏差嘛，因为CVR是从click到conversion，而CTCVR是从impression到conversion，CTR是从impression到conversion，所以CTR和CTCVR都可以从整个样本空间进行训练，一定程度的消除了样本选择偏差。</p>
<p>关于该方法更多的介绍，参考文章：<a href="https://www.jianshu.com/p/35f00299c059" target="_blank" rel="external">https://www.jianshu.com/p/35f00299c059</a></p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-c793bfe88d300871.jpeg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<h1 id="推荐系统遇上深度学习-五十-使用强化学习优化用户的长期体验"><a href="#推荐系统遇上深度学习-五十-使用强化学习优化用户的长期体验" class="headerlink" title="推荐系统遇上深度学习(五十)-使用强化学习优化用户的长期体验"></a>推荐系统遇上深度学习(五十)-使用强化学习优化用户的长期体验</h1><p>在现有的推荐模型中，往往优化的目标是点击率，而忽略了用户的长期体验。特别是在信息流推荐中，给用户推荐一个标题很吸引人但内容比较无聊的消息，往往点击率很高，但用户会觉得体验很差。因此，用户的长期体验也需要重视。本文介绍京东与清华大学合作发表的论文，该论文使用强化学习来优化信息流推荐中用户的长期体验。咱们一起来学习一下。</p>
<p>论文题目：《Reinforcement Learning to Optimize Long-term User Engagement in Recommender Systems》</p>
<p>论文下载地址：<a href="http://export.arxiv.org/abs/1902.05570" target="_blank" rel="external">http://export.arxiv.org/abs/1902.05570</a></p>
<h2 id="1、问题定义-1"><a href="#1、问题定义-1" class="headerlink" title="1、问题定义"></a>1、问题定义</h2><p>本文是京东和清华合作发表的论文，所以我们可以把它视作京东的推荐场景，用户会不断的与推荐系统推荐出的物品进行交互。当页面出现一个物品时，用户可以点击并查看消息详情、进行下单、忽略该物品、继续下滑查看更多物品或者关闭app。推荐系统要做的就是在这个过程中优化用户的体验。这种体验包括两方面，即时体验如用户点击率 和 长期体验如用户的黏度。用户的黏度又可以体现在许多方面，如用户在APP中的停留时间、打开APP的频率等等。</p>
<p>信息流推荐中，可以看作是第1次推荐-用户反馈-第2次推荐-用户反馈—-第n次推荐-用户反馈这样的一个过程，或者说，我们可以将其定义为一个马尔可夫决策过程。因而可以通过强化学习方法来进行学习。对于一个马尔可夫决策过程，通常用<s、a、p、r、γ>来表示。在本文的推荐场景中，这几部分的定义分别如下：</s、a、p、r、γ></p>
<p><strong>状态(State)S</strong> : 在这里，我们假定每次推荐一个物品i。初始的状态s<sub>1</sub>={u}，即只有用户的信息。当进行了第一次推荐后，状态变为s<sub>2</sub>={u,(i<sub>1</sub>,f<sub>1</sub>,d<sub>1</sub>)}。当推荐过t-1个物品后，状态s<sub>t</sub> = {u,(i<sub>1</sub>,f<sub>1</sub>,d<sub>1</sub>),(i<sub>2</sub>,f<sub>2</sub>,d<sub>2</sub>),…,(i<sub>t-1</sub>,f<sub>t-1</sub>,d<sub>t-1</sub>)}。即s<sub>t</sub> = s<sub>t-1</sub> + {(i<sub>t-1</sub>,f<sub>t-1</sub>,d<sub>t-1</sub>)}。这里i<sub>t-1</sub>代表第t-1时刻推荐的物品，f<sub>t-1</sub>表示用户对物品i<sub>t-1</sub>作出的反馈，d<sub>t-1</sub>表示用户对推荐的物品i<sub>t-1</sub>的停留时间。</p>
<p><strong>动作(Action)A</strong>：这里的动作空间是可推荐的物品的集合，与当前的状态s相关，计作A(s)。初始阶段，所有的物品都可以进行推荐，随着时间步的推进，每推荐一个物品，就将其从可推荐物品集合中剔除。而时刻t的动作就是该轮推荐的物品i<sub>t</sub></p>
<p><strong>状态转移概率P</strong>：状态转移概率可以用如下的式子表示：p(s<sub>t+1</sub> | s<sub>t</sub>,i<sub>t</sub>)。</p>
<p><strong>奖励(Reward)R</strong>: 由于我们不仅要优化用户的即时体验，还要优化用户的长期体验，因此这的即时奖励r比较复杂，这里将其定义为:</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-306b286f881c22f7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>可以理解为总的即时奖励r，是一堆分解奖励的加权平均，那具体包含哪些因素呢？这里主要包含以下三个方面：</p>
<p><strong>点击次数</strong>：用户对该次推荐的物品的点击次数。<br><strong>滑动深度</strong>：表示用户在经过该次推荐之后，又往下滑动了多少屏。也就是说，该次推荐之后，又经过了多少轮推荐。上面解释的可能比较晦涩，举个简单的例子。假设在一次信息流推荐的过程中，共进行了10轮，当前是第4轮，后面还有6轮，那么其滑动深度的奖励就是6 。<br><strong>回归时间</strong>：用户下次访问APP的时间，如果这个时间间隔越小，奖励就越大，所以通常表示为此次访问和下次访问APP中间的时间间隔的倒数：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-2df384d344e0d226.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>介绍完了MDP中的各个部分，接下来我们就看一下如何使用强化学习进行推荐吧。</p>
<h2 id="2、模型介绍-3"><a href="#2、模型介绍-3" class="headerlink" title="2、模型介绍"></a>2、模型介绍</h2><p>这里模型分为两部分，一个是Q网络，一个是S网络，Q网络来拟合状态价值函数Q(s<sub>t</sub>,i<sub>t</sub>)，S网络是一个仿真环境，用于输出上文介绍的各个部分的奖励值。接下来，我们分别介绍以下两个部分。</p>
<h4 id="2-1-Q网络"><a href="#2-1-Q网络" class="headerlink" title="2.1 Q网络"></a>2.1 Q网络</h4><p>这里的模型学习的是状态价值函数Q(s<sub>t</sub>,i<sub>t</sub>),其网络结构如下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-1b0b1129cb1ee0b7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>前面介绍了，状态是s<sub>t</sub> = {u,(i<sub>1</sub>,f<sub>1</sub>,d<sub>1</sub>),(i<sub>2</sub>,f<sub>2</sub>,d<sub>2</sub>),…,(i<sub>t-1</sub>,f<sub>t-1</sub>,d<sub>t-1</sub>)}，这里第t轮时之前推荐过的所有物品组成的集合{i<sub>j</sub>}={i<sub>1</sub>,i<sub>2</sub>,…,i<sub>t-1</sub>}。物品首先转换为对应的embedding，计作{<strong>i<sub>j</sub></strong>}(这里是粗体)。</p>
<p>随后，每一种用户的反馈形式都会对应一个投影矩阵，假设物品的embedding长度为H，那么投影矩阵的形状是H * H的。这样，将物品的embedding经过投影矩阵的变换后，得到新的物品embedding：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-1d96d031c66bd767.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>随后，新的物品embedding，会和停留时间进行拼接，输入到4个不同的LSTM部分中。这四个LSTM含义分别如下：</p>
<p>1）将状态s中包含的物品集合{i<sub>j</sub>}，按顺序输入到LSTM中，得到最终的输出。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-3908706420c82e69.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>2）将状态s包含的物品集合{i<sub>j</sub>}中，用户反馈形式是skip的物品按顺序输入到LSTM中，得到最终的输出。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-23ced314d7105fcb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>3）将状态s包含的物品集合{i<sub>j</sub>}中，用户反馈形式是click的物品按顺序输入到LSTM中，得到最终的输出。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-a92f10754346cdbc.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>4）将状态s包含的物品集合{i<sub>j</sub>}中，用户反馈形式是order的物品按顺序输入到LSTM中，得到最终的输出。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-1dd305aae670aeab.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>你可能会觉得，只要有第一个LSTM就足够了，为什么还要加入后面的部分呢？文中解释了两点原因。首先是，用户的不同行为是非常不均衡的，比如10个物品中，可能8个是忽略，只有1个是点击或购买，那么点击或购买的信息很容易被忽略掉。其次是，用户的每种不同的行为，有其独有的特点。如点击行为通常表现出用户的当前兴趣偏好；购买行为表现出用户的兴趣转移过程等等。</p>
<p>在得到4部分的输出之后，将这四部分以及用户的embedding共五部分进行拼接，得到最终的state的向量表示：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-25d258b4efb6dd80.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>最后，将state的向量表示，以及待推荐物品的embedding(即action的向量表示)，输入到多层全连接神经网络中，得到最终输出状态价值Q的预估值：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-82ddb32266b1b04e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>而模型的损失函数为：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-7acf226020877794.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>这里好像是没有用到双网络结构的。因为文中给出的损失函数计算公式中，下一个状态-动作对的价值仍然使用网络θ<sub>q</sub>来计算得到。</p>
<h4 id="2-2-S网络"><a href="#2-2-S网络" class="headerlink" title="2.2 S网络"></a>2.2 S网络</h4><p>为了离线训练Q网络，这里文中通过一个S网络来对真实环境进行模拟，用来计算即时奖励r。该网络的结构如下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-741f57daf80ec332.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>S网络得到state的向量表示以及action的向量表示的过程，同Q网络。尽管结构与Q网络一样，但是参数与Q网络并不相同。这里模型的输出共有四部分，分别是预测用户的反馈形式、预测用户的停留时间、预测用户再次进入App的时间间隔、预测用户是否会关闭APP：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-d4d33881bd5c1ef2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-8c48a1f708512dcb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>S网络的损失函数如下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-77b3d22689e4a58f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>可以看到，这里对损失函数是进行了一定加权处理的，这里主要的原因是S网络只能通过实际线上日志来训练，而在S网络训练好之后，Q网络是可以利用S网络来生成训练数据并进行训练的。这样，Q网络的策略π和日志中的策略π<sub>b</sub>是有一定的偏差的。但我们希望S网络的策略π<sub>b</sub>与Q网络的策略π更相近。这里的策略可以理解为在某个状态s下推荐某个物品的概率。</p>
<p>文中通过加权的方式对损失函数进行处理，处理过程如下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-1496b661476dcf88.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<h4 id="2-3-整体架构"><a href="#2-3-整体架构" class="headerlink" title="2.3 整体架构"></a>2.3 整体架构</h4><p>总结一下整个模型的训练过程：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-c0a9e2104d7b2bc9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>首先我们会通过日志记录预训练一个S网络，这里应该不会对比Q网络的策略对损失进行加权。</p>
<p>随后，我们依次训练Q网络和S网络。这里Q网络的训练数据就可以配合S网络来生成。而S网络需要不断训练的原因是Q网络的策略π是不断变化的，那么损失函数中权重是不断变化的，因此需要反复训练来尽可能消除策略π<sub>b</sub>与Q网络的策略π的差异。</p>
<p>写在最后吧，文中有提到强化学习中的<strong>Deadly Triad</strong>问题。这里自己现在还没有理解。如果大家对这个感兴趣，可以参考下面的论文：<a href="https://arxiv.org/abs/1812.02648。" target="_blank" rel="external">https://arxiv.org/abs/1812.02648。</a></p>
<p>好了，本文的介绍就到这了，对强化学习和推荐系统感兴趣的小伙伴，一定要关注下面的公众号：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/4155986-c793bfe88d300871.jpeg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>#未完待续！</p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2020/03/07/推荐系统遇上深度学习系列二/" rel="next" title="">
                <i class="fa fa-chevron-left"></i> 
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/avatar1.jpg"
               alt="InvictusY" />
          <p class="site-author-name" itemprop="name">InvictusY</p>
           
              <p class="site-description motion-element" itemprop="description"></p>
          
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
              <a href="/archives/">
                <span class="site-state-item-count">20</span>
                <span class="site-state-item-name">日志</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-categories">
              <a href="/categories/index.html">
                <span class="site-state-item-count">7</span>
                <span class="site-state-item-name">分类</span>
              </a>
            </div>
          

          

        </nav>

        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="https://github.com/InvictusY" target="_blank" title="GitHub">
                  
                    <i class="fa fa-fw fa-rocket"></i>
                  
                    
                      GitHub
                    
                </a>
              </span>
            
          
        </div>

        
        

        
        

        


      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#推荐系统遇上深度学习-一-–FM模型理论和实践"><span class="nav-number">1.</span> <span class="nav-text">推荐系统遇上深度学习(一)–FM模型理论和实践</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1、FM背景"><span class="nav-number">1.1.</span> <span class="nav-text">1、FM背景</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2、one-hot编码带来的问题"><span class="nav-number">1.2.</span> <span class="nav-text">2、one-hot编码带来的问题</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3、对特征进行组合"><span class="nav-number">1.3.</span> <span class="nav-text">3、对特征进行组合</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4、FM求解"><span class="nav-number">1.4.</span> <span class="nav-text">4、FM求解</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#参考文章"><span class="nav-number">1.5.</span> <span class="nav-text">参考文章</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#推荐系统遇上深度学习-二-–FFM模型理论和实践"><span class="nav-number">2.</span> <span class="nav-text">推荐系统遇上深度学习(二)–FFM模型理论和实践</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1、FFM理论"><span class="nav-number">2.1.</span> <span class="nav-text">1、FFM理论</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2、FFM实现细节"><span class="nav-number">2.2.</span> <span class="nav-text">2、FFM实现细节</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#参考文章-1"><span class="nav-number">2.3.</span> <span class="nav-text">参考文章</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#推荐系统遇上深度学习-三-–DeepFM模型理论和实践"><span class="nav-number">3.</span> <span class="nav-text">推荐系统遇上深度学习(三)–DeepFM模型理论和实践</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1、背景"><span class="nav-number">3.1.</span> <span class="nav-text">1、背景</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2、DeepFM模型"><span class="nav-number">3.2.</span> <span class="nav-text">2、DeepFM模型</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3、相关知识"><span class="nav-number">3.3.</span> <span class="nav-text">3、相关知识</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4、代码解析"><span class="nav-number">3.4.</span> <span class="nav-text">4、代码解析</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#参考资料"><span class="nav-number">3.5.</span> <span class="nav-text">参考资料</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#推荐系统遇上深度学习-四-–多值离散特征的embedding解决方案"><span class="nav-number">4.</span> <span class="nav-text">推荐系统遇上深度学习(四)–多值离散特征的embedding解决方案</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1、背景-1"><span class="nav-number">4.1.</span> <span class="nav-text">1、背景</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2、解决方案"><span class="nav-number">4.2.</span> <span class="nav-text">2、解决方案</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#推荐系统遇上深度学习-五-–Deep-amp-Cross-Network模型理论和实践"><span class="nav-number">5.</span> <span class="nav-text">推荐系统遇上深度学习(五)–Deep&Cross-Network模型理论和实践</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1、原理"><span class="nav-number">5.1.</span> <span class="nav-text">1、原理</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2、实现解析"><span class="nav-number">5.2.</span> <span class="nav-text">2、实现解析</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#参考文章："><span class="nav-number">5.3.</span> <span class="nav-text">参考文章：</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#推荐系统遇上深度学习-六-–PNN模型理论和实践"><span class="nav-number">6.</span> <span class="nav-text">推荐系统遇上深度学习(六)–PNN模型理论和实践</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1、原理-1"><span class="nav-number">6.1.</span> <span class="nav-text">1、原理</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2、Product-Layer详细介绍"><span class="nav-number">6.2.</span> <span class="nav-text">2、Product Layer详细介绍</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-IPNN"><span class="nav-number">6.2.1.</span> <span class="nav-text">2.1 IPNN</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-OPNN"><span class="nav-number">6.2.2.</span> <span class="nav-text">2.2 OPNN</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3、代码实战"><span class="nav-number">6.3.</span> <span class="nav-text">3、代码实战</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#推荐系统遇上深度学习-七-–NFM模型理论和实践"><span class="nav-number">7.</span> <span class="nav-text">推荐系统遇上深度学习(七)–NFM模型理论和实践</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1、引言"><span class="nav-number">7.1.</span> <span class="nav-text">1、引言</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2、NFM模型介绍"><span class="nav-number">7.2.</span> <span class="nav-text">2、NFM模型介绍</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3、代码实战-1"><span class="nav-number">7.3.</span> <span class="nav-text">3、代码实战</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4、小结"><span class="nav-number">7.4.</span> <span class="nav-text">4、小结</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#推荐系统遇上深度学习-八-–AFM模型理论和实践"><span class="nav-number">8.</span> <span class="nav-text">推荐系统遇上深度学习(八)–AFM模型理论和实践</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1、引言-1"><span class="nav-number">8.1.</span> <span class="nav-text">1、引言</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2、AFM模型介绍"><span class="nav-number">8.2.</span> <span class="nav-text">2、AFM模型介绍</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3、代码实现"><span class="nav-number">8.3.</span> <span class="nav-text">3、代码实现</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#推荐系统遇上深度学习-九-–评价指标AUC原理及实践"><span class="nav-number">9.</span> <span class="nav-text">推荐系统遇上深度学习(九)–评价指标AUC原理及实践</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#引言"><span class="nav-number">9.1.</span> <span class="nav-text">引言</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1、从二分类评估指标说起"><span class="nav-number">9.2.</span> <span class="nav-text">1、从二分类评估指标说起</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-1-混淆矩阵"><span class="nav-number">9.2.0.1.</span> <span class="nav-text">1.1 混淆矩阵</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-2-准确率Accruacy"><span class="nav-number">9.2.0.2.</span> <span class="nav-text">1.2 准确率Accruacy</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-3-精确率Precision和召回率Recall"><span class="nav-number">9.2.0.3.</span> <span class="nav-text">1.3 精确率Precision和召回率Recall</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-4-F-1-Score"><span class="nav-number">9.2.0.4.</span> <span class="nav-text">1.4 F-1 Score</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-5-ROC与AUC"><span class="nav-number">9.2.0.5.</span> <span class="nav-text">1.5 ROC与AUC</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2、AUC的计算"><span class="nav-number">9.3.</span> <span class="nav-text">2、AUC的计算</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#2-1-积分思维"><span class="nav-number">9.3.0.1.</span> <span class="nav-text">2.1 积分思维</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-2-Wilcoxon-Mann-Witney-Test"><span class="nav-number">9.3.0.2.</span> <span class="nav-text">2.2 Wilcoxon-Mann-Witney Test</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-3-Wilcoxon-Mann-Witney-Test的化简"><span class="nav-number">9.3.0.3.</span> <span class="nav-text">2.3 Wilcoxon-Mann-Witney Test的化简</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3、AUC计算代码示例"><span class="nav-number">9.4.</span> <span class="nav-text">3、AUC计算代码示例</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#推荐系统遇上深度学习-十-–GBDT-LR融合方案实战"><span class="nav-number">10.</span> <span class="nav-text">推荐系统遇上深度学习(十)–GBDT+LR融合方案实战</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#写在前面的话"><span class="nav-number">10.1.</span> <span class="nav-text">写在前面的话</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1、背景-2"><span class="nav-number">10.2.</span> <span class="nav-text">1、背景</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2、GBDT和LR的融合方案"><span class="nav-number">10.3.</span> <span class="nav-text">2、GBDT和LR的融合方案</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3、GBDT-LR代码实践"><span class="nav-number">10.4.</span> <span class="nav-text">3、GBDT+LR代码实践</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4、反思"><span class="nav-number">10.5.</span> <span class="nav-text">4、反思</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#推荐系统遇上深度学习-十一-–神经协同过滤NCF原理及实战"><span class="nav-number">11.</span> <span class="nav-text">推荐系统遇上深度学习(十一)–神经协同过滤NCF原理及实战</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1、Neural-Collaborative-Filtering"><span class="nav-number">11.1.</span> <span class="nav-text">1、Neural Collaborative Filtering</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-1-背景"><span class="nav-number">11.1.0.1.</span> <span class="nav-text">1.1 背景</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-2-矩阵分解及其缺陷"><span class="nav-number">11.1.0.2.</span> <span class="nav-text">1.2 矩阵分解及其缺陷</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-3-NCF"><span class="nav-number">11.1.0.3.</span> <span class="nav-text">1.3 NCF</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-4-模型实验"><span class="nav-number">11.1.0.4.</span> <span class="nav-text">1.4 模型实验</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2、NCF实战"><span class="nav-number">11.2.</span> <span class="nav-text">2、NCF实战</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#推荐系统遇上深度学习-十二-–推荐系统中的EE问题及基本Bandit算法"><span class="nav-number">12.</span> <span class="nav-text">推荐系统遇上深度学习(十二)–推荐系统中的EE问题及基本Bandit算法</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1、推荐系统中的EE问题"><span class="nav-number">12.1.</span> <span class="nav-text">1、推荐系统中的EE问题</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2、Bandit算法"><span class="nav-number">12.2.</span> <span class="nav-text">2、Bandit算法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3、基础知识"><span class="nav-number">12.3.</span> <span class="nav-text">3、基础知识</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#3-1-累积遗憾"><span class="nav-number">12.3.0.1.</span> <span class="nav-text">3.1 累积遗憾</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-2-Beta分布"><span class="nav-number">12.3.0.2.</span> <span class="nav-text">3.2 Beta分布</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4、经典Bandit算法原理及实现"><span class="nav-number">12.4.</span> <span class="nav-text">4、经典Bandit算法原理及实现</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#4-1-朴素Bandit算法"><span class="nav-number">12.4.0.1.</span> <span class="nav-text">4.1 朴素Bandit算法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-2-Epsilon-Greedy算法"><span class="nav-number">12.4.0.2.</span> <span class="nav-text">4.2 Epsilon-Greedy算法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-3-Thompson-sampling算法"><span class="nav-number">12.4.0.3.</span> <span class="nav-text">4.3 Thompson sampling算法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-4-UCB算法"><span class="nav-number">12.4.0.4.</span> <span class="nav-text">4.4 UCB算法</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5、代码实现"><span class="nav-number">12.5.</span> <span class="nav-text">5、代码实现</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#推荐系统遇上深度学习-十三-–linUCB方法浅析及实现"><span class="nav-number">13.</span> <span class="nav-text">推荐系统遇上深度学习(十三)–linUCB方法浅析及实现</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1、LinUCB浅析"><span class="nav-number">13.1.</span> <span class="nav-text">1、LinUCB浅析</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2、linUCB代码实战"><span class="nav-number">13.2.</span> <span class="nav-text">2、linUCB代码实战</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#推荐系统遇上深度学习-十四-–《DRN-A-Deep-Reinforcement-Learning-Framework-for-News-Recommendation》"><span class="nav-number">14.</span> <span class="nav-text">推荐系统遇上深度学习(十四)–《DRN-A-Deep-Reinforcement-Learning-Framework-for-News-Recommendation》</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1、引言-2"><span class="nav-number">14.1.</span> <span class="nav-text">1、引言</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2、问题定义"><span class="nav-number">14.2.</span> <span class="nav-text">2、问题定义</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3、模型详解"><span class="nav-number">14.3.</span> <span class="nav-text">3、模型详解</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#3-1-模型整体框架"><span class="nav-number">14.3.0.1.</span> <span class="nav-text">3.1 模型整体框架</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-2-特征设计"><span class="nav-number">14.3.0.2.</span> <span class="nav-text">3.2 特征设计</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-3-深度强化学习作推荐"><span class="nav-number">14.3.0.3.</span> <span class="nav-text">3.3 深度强化学习作推荐</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-4-用户活跃度"><span class="nav-number">14.3.0.4.</span> <span class="nav-text">3.4 用户活跃度</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-5探索"><span class="nav-number">14.3.0.5.</span> <span class="nav-text">3.5探索</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4、实验比较"><span class="nav-number">14.4.</span> <span class="nav-text">4、实验比较</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#4-1-数据集"><span class="nav-number">14.4.0.1.</span> <span class="nav-text">4.1 数据集</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-2-评估指标："><span class="nav-number">14.4.0.2.</span> <span class="nav-text">4.2 评估指标：</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-3-实验设定"><span class="nav-number">14.4.0.3.</span> <span class="nav-text">4.3 实验设定</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-4-对比模型"><span class="nav-number">14.4.0.4.</span> <span class="nav-text">4.4 对比模型</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-5-离线实验"><span class="nav-number">14.4.0.5.</span> <span class="nav-text">4.5 离线实验</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-6-在线实验"><span class="nav-number">14.4.0.6.</span> <span class="nav-text">4.6 在线实验</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#推荐系统遇上深度学习-十五-–强化学习在京东推荐中的探索"><span class="nav-number">15.</span> <span class="nav-text">推荐系统遇上深度学习(十五)–强化学习在京东推荐中的探索</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1、引言-3"><span class="nav-number">15.1.</span> <span class="nav-text">1、引言</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2、系统框架"><span class="nav-number">15.2.</span> <span class="nav-text">2、系统框架</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#2-1-问题描述"><span class="nav-number">15.2.0.1.</span> <span class="nav-text">2.1 问题描述</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-2-线上User-Agent交互仿真环境构建"><span class="nav-number">15.2.0.2.</span> <span class="nav-text">2.2 线上User-Agent交互仿真环境构建</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-3-模型结构"><span class="nav-number">15.2.0.3.</span> <span class="nav-text">2.3 模型结构</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3、实验评估"><span class="nav-number">15.3.</span> <span class="nav-text">3、实验评估</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#推荐系统遇上深度学习-十六-–详解推荐系统中的常用评测指标"><span class="nav-number">16.</span> <span class="nav-text">推荐系统遇上深度学习(十六)–详解推荐系统中的常用评测指标</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1、精确率、召回率、F1值"><span class="nav-number">16.1.</span> <span class="nav-text">1、精确率、召回率、F1值</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2、AUC"><span class="nav-number">16.2.</span> <span class="nav-text">2、AUC</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3、Hit-Ratio-HR"><span class="nav-number">16.3.</span> <span class="nav-text">3、Hit Ratio(HR)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4、Mean-Average-Precision-MAP"><span class="nav-number">16.4.</span> <span class="nav-text">4、Mean Average Precision(MAP)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5、Normalized-Discounted-Cummulative-Gain-NDCG"><span class="nav-number">16.5.</span> <span class="nav-text">5、Normalized Discounted Cummulative Gain(NDCG)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#6、Mean-Reciprocal-Rank-MRR"><span class="nav-number">16.6.</span> <span class="nav-text">6、Mean Reciprocal Rank (MRR)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#7、ILS"><span class="nav-number">16.7.</span> <span class="nav-text">7、ILS</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#8、代码实践"><span class="nav-number">16.8.</span> <span class="nav-text">8、代码实践</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#推荐系统遇上深度学习-十七-–探秘阿里之MLR算法浅析及实现"><span class="nav-number">17.</span> <span class="nav-text">推荐系统遇上深度学习(十七)–探秘阿里之MLR算法浅析及实现</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1、算法介绍"><span class="nav-number">17.1.</span> <span class="nav-text">1、算法介绍</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2、算法简单实现"><span class="nav-number">17.2.</span> <span class="nav-text">2、算法简单实现</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#参考文献"><span class="nav-number">17.3.</span> <span class="nav-text">参考文献</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#推荐系统遇上深度学习-十八-–探秘阿里之深度兴趣网络-DIN-浅析及实现"><span class="nav-number">18.</span> <span class="nav-text">推荐系统遇上深度学习(十八)–探秘阿里之深度兴趣网络(DIN)浅析及实现</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1、背景-3"><span class="nav-number">18.1.</span> <span class="nav-text">1、背景</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2、模型设计"><span class="nav-number">18.2.</span> <span class="nav-text">2、模型设计</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3、模型细节"><span class="nav-number">18.3.</span> <span class="nav-text">3、模型细节</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#3-1-评价指标GAUC"><span class="nav-number">18.3.0.1.</span> <span class="nav-text">3.1 评价指标GAUC</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-2-Dice激活函数"><span class="nav-number">18.3.0.2.</span> <span class="nav-text">3.2 Dice激活函数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-3-自适应正则-Adaptive-Regularization"><span class="nav-number">18.3.0.3.</span> <span class="nav-text">3.3 自适应正则 Adaptive Regularization</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4、效果展示"><span class="nav-number">18.4.</span> <span class="nav-text">4、效果展示</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5、实战DIN"><span class="nav-number">18.5.</span> <span class="nav-text">5、实战DIN</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#推荐系统遇上深度学习-十九-–探秘阿里之完整空间多任务模型ESMM"><span class="nav-number">19.</span> <span class="nav-text">推荐系统遇上深度学习(十九)–探秘阿里之完整空间多任务模型ESMM</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1、背景-4"><span class="nav-number">19.1.</span> <span class="nav-text">1、背景</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2、ESMM模型"><span class="nav-number">19.2.</span> <span class="nav-text">2、ESMM模型</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#2-1-模型结构"><span class="nav-number">19.2.0.1.</span> <span class="nav-text">2.1 模型结构</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-2-模型特点"><span class="nav-number">19.2.0.2.</span> <span class="nav-text">2.2 模型特点</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3、实验效果"><span class="nav-number">19.3.</span> <span class="nav-text">3、实验效果</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#3-1-对比模型介绍"><span class="nav-number">19.3.0.1.</span> <span class="nav-text">3.1 对比模型介绍</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-2-公开数据集实验"><span class="nav-number">19.3.0.2.</span> <span class="nav-text">3.2 公开数据集实验</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-3-淘宝数据集实验"><span class="nav-number">19.3.0.3.</span> <span class="nav-text">3.3 淘宝数据集实验</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4、总结"><span class="nav-number">19.4.</span> <span class="nav-text">4、总结</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#推荐系统遇上深度学习-二十-–贝叶斯个性化排序-BPR-算法原理及实战"><span class="nav-number">20.</span> <span class="nav-text">推荐系统遇上深度学习(二十)–贝叶斯个性化排序(BPR)算法原理及实战</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1、BPR算法简介"><span class="nav-number">20.1.</span> <span class="nav-text">1、BPR算法简介</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-1-基本思路"><span class="nav-number">20.1.0.1.</span> <span class="nav-text">1.1 基本思路</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-2-算法运算思路"><span class="nav-number">20.1.0.2.</span> <span class="nav-text">1.2 算法运算思路</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2、算法实现"><span class="nav-number">20.2.</span> <span class="nav-text">2、算法实现</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3、总结"><span class="nav-number">20.3.</span> <span class="nav-text">3、总结</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#推荐系统遇上深度学习-二十一-–阶段性回顾"><span class="nav-number">21.</span> <span class="nav-text">推荐系统遇上深度学习(二十一)–阶段性回顾</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1、推荐系统中常用评测指标"><span class="nav-number">21.1.</span> <span class="nav-text">1、推荐系统中常用评测指标</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-1精确率、召回率、F1值"><span class="nav-number">21.1.0.1.</span> <span class="nav-text">1.1精确率、召回率、F1值</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-2-AUC"><span class="nav-number">21.1.0.2.</span> <span class="nav-text">1.2 AUC</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-3-Hit-Ratio-HR"><span class="nav-number">21.1.0.3.</span> <span class="nav-text">1.3 Hit Ratio(HR)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-4-Mean-Average-Precision-MAP"><span class="nav-number">21.1.0.4.</span> <span class="nav-text">1.4 Mean Average Precision(MAP)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-5-Normalized-Discounted-Cummulative-Gain-NDCG"><span class="nav-number">21.1.0.5.</span> <span class="nav-text">1.5 Normalized Discounted Cummulative Gain(NDCG)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-6-Mean-Reciprocal-Rank-MRR"><span class="nav-number">21.1.0.6.</span> <span class="nav-text">1.6 Mean Reciprocal Rank (MRR)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-7-ILS"><span class="nav-number">21.1.0.7.</span> <span class="nav-text">1.7 ILS</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3、传统方法"><span class="nav-number">21.2.</span> <span class="nav-text">3、传统方法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#3-1-线性模型"><span class="nav-number">21.2.0.1.</span> <span class="nav-text">3.1 线性模型</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-2-FM模型"><span class="nav-number">21.2.0.2.</span> <span class="nav-text">3.2 FM模型</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-3-FFM模型"><span class="nav-number">21.2.0.3.</span> <span class="nav-text">3.3 FFM模型</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-4-GBDT-LR模型"><span class="nav-number">21.2.0.4.</span> <span class="nav-text">3.4 GBDT+LR模型</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4、深度学习方法"><span class="nav-number">21.3.</span> <span class="nav-text">4、深度学习方法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#4-1-并行结构"><span class="nav-number">21.3.0.1.</span> <span class="nav-text">4.1 并行结构</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-1-1-Wide-amp-Deep模型"><span class="nav-number">21.3.0.2.</span> <span class="nav-text">4.1.1 Wide & Deep模型</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-1-2-DeepFM模型"><span class="nav-number">21.3.0.3.</span> <span class="nav-text">4.1.2 DeepFM模型</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-1-3-Deep-Cross-Network"><span class="nav-number">21.3.0.4.</span> <span class="nav-text">4.1.3 Deep Cross Network</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-2-串行结构"><span class="nav-number">21.3.0.5.</span> <span class="nav-text">4.2 串行结构</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#4-2-1-Product-based-Neural-Network"><span class="nav-number">21.3.0.5.1.</span> <span class="nav-text">4.2.1 Product-based Neural Network</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#4-2-2-Neural-factorization-machines"><span class="nav-number">21.3.0.5.2.</span> <span class="nav-text">4.2.2 Neural factorization machines</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#4-2-3-Attention-Based-factorization-machines"><span class="nav-number">21.3.0.5.3.</span> <span class="nav-text">4.2.3 Attention-Based factorization machines</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5、强化学习方法"><span class="nav-number">21.4.</span> <span class="nav-text">5、强化学习方法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#6、推荐系统的EE问题"><span class="nav-number">21.5.</span> <span class="nav-text">6、推荐系统的EE问题</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#6-1-Bandit算法"><span class="nav-number">21.5.0.1.</span> <span class="nav-text">6.1 Bandit算法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#6-2-LinUCB算法"><span class="nav-number">21.5.0.2.</span> <span class="nav-text">6.2 LinUCB算法</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#7、推荐系统在公司中的实战"><span class="nav-number">21.6.</span> <span class="nav-text">7、推荐系统在公司中的实战</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#7-1-阿里MLR算法"><span class="nav-number">21.6.0.1.</span> <span class="nav-text">7.1 阿里MLR算法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#7-2-阿里Deep-Interest-Network"><span class="nav-number">21.6.0.2.</span> <span class="nav-text">7.2 阿里Deep Interest Network</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#7-3-阿里ESSM模型"><span class="nav-number">21.6.0.3.</span> <span class="nav-text">7.3 阿里ESSM模型</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#7-4-京东强化学习推荐模型"><span class="nav-number">21.6.0.4.</span> <span class="nav-text">7.4 京东强化学习推荐模型</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#9、论文整理"><span class="nav-number">21.7.</span> <span class="nav-text">9、论文整理</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#推荐系统遇上深度学习-二十二-–DeepFM升级版XDeepFM模型强势来袭！"><span class="nav-number">22.</span> <span class="nav-text">推荐系统遇上深度学习(二十二)–DeepFM升级版XDeepFM模型强势来袭！</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1、引言-4"><span class="nav-number">22.1.</span> <span class="nav-text">1、引言</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2、xDeepFM模型介绍"><span class="nav-number">22.2.</span> <span class="nav-text">2、xDeepFM模型介绍</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#2-1-Compressed-Interaction-Network"><span class="nav-number">22.2.0.1.</span> <span class="nav-text">2.1 Compressed Interaction Network</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-2-xDeepFM"><span class="nav-number">22.2.0.2.</span> <span class="nav-text">2.2 xDeepFM</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3、Tensorflow充电"><span class="nav-number">22.3.</span> <span class="nav-text">3、Tensorflow充电</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4、XDeepFM的TF实现"><span class="nav-number">22.4.</span> <span class="nav-text">4、XDeepFM的TF实现</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5、总结"><span class="nav-number">22.5.</span> <span class="nav-text">5、总结</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#参考文献-1"><span class="nav-number">22.6.</span> <span class="nav-text">参考文献</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#推荐系统遇上深度学习-二十三-–大一统信息检索模型IRGAN在推荐领域的应用"><span class="nav-number">23.</span> <span class="nav-text">推荐系统遇上深度学习(二十三)–大一统信息检索模型IRGAN在推荐领域的应用</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1、引言-5"><span class="nav-number">23.1.</span> <span class="nav-text">1、引言</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2、IRGAN介绍"><span class="nav-number">23.2.</span> <span class="nav-text">2、IRGAN介绍</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#定义问题"><span class="nav-number">23.2.0.1.</span> <span class="nav-text">定义问题</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#优化判别模型D"><span class="nav-number">23.2.0.2.</span> <span class="nav-text">优化判别模型D</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#优化生成模型G"><span class="nav-number">23.2.0.3.</span> <span class="nav-text">优化生成模型G</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#总体流程"><span class="nav-number">23.2.0.4.</span> <span class="nav-text">总体流程</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Pair-wise的情况"><span class="nav-number">23.2.0.5.</span> <span class="nav-text">Pair-wise的情况</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3、IRGAN的TF实现"><span class="nav-number">23.3.</span> <span class="nav-text">3、IRGAN的TF实现</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#数据"><span class="nav-number">23.3.0.1.</span> <span class="nav-text">数据</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Discriminator"><span class="nav-number">23.3.0.2.</span> <span class="nav-text">Discriminator</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#模型训练"><span class="nav-number">23.3.0.3.</span> <span class="nav-text">模型训练</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#参考文献："><span class="nav-number">23.4.</span> <span class="nav-text">参考文献：</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#推荐系统遇上深度学习-二十四-–深度兴趣进化网络DIEN原理及实战！"><span class="nav-number">24.</span> <span class="nav-text">推荐系统遇上深度学习(二十四)–深度兴趣进化网络DIEN原理及实战！</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1、背景-5"><span class="nav-number">24.1.</span> <span class="nav-text">1、背景</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2、DIEN模型原理"><span class="nav-number">24.2.</span> <span class="nav-text">2、DIEN模型原理</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#2-1-模型总体结构"><span class="nav-number">24.2.0.1.</span> <span class="nav-text">2.1 模型总体结构</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-2-兴趣抽取层Interest-Extractor-Layer"><span class="nav-number">24.2.0.2.</span> <span class="nav-text">2.2 兴趣抽取层Interest Extractor Layer</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-3-兴趣进化层Interest-Evolution-Layer"><span class="nav-number">24.2.0.3.</span> <span class="nav-text">2.3 兴趣进化层Interest Evolution Layer</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-4-模型试验"><span class="nav-number">24.2.0.4.</span> <span class="nav-text">2.4 模型试验</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3、DIEN模型实现"><span class="nav-number">24.3.</span> <span class="nav-text">3、DIEN模型实现</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#输入数据介绍"><span class="nav-number">24.3.0.1.</span> <span class="nav-text">输入数据介绍</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#输入数据转换为对应的embedding"><span class="nav-number">24.3.0.2.</span> <span class="nav-text">输入数据转换为对应的embedding</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#第一层GRU"><span class="nav-number">24.3.0.3.</span> <span class="nav-text">第一层GRU</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#AUGRU"><span class="nav-number">24.3.0.4.</span> <span class="nav-text">AUGRU</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#参考文献-2"><span class="nav-number">24.4.</span> <span class="nav-text">参考文献</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#推荐系统遇上深度学习-二十五-–当知识图谱遇上个性化推荐"><span class="nav-number">25.</span> <span class="nav-text">推荐系统遇上深度学习(二十五)–当知识图谱遇上个性化推荐</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1、推荐系统的任务和难点"><span class="nav-number">25.1.</span> <span class="nav-text">1、推荐系统的任务和难点</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2、什么是知识图谱"><span class="nav-number">25.2.</span> <span class="nav-text">2、什么是知识图谱</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3、知识图谱的优势"><span class="nav-number">25.3.</span> <span class="nav-text">3、知识图谱的优势</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4、知识图谱与推荐系统相结合的方法"><span class="nav-number">25.4.</span> <span class="nav-text">4、知识图谱与推荐系统相结合的方法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#4-1-基于特征的推荐方法"><span class="nav-number">25.4.0.1.</span> <span class="nav-text">4.1 基于特征的推荐方法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-2-基于路径的推荐方法"><span class="nav-number">25.4.0.2.</span> <span class="nav-text">4.2 基于路径的推荐方法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-3-知识图谱特征学习Knowledge-Graph-Embedding"><span class="nav-number">25.4.0.3.</span> <span class="nav-text">4.3 知识图谱特征学习Knowledge Graph Embedding</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#基于距离的翻译模型（distance-based-translational-models）"><span class="nav-number">25.4.0.3.1.</span> <span class="nav-text">基于距离的翻译模型（distance-based translational models）</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#基于语义的匹配模型（semantic-based-matching-models）"><span class="nav-number">25.4.0.3.2.</span> <span class="nav-text">基于语义的匹配模型（semantic-based matching models）</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#结合知识图谱特征学习的推荐系统"><span class="nav-number">25.4.0.3.3.</span> <span class="nav-text">结合知识图谱特征学习的推荐系统</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#参考文献-3"><span class="nav-number">25.5.</span> <span class="nav-text">参考文献</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#推荐系统遇上深度学习-二十六-–知识图谱与推荐系统结合之DKN模型原理及实现"><span class="nav-number">26.</span> <span class="nav-text">推荐系统遇上深度学习(二十六)–知识图谱与推荐系统结合之DKN模型原理及实现</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1、DKN原理"><span class="nav-number">26.1.</span> <span class="nav-text">1、DKN原理</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-1-背景-1"><span class="nav-number">26.1.0.1.</span> <span class="nav-text">1.1 背景</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-2-基础概念"><span class="nav-number">26.1.0.2.</span> <span class="nav-text">1.2 基础概念</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#1-2-1-知识图谱特征学习（Knowledge-Graph-Embedding）"><span class="nav-number">26.1.0.2.1.</span> <span class="nav-text">1.2.1 知识图谱特征学习（Knowledge Graph Embedding）</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#1-2-2-基于CNN的句子特征提取"><span class="nav-number">26.1.0.2.2.</span> <span class="nav-text">1.2.2 基于CNN的句子特征提取</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-3-问题定义"><span class="nav-number">26.1.0.3.</span> <span class="nav-text">1.3 问题定义</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-4-模型框架"><span class="nav-number">26.1.0.4.</span> <span class="nav-text">1.4 模型框架</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#1-4-1-知识提取（Knowledge-Distillation）"><span class="nav-number">26.1.0.4.1.</span> <span class="nav-text">1.4.1 知识提取（Knowledge Distillation）</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#1-4-2-新闻特征提取KCNN-Knowledge-aware-CNN"><span class="nav-number">26.1.0.4.2.</span> <span class="nav-text">1.4.2 新闻特征提取KCNN(Knowledge-aware CNN)</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#1-4-3-基于注意力机制的用户兴趣预测"><span class="nav-number">26.1.0.4.3.</span> <span class="nav-text">1.4.3  基于注意力机制的用户兴趣预测</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-5-实验结果"><span class="nav-number">26.1.0.5.</span> <span class="nav-text">1.5 实验结果</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2、DKN模型tensorflow实现"><span class="nav-number">26.2.</span> <span class="nav-text">2、DKN模型tensorflow实现</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#推荐系统遇上深度学习-二十七-–知识图谱与推荐系统结合之RippleNet模型原理及实现"><span class="nav-number">27.</span> <span class="nav-text">推荐系统遇上深度学习(二十七)–知识图谱与推荐系统结合之RippleNet模型原理及实现</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1、RippleNet原理"><span class="nav-number">27.1.</span> <span class="nav-text">1、RippleNet原理</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-1-RippleNet背景"><span class="nav-number">27.1.0.1.</span> <span class="nav-text">1.1 RippleNet背景</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-2-RippleNet网络结构"><span class="nav-number">27.1.0.2.</span> <span class="nav-text">1.2 RippleNet网络结构</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-3-RippleNet损失函数"><span class="nav-number">27.1.0.3.</span> <span class="nav-text">1.3 RippleNet损失函数</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2、RippleNet的Tensorflow实现"><span class="nav-number">27.2.</span> <span class="nav-text">2、RippleNet的Tensorflow实现</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#参考文献：-1"><span class="nav-number">27.3.</span> <span class="nav-text">参考文献：</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#推荐系统遇上深度学习-二十八-–知识图谱与推荐系统结合之MKR模型原理及实现"><span class="nav-number">28.</span> <span class="nav-text">推荐系统遇上深度学习(二十八)–知识图谱与推荐系统结合之MKR模型原理及实现</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1、MKR原理介绍"><span class="nav-number">28.1.</span> <span class="nav-text">1、MKR原理介绍</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2、MKR模型tensorflow实现"><span class="nav-number">28.2.</span> <span class="nav-text">2、MKR模型tensorflow实现</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#参考文献-4"><span class="nav-number">28.3.</span> <span class="nav-text">参考文献</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#推荐系统遇上深度学习-二十九-–协同记忆网络理论及实践"><span class="nav-number">29.</span> <span class="nav-text">推荐系统遇上深度学习(二十九)–协同记忆网络理论及实践</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1、协同过滤介绍"><span class="nav-number">29.1.</span> <span class="nav-text">1、协同过滤介绍</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2、记忆网络Memory-Network简介"><span class="nav-number">29.2.</span> <span class="nav-text">2、记忆网络Memory Network简介</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#2-1-Basic-Memory-Network"><span class="nav-number">29.2.0.1.</span> <span class="nav-text">2.1 Basic Memory Network</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-2-End-to-End-Memory-Network"><span class="nav-number">29.2.0.2.</span> <span class="nav-text">2.2 End to End Memory Network</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3、协同记忆网络原理"><span class="nav-number">29.3.</span> <span class="nav-text">3、协同记忆网络原理</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#3-1-User-Embedding"><span class="nav-number">29.3.0.1.</span> <span class="nav-text">3.1 User Embedding</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-2-Neighborhood-Attention"><span class="nav-number">29.3.0.2.</span> <span class="nav-text">3.2 Neighborhood Attention</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-3-Output-Module"><span class="nav-number">29.3.0.3.</span> <span class="nav-text">3.3 Output Module</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-4-Multiple-Hops"><span class="nav-number">29.3.0.4.</span> <span class="nav-text">3.4 Multiple Hops</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-5-Parameter-Estimation"><span class="nav-number">29.3.0.5.</span> <span class="nav-text">3.5 Parameter Estimation</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4、CMN模型的tensorflow实现"><span class="nav-number">29.4.</span> <span class="nav-text">4、CMN模型的tensorflow实现</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#参考文献-5"><span class="nav-number">29.5.</span> <span class="nav-text">参考文献</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#推荐系统遇上深度学习-三十-–深度矩阵分解模型理论及实践"><span class="nav-number">30.</span> <span class="nav-text">推荐系统遇上深度学习(三十)–深度矩阵分解模型理论及实践</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1、DMF原理介绍"><span class="nav-number">30.1.</span> <span class="nav-text">1、DMF原理介绍</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-1-问题陈述"><span class="nav-number">30.1.0.1.</span> <span class="nav-text">1.1 问题陈述</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-2-DMF模型"><span class="nav-number">30.1.0.2.</span> <span class="nav-text">2.2 DMF模型</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-3-损失函数设计"><span class="nav-number">30.1.0.3.</span> <span class="nav-text">2.3 损失函数设计</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2、DMF代码实现"><span class="nav-number">30.2.</span> <span class="nav-text">2、DMF代码实现</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#推荐系统遇上深度学习-三十一-–使用自注意力机制进行物品推荐"><span class="nav-number">31.</span> <span class="nav-text">推荐系统遇上深度学习(三十一)–使用自注意力机制进行物品推荐</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1、为什么要用自注意力机制？"><span class="nav-number">31.1.</span> <span class="nav-text">1、为什么要用自注意力机制？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2、模型原理"><span class="nav-number">31.2.</span> <span class="nav-text">2、模型原理</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#2-1-问题定义"><span class="nav-number">31.2.0.1.</span> <span class="nav-text">2.1 问题定义</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-2-使用自注意力机制建模短期兴趣"><span class="nav-number">31.2.0.2.</span> <span class="nav-text">2.2 使用自注意力机制建模短期兴趣</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-3-长期兴趣建模"><span class="nav-number">31.2.0.3.</span> <span class="nav-text">2.3 长期兴趣建模</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-4-模型训练"><span class="nav-number">31.2.0.4.</span> <span class="nav-text">2.4 模型训练</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#推荐系统遇上深度学习-三十二-–《推荐系统实践》思维导图"><span class="nav-number">32.</span> <span class="nav-text">推荐系统遇上深度学习(三十二)–《推荐系统实践》思维导图</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#推荐系统遇上深度学习-三十三-–Neural-Attentive-Item-Similarity-Model"><span class="nav-number">33.</span> <span class="nav-text">推荐系统遇上深度学习(三十三)–Neural-Attentive-Item-Similarity-Model</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1、ItemCF问题简介"><span class="nav-number">33.1.</span> <span class="nav-text">1、ItemCF问题简介</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-1-标准ItemCF问题"><span class="nav-number">33.1.0.1.</span> <span class="nav-text">1.1 标准ItemCF问题</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-2-Learning-based-Methods-for-Item-based-CF"><span class="nav-number">33.1.0.2.</span> <span class="nav-text">1.2 Learning-based Methods for Item-based CF</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2、NASI模型介绍"><span class="nav-number">33.2.</span> <span class="nav-text">2、NASI模型介绍</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#2-1-第一版"><span class="nav-number">33.2.0.1.</span> <span class="nav-text">2.1 第一版</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-2-第二版"><span class="nav-number">33.2.0.2.</span> <span class="nav-text">2.2 第二版</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-3-第三版"><span class="nav-number">33.2.0.3.</span> <span class="nav-text">2.3 第三版</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-4-最终版"><span class="nav-number">33.2.0.4.</span> <span class="nav-text">2.4 最终版</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3、NASI代码实现"><span class="nav-number">33.3.</span> <span class="nav-text">3、NASI代码实现</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#推荐系统遇上深度学习-三十四-–YouTube深度学习推荐系统"><span class="nav-number">34.</span> <span class="nav-text">推荐系统遇上深度学习(三十四)–YouTube深度学习推荐系统</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1、引言-6"><span class="nav-number">34.1.</span> <span class="nav-text">1、引言</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2、Youtube推荐系统-整体架构"><span class="nav-number">34.2.</span> <span class="nav-text">2、Youtube推荐系统 整体架构</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3、候选集生成Candidate-Generation"><span class="nav-number">34.3.</span> <span class="nav-text">3、候选集生成Candidate Generation</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#3-1-输入特征"><span class="nav-number">34.3.0.1.</span> <span class="nav-text">3.1 输入特征</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-2-样本和上下文选择"><span class="nav-number">34.3.0.2.</span> <span class="nav-text">3.2 样本和上下文选择</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-3-离线训练"><span class="nav-number">34.3.0.3.</span> <span class="nav-text">3.3 离线训练</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-4-在线服务"><span class="nav-number">34.3.0.4.</span> <span class="nav-text">3.4 在线服务</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4、排序Ranking"><span class="nav-number">34.4.</span> <span class="nav-text">4、排序Ranking</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#4-1-输入特征"><span class="nav-number">34.4.0.1.</span> <span class="nav-text">4.1 输入特征</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-2-特征处理"><span class="nav-number">34.4.0.2.</span> <span class="nav-text">4.2 特征处理</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-3-建模期望观看时间"><span class="nav-number">34.4.0.3.</span> <span class="nav-text">4.3 建模期望观看时间</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5、总结-1"><span class="nav-number">34.5.</span> <span class="nav-text">5、总结</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#推荐阅读"><span class="nav-number">34.6.</span> <span class="nav-text">推荐阅读</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#推荐系统遇上深度学习-三十五-–强化学习在京东推荐中的探索-二"><span class="nav-number">35.</span> <span class="nav-text">推荐系统遇上深度学习(三十五)–强化学习在京东推荐中的探索(二)</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1、背景-6"><span class="nav-number">35.1.</span> <span class="nav-text">1、背景</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2、问题陈述"><span class="nav-number">35.2.</span> <span class="nav-text">2、问题陈述</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3、基本模型框架"><span class="nav-number">35.3.</span> <span class="nav-text">3、基本模型框架</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4、考虑负反馈以及偏序关系的强化学习推荐框架"><span class="nav-number">35.4.</span> <span class="nav-text">4、考虑负反馈以及偏序关系的强化学习推荐框架</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#推荐系统遇上深度学习-三十六-–Learning-and-Transferring-IDs-Representation-in-E-commerce"><span class="nav-number">36.</span> <span class="nav-text">推荐系统遇上深度学习(三十六)–Learning-and-Transferring-IDs-Representation-in-E-commerce</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1、背景-7"><span class="nav-number">36.1.</span> <span class="nav-text">1、背景</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2、学习ID的表征方式"><span class="nav-number">36.2.</span> <span class="nav-text">2、学习ID的表征方式</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#2-1-Skip-gram-on-User’s-Interactive-Sequences"><span class="nav-number">36.2.0.1.</span> <span class="nav-text">2.1 Skip-gram on User’s Interactive Sequences</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-2-Log-uniform-Negative-sampling"><span class="nav-number">36.2.0.2.</span> <span class="nav-text">2.2 Log-uniform Negative-sampling</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-3-IDs-and-Their-Structural-Connections"><span class="nav-number">36.2.0.3.</span> <span class="nav-text">2.3 IDs and Their Structural Connections</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-4-Jointly-Embedding-Attribute-IDs"><span class="nav-number">36.2.0.4.</span> <span class="nav-text">2.4 Jointly Embedding Attribute IDs</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-5-Embedding-User-IDs"><span class="nav-number">36.2.0.5.</span> <span class="nav-text">2.5 Embedding User IDs</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-6-Model-Learning"><span class="nav-number">36.2.0.6.</span> <span class="nav-text">2.6 Model Learning</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3、应用"><span class="nav-number">36.3.</span> <span class="nav-text">3、应用</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#3-2-Transferring-from-Seen-Items-to-Unseen-Items"><span class="nav-number">36.3.0.1.</span> <span class="nav-text">3.2 Transferring from Seen Items to Unseen Items</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-3-Transferring-across-Different-Domains"><span class="nav-number">36.3.0.2.</span> <span class="nav-text">3.3 Transferring across Different Domains</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-4-Transferring-across-Different-Tasks"><span class="nav-number">36.3.0.3.</span> <span class="nav-text">3.4 Transferring across Different Tasks</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4、总结-1"><span class="nav-number">36.4.</span> <span class="nav-text">4、总结</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#推荐系统遇上深度学习-三十七-–基于多任务学习的可解释性推荐系统"><span class="nav-number">37.</span> <span class="nav-text">推荐系统遇上深度学习(三十七)–基于多任务学习的可解释性推荐系统</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1、整体框架"><span class="nav-number">37.1.</span> <span class="nav-text">1、整体框架</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2、对抗式Seq2Seq模型"><span class="nav-number">37.2.</span> <span class="nav-text">2、对抗式Seq2Seq模型</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#2-1-Recurrent-Review-Generator"><span class="nav-number">37.2.0.1.</span> <span class="nav-text">2.1 Recurrent Review Generator</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-2-Convolutional-Review-Discriminator"><span class="nav-number">37.2.0.2.</span> <span class="nav-text">2.2 Convolutional Review Discriminator</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-3-Adversarial-Training-for-Review-Generation-with-REINFORCE"><span class="nav-number">37.2.0.3.</span> <span class="nav-text">2.3 Adversarial Training for Review Generation with REINFORCE</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-4-扩展"><span class="nav-number">37.2.0.4.</span> <span class="nav-text">2.4 扩展</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3、上下文感知的PMF模型"><span class="nav-number">37.3.</span> <span class="nav-text">3、上下文感知的PMF模型</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4、模型训练流程"><span class="nav-number">37.4.</span> <span class="nav-text">4、模型训练流程</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5、实验结论"><span class="nav-number">37.5.</span> <span class="nav-text">5、实验结论</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#6、总结"><span class="nav-number">37.6.</span> <span class="nav-text">6、总结</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#参考文献-6"><span class="nav-number">37.7.</span> <span class="nav-text">参考文献</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#推荐系统遇上深度学习-三十八-–CFGAN-一种基于GAN的协同过滤推荐框架"><span class="nav-number">38.</span> <span class="nav-text">推荐系统遇上深度学习(三十八)–CFGAN-一种基于GAN的协同过滤推荐框架</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1、背景-8"><span class="nav-number">38.1.</span> <span class="nav-text">1、背景</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2、模型框架"><span class="nav-number">38.2.</span> <span class="nav-text">2、模型框架</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#2-1-整体框架"><span class="nav-number">38.2.0.1.</span> <span class="nav-text">2.1 整体框架</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-2-框架改进"><span class="nav-number">38.2.0.2.</span> <span class="nav-text">2.2 框架改进</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-3-迁移到物品角度"><span class="nav-number">38.2.0.3.</span> <span class="nav-text">2.3 迁移到物品角度</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3、模型实验"><span class="nav-number">38.3.</span> <span class="nav-text">3、模型实验</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#3-1-模型的有效性"><span class="nav-number">38.3.0.1.</span> <span class="nav-text">3.1 模型的有效性</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-2-超参数设置对于模型的影响"><span class="nav-number">38.3.0.2.</span> <span class="nav-text">3.2 超参数设置对于模型的影响</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-3-与目前主流推荐方法的比较"><span class="nav-number">38.3.0.3.</span> <span class="nav-text">3.3 与目前主流推荐方法的比较</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4、总结-2"><span class="nav-number">38.4.</span> <span class="nav-text">4、总结</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#推荐系统遇上深度学习-三十九-推荐系统中召回策略演进！"><span class="nav-number">39.</span> <span class="nav-text">推荐系统遇上深度学习(三十九)-推荐系统中召回策略演进！</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1、协同过滤"><span class="nav-number">39.1.</span> <span class="nav-text">1、协同过滤</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-1-基于用户的协同过滤"><span class="nav-number">39.1.0.1.</span> <span class="nav-text">1.1 基于用户的协同过滤</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-2-基于物品的协同过滤"><span class="nav-number">39.1.0.2.</span> <span class="nav-text">1.2 基于物品的协同过滤</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-3-UserCF和ItemCF的比较"><span class="nav-number">39.1.0.3.</span> <span class="nav-text">1.3 UserCF和ItemCF的比较</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-4-协同过滤总结"><span class="nav-number">39.1.0.4.</span> <span class="nav-text">1.4 协同过滤总结</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2、向量化召回"><span class="nav-number">39.2.</span> <span class="nav-text">2、向量化召回</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#2-1-Youtube召回模型"><span class="nav-number">39.2.0.1.</span> <span class="nav-text">2.1 Youtube召回模型</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-2-局部敏感哈希-Locality-Sensitive-Hashing-LSH"><span class="nav-number">39.2.0.2.</span> <span class="nav-text">2.2 局部敏感哈希(Locality-Sensitive Hashing, LSH)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-3-向量化召回评价"><span class="nav-number">39.2.0.3.</span> <span class="nav-text">2.3 向量化召回评价</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3、深度树匹配"><span class="nav-number">39.3.</span> <span class="nav-text">3、深度树匹配</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#3-1-怎么基于树来实现高效的检索"><span class="nav-number">39.3.0.1.</span> <span class="nav-text">3.1 怎么基于树来实现高效的检索</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-2-怎么在树上面做兴趣建模"><span class="nav-number">39.3.0.2.</span> <span class="nav-text">3.2 怎么在树上面做兴趣建模</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-3-兴趣树是怎么构建的"><span class="nav-number">39.3.0.3.</span> <span class="nav-text">3.3 兴趣树是怎么构建的</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4、总结-3"><span class="nav-number">39.4.</span> <span class="nav-text">4、总结</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#参考文献-7"><span class="nav-number">39.5.</span> <span class="nav-text">参考文献</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#推荐系统遇上深度学习-四十-SESSION-BASED-RECOMMENDATIONS-WITH-RECURRENT-NEURAL-NETWORKS"><span class="nav-number">40.</span> <span class="nav-text">推荐系统遇上深度学习(四十)-SESSION-BASED-RECOMMENDATIONS-WITH-RECURRENT-NEURAL-NETWORKS</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1、模型介绍"><span class="nav-number">40.1.</span> <span class="nav-text">1、模型介绍</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-1-背景介绍"><span class="nav-number">40.1.0.1.</span> <span class="nav-text">1.1 背景介绍</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-2-基于RNN的会话推荐"><span class="nav-number">40.1.0.2.</span> <span class="nav-text">1.2 基于RNN的会话推荐</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-3-模型的小trick"><span class="nav-number">40.1.0.3.</span> <span class="nav-text">1.3 模型的小trick</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-4-Rank-Loss"><span class="nav-number">40.1.0.4.</span> <span class="nav-text">1.4 Rank Loss</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-6-关于损失函数的讨论"><span class="nav-number">40.1.0.5.</span> <span class="nav-text">1.6 关于损失函数的讨论</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#推荐系统遇上深度学习-四十一-Improved-Recurrent-Neural-Networks-for-Session-based-Recommendations"><span class="nav-number">41.</span> <span class="nav-text">推荐系统遇上深度学习(四十一)-Improved-Recurrent-Neural-Networks-for-Session-based-Recommendations</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1、基础模型"><span class="nav-number">41.1.</span> <span class="nav-text">1、基础模型</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2、模型改进"><span class="nav-number">41.2.</span> <span class="nav-text">2、模型改进</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#2-1-Data-augmentation"><span class="nav-number">41.2.0.1.</span> <span class="nav-text">2.1 Data augmentation</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-2-Adapting-to-temporal-changes"><span class="nav-number">41.2.0.2.</span> <span class="nav-text">2.2 Adapting to temporal changes</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-3-Use-of-privileged-information"><span class="nav-number">41.2.0.3.</span> <span class="nav-text">2.3 Use of privileged information</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-4-Output-embeddings-for-faster-predictions"><span class="nav-number">41.2.0.4.</span> <span class="nav-text">2.4 Output embeddings for faster predictions</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3、实验效果及结论"><span class="nav-number">41.3.</span> <span class="nav-text">3、实验效果及结论</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#推荐系统遇上深度学习-四十二-使用图神经网络做基于会话的推荐"><span class="nav-number">42.</span> <span class="nav-text">推荐系统遇上深度学习(四十二)-使用图神经网络做基于会话的推荐</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1、背景介绍"><span class="nav-number">42.1.</span> <span class="nav-text">1、背景介绍</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2、模型介绍"><span class="nav-number">42.2.</span> <span class="nav-text">2、模型介绍</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#2-1-符号定义"><span class="nav-number">42.2.0.1.</span> <span class="nav-text">2.1 符号定义</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-2-子图构建"><span class="nav-number">42.2.0.2.</span> <span class="nav-text">2.2 子图构建</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-3-基于Graph学习物品嵌入向量"><span class="nav-number">42.2.0.3.</span> <span class="nav-text">2.3 基于Graph学习物品嵌入向量</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-4-生成Session对应的嵌入向量"><span class="nav-number">42.2.0.4.</span> <span class="nav-text">2.4 生成Session对应的嵌入向量</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-5-给出推荐结果及模型训练"><span class="nav-number">42.2.0.5.</span> <span class="nav-text">2.5 给出推荐结果及模型训练</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3、一点小疑问"><span class="nav-number">42.3.</span> <span class="nav-text">3、一点小疑问</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4、总结-4"><span class="nav-number">42.4.</span> <span class="nav-text">4、总结</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#推荐系统遇上深度学习-四十三-考虑用户微观行为的电商推荐"><span class="nav-number">43.</span> <span class="nav-text">推荐系统遇上深度学习(四十三)-考虑用户微观行为的电商推荐</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1、问题定义"><span class="nav-number">43.1.</span> <span class="nav-text">1、问题定义</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2、数据分析"><span class="nav-number">43.2.</span> <span class="nav-text">2、数据分析</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3、推荐模型"><span class="nav-number">43.3.</span> <span class="nav-text">3、推荐模型</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#推荐系统遇上深度学习-四十四-Airbnb实时搜索排序中的Embedding技巧"><span class="nav-number">44.</span> <span class="nav-text">推荐系统遇上深度学习(四十四)-Airbnb实时搜索排序中的Embedding技巧</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1、背景-9"><span class="nav-number">44.1.</span> <span class="nav-text">1、背景</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2、Listing-Embedding"><span class="nav-number">44.2.</span> <span class="nav-text">2、Listing Embedding</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3、User-Type-amp-Listing-Type-Embedding"><span class="nav-number">44.3.</span> <span class="nav-text">3、User Type & Listing Type Embedding</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4、总结-5"><span class="nav-number">44.4.</span> <span class="nav-text">4、总结</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#参考文献：-2"><span class="nav-number">44.5.</span> <span class="nav-text">参考文献：</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#推荐系统遇上深度学习-四十五-探秘阿里之深度会话兴趣网络DSIN"><span class="nav-number">45.</span> <span class="nav-text">推荐系统遇上深度学习(四十五)-探秘阿里之深度会话兴趣网络DSIN</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1、背景-10"><span class="nav-number">45.1.</span> <span class="nav-text">1、背景</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2、模型结构"><span class="nav-number">45.2.</span> <span class="nav-text">2、模型结构</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#2-1-Base-Model"><span class="nav-number">45.2.0.1.</span> <span class="nav-text">2.1 Base Model</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-2-DSIN"><span class="nav-number">45.2.0.2.</span> <span class="nav-text">2.2 DSIN</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#2-2-1-Session-Division-Layer"><span class="nav-number">45.2.0.2.1.</span> <span class="nav-text">2.2.1 Session Division Layer</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#2-2-2-Session-Interest-Extractor-Layer"><span class="nav-number">45.2.0.2.2.</span> <span class="nav-text">2.2.2 Session Interest Extractor Layer</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#2-2-3-Session-Interest-Interacting-Layer"><span class="nav-number">45.2.0.2.3.</span> <span class="nav-text">2.2.3 Session Interest Interacting Layer</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#2-2-4-Session-Interest-Activating-Layer"><span class="nav-number">45.2.0.2.4.</span> <span class="nav-text">2.2.4 Session Interest Activating Layer</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3、模型试验"><span class="nav-number">45.3.</span> <span class="nav-text">3、模型试验</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4、总结讨论"><span class="nav-number">45.4.</span> <span class="nav-text">4、总结讨论</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#4-1-Effect-of-Multiple-Sessions"><span class="nav-number">45.4.0.1.</span> <span class="nav-text">4.1 Effect of Multiple Sessions</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-2-Effect-of-Session-Interest-Interacting-Layer"><span class="nav-number">45.4.0.2.</span> <span class="nav-text">4.2 Effect of Session Interest Interacting Layer</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-3-Effect-of-Bias-Encoding"><span class="nav-number">45.4.0.3.</span> <span class="nav-text">4.3 Effect of Bias Encoding</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#推荐系统遇上深度学习-四十六-阿里电商推荐中亿级商品的embedding策略"><span class="nav-number">46.</span> <span class="nav-text">推荐系统遇上深度学习(四十六)-阿里电商推荐中亿级商品的embedding策略</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1、背景-11"><span class="nav-number">46.1.</span> <span class="nav-text">1、背景</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2、模型介绍-1"><span class="nav-number">46.2.</span> <span class="nav-text">2、模型介绍</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#2-1-Base-Graph-Embedding-BGE"><span class="nav-number">46.2.0.1.</span> <span class="nav-text">2.1 Base Graph Embedding (BGE)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-2-Graph-Embedding-with-Side-information-GES"><span class="nav-number">46.2.0.2.</span> <span class="nav-text">2.2 Graph Embedding with Side information (GES)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-3-Enhanced-Graph-Embedding-with-Side-information-EGES"><span class="nav-number">46.2.0.3.</span> <span class="nav-text">2.3 Enhanced Graph Embedding with Side information (EGES)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-3-GES和EGES的学习"><span class="nav-number">46.2.0.4.</span> <span class="nav-text">2.3 GES和EGES的学习</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3、实验分析"><span class="nav-number">46.3.</span> <span class="nav-text">3、实验分析</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#3-1-实验结果"><span class="nav-number">46.3.0.1.</span> <span class="nav-text">3.1 实验结果</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-2-案例分析"><span class="nav-number">46.3.0.2.</span> <span class="nav-text">3.2 案例分析</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4、对比总结"><span class="nav-number">46.4.</span> <span class="nav-text">4、对比总结</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#推荐系统遇上深度学习-四十七-TEM-基于树模型构建可解释性推荐系统"><span class="nav-number">47.</span> <span class="nav-text">推荐系统遇上深度学习(四十七)-TEM-基于树模型构建可解释性推荐系统</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1、背景-12"><span class="nav-number">47.1.</span> <span class="nav-text">1、背景</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2、模型介绍-2"><span class="nav-number">47.2.</span> <span class="nav-text">2、模型介绍</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#2-1-构建交叉特征"><span class="nav-number">47.2.0.1.</span> <span class="nav-text">2.1 构建交叉特征</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-2-通过交叉特征进行预测"><span class="nav-number">47.2.0.2.</span> <span class="nav-text">2.2 通过交叉特征进行预测</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-3-模型学习"><span class="nav-number">47.2.0.3.</span> <span class="nav-text">2.3 模型学习</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3、可解释性分析"><span class="nav-number">47.3.</span> <span class="nav-text">3、可解释性分析</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4、实验分析"><span class="nav-number">47.4.</span> <span class="nav-text">4、实验分析</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#4-1-性能分析"><span class="nav-number">47.4.0.1.</span> <span class="nav-text">4.1 性能分析</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-2-可解释性案例分析"><span class="nav-number">47.4.0.2.</span> <span class="nav-text">4.2 可解释性案例分析</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-3-超参数学习"><span class="nav-number">47.4.0.3.</span> <span class="nav-text">4.3 超参数学习</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#推荐系统遇上深度学习-四十八-BST-将Transformer用于淘宝电商推荐"><span class="nav-number">48.</span> <span class="nav-text">推荐系统遇上深度学习(四十八)-BST-将Transformer用于淘宝电商推荐</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1、背景-13"><span class="nav-number">48.1.</span> <span class="nav-text">1、背景</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2、模型框架-1"><span class="nav-number">48.2.</span> <span class="nav-text">2、模型框架</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#2-1-Embedding-Layer"><span class="nav-number">48.2.0.1.</span> <span class="nav-text">2.1 Embedding Layer</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-2-Transformer-Layer"><span class="nav-number">48.2.0.2.</span> <span class="nav-text">2.2 Transformer Layer</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-3-MLP-Layer"><span class="nav-number">48.2.0.3.</span> <span class="nav-text">2.3 MLP Layer</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3、实验结果及分析"><span class="nav-number">48.3.</span> <span class="nav-text">3、实验结果及分析</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#3-1-对比试验"><span class="nav-number">48.3.0.1.</span> <span class="nav-text">3.1 对比试验</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-2-Transformer中block层数实验"><span class="nav-number">48.3.0.2.</span> <span class="nav-text">3.2 Transformer中block层数实验</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#推荐系统遇上深度学习-四十九-九篇阿里推荐相关论文汇总！"><span class="nav-number">49.</span> <span class="nav-text">推荐系统遇上深度学习(四十九)-九篇阿里推荐相关论文汇总！</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1、Billion-scale-Commodity-Embedding-for-E-commerce-Recommendation-in-Alibaba"><span class="nav-number">49.1.</span> <span class="nav-text">1、Billion-scale Commodity Embedding for E-commerce Recommendation in Alibaba</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2、Learning-and-Transferring-IDs-Representation-in-E-commerce"><span class="nav-number">49.2.</span> <span class="nav-text">2、Learning and Transferring IDs Representation in E-commerce</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3、TDM：Learning-Tree-based-Deep-Model-for-Recommender-Systems"><span class="nav-number">49.3.</span> <span class="nav-text">3、TDM：Learning Tree-based Deep Model for Recommender Systems</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#3-1-怎么基于树来实现高效的检索-1"><span class="nav-number">49.3.0.1.</span> <span class="nav-text">3.1 怎么基于树来实现高效的检索</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-2-怎么在树上面做兴趣建模-1"><span class="nav-number">49.3.0.2.</span> <span class="nav-text">3.2 怎么在树上面做兴趣建模</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-3-兴趣树是怎么构建的-1"><span class="nav-number">49.3.0.3.</span> <span class="nav-text">3.3 兴趣树是怎么构建的</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4、Learning-Piece-wise-Linear-Models-from-Large-Scale-Data-for-Ad-Click-Prediction"><span class="nav-number">49.4.</span> <span class="nav-text">4、Learning Piece-wise Linear Models from Large Scale Data for Ad Click Prediction</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5、Deep-Interest-Network-for-Click-Through-Rate-Prediction"><span class="nav-number">49.5.</span> <span class="nav-text">5、Deep Interest Network for Click-Through Rate Prediction</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#6、Deep-Interest-Evolution-Network-for-Click-Through-Rate-Prediction"><span class="nav-number">49.6.</span> <span class="nav-text">6、Deep Interest Evolution Network for Click-Through Rate Prediction</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#7、Deep-Session-Interest-Network-for-Click-Through-Rate-Prediction"><span class="nav-number">49.7.</span> <span class="nav-text">7、Deep Session Interest Network for Click-Through Rate Prediction</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#8、Behavior-Sequence-Transformer-for-E-commerce-Recommendation-in-Alibaba"><span class="nav-number">49.8.</span> <span class="nav-text">8、Behavior Sequence Transformer for E-commerce Recommendation in Alibaba</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#9、Entire-Space-Multi-Task-Model-An-E-ective-Approach-for-Estimating-Post-Click-Conversion-Rate"><span class="nav-number">49.9.</span> <span class="nav-text">9、Entire Space Multi-Task Model: An E ective Approach for Estimating Post-Click Conversion Rate</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#推荐系统遇上深度学习-五十-使用强化学习优化用户的长期体验"><span class="nav-number">50.</span> <span class="nav-text">推荐系统遇上深度学习(五十)-使用强化学习优化用户的长期体验</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1、问题定义-1"><span class="nav-number">50.1.</span> <span class="nav-text">1、问题定义</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2、模型介绍-3"><span class="nav-number">50.2.</span> <span class="nav-text">2、模型介绍</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#2-1-Q网络"><span class="nav-number">50.2.0.1.</span> <span class="nav-text">2.1 Q网络</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-2-S网络"><span class="nav-number">50.2.0.2.</span> <span class="nav-text">2.2 S网络</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-3-整体架构"><span class="nav-number">50.2.0.3.</span> <span class="nav-text">2.3 整体架构</span></a></li></ol></li></ol></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>
<div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">InvictusY</span>
</div>



<div class="powered-by">
<i class="fa fa-user-md"></i><span id="busuanzi_container_site_uv">
  本站访客数:<span id="busuanzi_value_site_uv"></span>
</span>
</div <p>Hosted by <a href="https://pages.coding.me" style="font-weight: bold">Coding Pages</a></p>

<!-- div class="powered-by">
  由 <a class="theme-link" href="https://hexo.io">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Gemini
  </a>
</div>
-->



        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  


  











  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>

  
  <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.2"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.2"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.2"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.2"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.2"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.2"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.2"></script>



  


  




	





  





  






  





  

  
  <script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.4.js"></script>
  <script>AV.initialize("yjKCO69gNOrNreIkps0n04MU-gzGzoHsz", "gup63WiloinVIHXau7iBULgE");</script>
  <script>
    function showTime(Counter) {
      var query = new AV.Query(Counter);
      var entries = [];
      var $visitors = $(".leancloud_visitors");

      $visitors.each(function () {
        entries.push( $(this).attr("id").trim() );
      });

      query.containedIn('url', entries);
      query.find()
        .done(function (results) {
          var COUNT_CONTAINER_REF = '.leancloud-visitors-count';

          if (results.length === 0) {
            $visitors.find(COUNT_CONTAINER_REF).text(0);
            return;
          }

          for (var i = 0; i < results.length; i++) {
            var item = results[i];
            var url = item.get('url');
            var time = item.get('time');
            var element = document.getElementById(url);

            $(element).find(COUNT_CONTAINER_REF).text(time);
          }
          for(var i = 0; i < entries.length; i++) {
            var url = entries[i];
            var element = document.getElementById(url);
            var countSpan = $(element).find(COUNT_CONTAINER_REF);
            if( countSpan.text() == '') {
              countSpan.text(0);
            }
          }
        })
        .fail(function (object, error) {
          console.log("Error: " + error.code + " " + error.message);
        });
    }

    function addCount(Counter) {
      var $visitors = $(".leancloud_visitors");
      var url = $visitors.attr('id').trim();
      var title = $visitors.attr('data-flag-title').trim();
      var query = new AV.Query(Counter);

      query.equalTo("url", url);
      query.find({
        success: function(results) {
          if (results.length > 0) {
            var counter = results[0];
            counter.fetchWhenSave(true);
            counter.increment("time");
            counter.save(null, {
              success: function(counter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(counter.get('time'));
              },
              error: function(counter, error) {
                console.log('Failed to save Visitor num, with error message: ' + error.message);
              }
            });
          } else {
            var newcounter = new Counter();
            /* Set ACL */
            var acl = new AV.ACL();
            acl.setPublicReadAccess(true);
            acl.setPublicWriteAccess(true);
            newcounter.setACL(acl);
            /* End Set ACL */
            newcounter.set("title", title);
            newcounter.set("url", url);
            newcounter.set("time", 1);
            newcounter.save(null, {
              success: function(newcounter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(newcounter.get('time'));
              },
              error: function(newcounter, error) {
                console.log('Failed to create');
              }
            });
          }
        },
        error: function(error) {
          console.log('Error:' + error.code + " " + error.message);
        }
      });
    }

    $(function() {
      var Counter = AV.Object.extend("Counter");
      if ($('.leancloud_visitors').length == 1) {
        addCount(Counter);
      } else if ($('.post-title-link').length > 1) {
        showTime(Counter);
      }
    });
  </script>



  

  

  

  

  

</body>
</html>
